[
  {
    "objectID": "research/citations_with_quarto/index.html",
    "href": "research/citations_with_quarto/index.html",
    "title": "Citations with quarto",
    "section": "",
    "text": "I use Zotero to manage my references. I have my entire library automatically dumped into a library.bib file on my computer. I created a symbolic link to this library in the root directory of my quarto website by calling ln -s /Users/karltayeb/Documents/biblatex/library.bib .\nThen I added bibliography: library.bib\nto the _quarto.yml file which controls global options for the website. Now we can add references by there citation key using @citekey\nE.g. here is a paper (1)\nThis works site-wide. Also note you can change citation style using csl option. Personally I like Vancouver format since the citations just appear as numbers which is less disruptive when reading. Particularly in this setting, where we can hover over the citation to get more information I think it is a good online format.\n\n\n\n\nReferences\n\n1. Arvanitis M, Tayeb K, Strober BJ, Battle A. Redefining tissue specificity of genetic regulation of gene expression in the presence of allelic heterogeneity. Am J Hum Genet [Internet]. 2022 Feb 3 [cited 2022 Jun 8];109(2):223–39. Available from: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8874223/"
  },
  {
    "objectID": "research/shift_invariant_prior/index.html",
    "href": "research/shift_invariant_prior/index.html",
    "title": "X-shift invariant prior for logistic regresion",
    "section": "",
    "text": "Consider the univariate logistic regression\n\\[\\begin{align}\n\\psi = b_0 + b_1 x \\\\\ny \\sim \\text{Binomial}(1, \\sigma(\\psi))\\\\\n\\begin{bmatrix}b_0 \\\\ b_1\\end{bmatrix} \\sim g\n\\end{align}\\]\nWe want \\(p(\\mathbf{b}| y, X) = p(\\mathbf{b}| y, \\tilde X)\\) where \\(\\tilde X = X + \\mathbf{c}\\mathbf{1}^T\\) for some \\(\\mathbf{c}\\in \\mathbb{R}^p\\)\nIn a meeting with Matthew on 2023-01-10 I mistakenly thought that \\(\\nabla^2_\\mathbf{b}\\log p(y | \\mathbf{b}, X)\\) does not change, but that is clearly not the case.\n\\[\\begin{align}\n\\nabla^2_\\mathbf{b}\\log p(y | \\mathbf{b}, X) &= X^T D X \\\\\nD &= \\text{diag}(\\sigma(\\psi_i)(1 - \\sigma(\\psi_i))) \\\\\n\\psi &= \\mathbf{b}^T \\mathbf{x}_i\n\\end{align}\\]\nInstead it’s that \\(\\left[\\nabla^2_\\mathbf{b}\\log p(y | \\mathbf{b}, X)\\right]^{-1}_{11}\\) for shifts in \\(\\mathbf{x}\\). In the example below we see that the MLE and stderr are unchanged for the effect, all the change is absorbed into the intercept.\nLet \\(X = [\\mathbf{1}, \\mathbf{x}]\\) and \\(\\tilde X = [\\mathbf{1}, \\mathbf{x}+ \\mathbf{1}c]\\). Note that the linear predictions at the MLE do not change since we can map parameters of the first regression problem to parameters of the second \\((b_0, b_1) \\mapsto (b_0 + b_1 c, b_1)\\). Then \\(D = \\tilde D\\)\nWrite \\(\\delta = \\mathbf{1}^T D \\mathbf{1}\\), \\(\\gamma = \\mathbf{x}^T D \\mathbf{x}\\) and \\(a = \\mathbf{x}^T D \\mathbf{1}\\)\n\n\nCode\nx <- rnorm(1000)\ny <- rbinom(1000, 1, 1/(1 + exp(-x)))\n\nx2 <- x + 10\n\nfit <- glm(y ~ x, family='binomial')\nfit2 <- glm(y ~ x2, family='binomial')\n\nsummary(fit)$coef\n\n\n              Estimate Std. Error    z value     Pr(>|z|)\n(Intercept) 0.03216293 0.07176779  0.4481527 6.540430e-01\nx           1.19888202 0.09080725 13.2024919 8.488004e-40\n\n\nCode\nsummary(fit2)$coef\n\n\n              Estimate Std. Error   z value     Pr(>|z|)\n(Intercept) -11.956657 0.90645402 -13.19058 9.941294e-40\nx2            1.198882 0.09080725  13.20249 8.488004e-40\n\n\n\\[\\begin{align}\n(\\tilde X ^T D \\tilde X)^{-1}\n&= \\begin{bmatrix}\\delta & a + \\delta \\mu \\\\ a + \\delta \\mu & \\delta \\mu + \\gamma + 2 a \\mu + \\mu^2 \\delta \\end{bmatrix}^{-1}\n&= \\frac{1}{\\delta\\gamma - a^2} \\begin{bmatrix}\\delta \\mu + \\gamma + 2 a \\mu + \\mu^2 \\delta  & - a - \\delta \\mu \\\\ - a - \\delta \\mu &  \\delta  \\end{bmatrix}\n\\end{align}\\]"
  },
  {
    "objectID": "research/polynomial_approximation/index.html",
    "href": "research/polynomial_approximation/index.html",
    "title": "Polynomial approximation for variational Bayes",
    "section": "",
    "text": "$$"
  },
  {
    "objectID": "research/polynomial_approximation/index.html#overview",
    "href": "research/polynomial_approximation/index.html#overview",
    "title": "Polynomial approximation for variational Bayes",
    "section": "Overview",
    "text": "Overview\nWe propose approximating Bayesian linear regression problems by replacing the conditional likelihood of each observation with a polynomial approximation designed to be close to the true likelihood on an interval, and a lower bound for values far outside the interval. If the log density of our prior distribution can also be expressed (or approximated) as a polynomial the exact posterior of this polynomial approximation is also a polynomial– i.e. conjugate.\nThis is the main benefit of our approach: inference boils down to simple manipulations of the polynomial coefficients, which can be performed in parallel and with a set of simple linear algebra operations (matrix multiplication, solving a triangular system, etc). There may exist more computationally efficient algorithms for carrying out these operations, which we should explore further.\nThis approach can be generalized to many likelihood (e.g. Binomial, Poisson) with various choices of link function (which relate the regression predictions to the distribution parameters)– at least to the extent that we can develop a good polynomial approximation for each combination of likelihood and link.\nTake a look at this rough implementation."
  },
  {
    "objectID": "research/polynomial_approximation/index.html#global-polynomial-approximations",
    "href": "research/polynomial_approximation/index.html#global-polynomial-approximations",
    "title": "Polynomial approximation for variational Bayes",
    "section": "Global polynomial approximations",
    "text": "Global polynomial approximations\n\nLocal approximations, and trouble with SuSiE\nThis approach was motivated by our work to develop a good variational approximation for logistic SuSiE, that is a logistic regression with the sum of single effects (SuSiE) [(susie?)]. For Gaussian linear models, using a variational approximation that factorizes over single effects produces a lightning fast Bayesian variable selection method. Rather than exploring each configuration of non-zero effect variables, this approximation allows us to update our approximating distribution for one effect while marginalizing over all other effects. Inference in the (Gaussian, linear) SuSiE model reduces to a sequence of embarrassingly parallel univariate linear regression problems.\nIn the case of Gaussian model with identify link (linear regression), the variational approximation is sufficient for computation to simplify, because the log likelihood is quadratic in the regression coefficients \\(\\beta\\). However, the Bernoulli likelihood with a logistic link function (logistic regression), and indeed for many other likelihood-link combinations of interest, do not enjoy the algebraic simplicity of a Gaussian model with identity link (linear regression). The main issue is that it is difficult to marginalize over the other effects. Even if the approximate posterior factorizes, the likelihood computation can combine single effects in a way that complicates the marginalization step.\nWe’ve attempted using local quadratic approximations to the likelihood. The idea is to construct a quadratic lower bound to the likelihood that is “good” in a neighborhood that we care about (contains most of the posterior mass). This is a popular technique for applying variational Bayes to logistic regression (see Jaakkola and Jordan, Polya-Gamma augmentation), and indeed these sorts of “local” approximation schemes can be generalized to super-Gaussian likelihoods [(1)].\nHowever, we find that the local approximation techniques are not well suited to variable selection techniques like SuSiE. In order to use the local approximation techniques, we must optimize a set of variational parameters that essentially dictate where the approximation is good. You only get to select one local approximation per observation. Holding these variational parameters fixed, our inference will systematically favor posterior distributions that place more mass in a neighborhood of these variational parameters. The strong coupling between the local variational parameters and optimal variational approximation make it difficult to navigate the optimization surface via coordinate ascent.\nAdditionally, uncertainty quantification is an important output to SuSiE and other Bayesian variable selection methods. Due to the nature of the local variational approximations we may tend to see a “winner take all” scenario. Assuming we find a good local optimum, the local variational parameters will be tuned to be consistent with the most likely variable configurations. In turn, the objective function will be a tighter lower bound in these regions, and the approximating distribution will place more probability mass on these configurations.\nNote: the Laplace approximation is also a “local” approximation, since we construct a quadratic approximation of the joint log likelihood around the MAP estimate. It is also closely related Gaussian variational approximations.\n\n\nGlobal approximations\nRather than find a “local” approximation which has variational parameters that must be tuned at each iteration, we propose finding a global approximation to the likelihood which is easier to work with.\n“Easier to work with”, in the context of SuSiE means that we can write \\(f(\\psi_l) =\\mathbb E_{q_{-l}}[\\log p(y | \\psi_l + \\psi_{-l})]\\), where \\(f(\\psi_l; q_{-l})\\) is easy to evaluate/optimize over. Notice that if we approximate \\(\\log p(y | \\psi) \\approx f(\\psi) = \\sum_k c_k \\psi^k\\) then taking the expectation of \\(f\\) over \\(q_{-l}\\) results in a polynomial in \\(\\psi_l\\).\n\\[\nE_{q_{-l}}[f(\\psi_l + \\psi_{-l})] = \\hat f(\\psi_l; q_{-l}) = \\sum \\hat c_k(q_{-l}) \\psi_l^k.\n\\]\nHere \\(\\hat c_k(q_{-l})\\) are a transformation of the original coefficients \\(c_k\\) that are achieved by (1) expanding the original polynomial in terms of \\(\\psi_l\\) and \\(\\psi_{-l}\\), (2) marginalizing over \\(\\psi_{-l}\\) w.r.t \\(q_{-l}\\) and then (3) regrouping terms to get coefficients of a polynomial in \\(\\psi_l\\).\nWhile the local variational approximations are only good around a point specified by the variational parameter, we can construct a “global” polynomial approximation that is good on any interval if we allow the degree of the polynomial to be sufficiently high. While we sidestep the issue of needing to tune the local variational parameters, we replace it with the need to select an interval that we care to approximate. There is a trade off here– for a fixed error tolerance, wider intervals will require higher degree approximations to bound the error."
  },
  {
    "objectID": "research/polynomial_approximation/index.html#polynomial-representation",
    "href": "research/polynomial_approximation/index.html#polynomial-representation",
    "title": "Polynomial approximation for variational Bayes",
    "section": "Polynomial representation",
    "text": "Polynomial representation\n\nRepresenting functions with polynomial series\nLet \\(\\set{P_k}_{k=1}^{\\infty}\\) be a sequence of polynomials that form a basis for continuous functions on an interval, e.g. \\([-1, 1]\\) (note: that through a change of variable we can stretch this interval to any interval we want). So that for any \\(f: [-1,1] \\\\rightarrow \\mathbb R\\) there are \\(\\set{c_k}\\) such that\nWe’ll assume that \\(P_k\\) are ordered by increasing degree, and that \\(deg(P_k) \\leq k\\).\n\n\nChebyshev polynomials\nThese are a family of polynomials on \\([-1, 1]\\) defined by\n\\[T_n(\\cos(\\theta)) = \\cos(n \\theta)\\]\nThey can also be obtained by the reccurrence\n\\[\n\\begin{aligned}\n  T_0(x) &= 1 \\\\\n  T_1(x) &= x \\\\\n  T_{n+1}(x) &= 2x T_n(x) - T_{n-1}(x)\n\\end{aligned}\n\\] The Chebyshev polynomials are orthogonal to eachother, and form a basis for ~a certain family of function~ on the interval \\([-1, 1]\\), so that \\(f(x) = \\sum_{k=0}^\\infty c_k T_k(x)\\). This means that we can get the coefficients by evaluating an inner product\n\\[\n\\langle f, T_k \\rangle = c_k ||T_k||^2\n\\] Through a change of variable, we can approximate functions on any interval. We can obtain a \\(K+1\\) degree polynomial approximation by computing the first \\(K\\) coefficients \\((c_k)_{k=0}^K\\). Each coefficient \\(c_k\\) is effectively computed via a \\(k+1\\) point quadrature (Note: it looks like there is a simple rule for computing the coefficients as linear combinations of \\(f\\) evauated at a set of “nodes”, this looks very closely related to the quadrature rule, but I want to figure this out in more detail)\n\n\nConversion between polynomial basis\nSuppose we have a polynomial basis \\(\\{A_k\\}\\) and \\(\\{B_k\\}\\). Let \\(M_k(x) = x^k\\), \\(\\{M_k\\}\\) is the “monomial basis”, so that we can write \\(A_k(x) = \\sum_{j=0}^k \\alpha_{kj} M_j(x)\\). We can arrange these coefficients for the first \\(K\\) polynomials into the upper triangular matrix:\n\\[\nA^{(k)} =\n\\begin{bmatrix}\n\\alpha_{00} & 0 & 0 &\\dots & 0 \\\\\\\\\n\\alpha_{10} & \\alpha_{11} & 0 & \\dots & 0 \\\\\\\\\n\\alpha_{20} & \\alpha_{21} & \\alpha_{22} &\\dots & 0 \\\\\\\\\n\\dots\\\\\\\\\n\\alpha_{K0} & \\alpha_{K1} & \\alpha_{K2} &\\dots & \\alpha_{KK}\n\\end{bmatrix}\n^T\n\\]\nNow we can see that for \\(f(x) = \\sum_{k=0}^K a_k A_k(x)\\). We can take the vector of coefficients \\(\\vec a = (a_0, \\dots, a_K)\\) and convert back to coefficients in the monomial basis with a simple matrix multiplication\nTo convert from the monomial basis to the basis \\(A\\) invovles solving the triangular system\nTo convert from the monomial basis to the basis \\(A\\) to the basis \\(B\\) involves (1) expanding to the monomial basis and (2) solving for the coefficients in basis \\(B\\).\nApparently there are \\(O(K\\log(K))\\) algorithms for changing basis, it may be worth understanding these. But it’s very easy to see how we can move between different bases for polynomials of degree \\(K\\) by matrix multiplication.\n\n\nShifting and scaling\nWe can “shift” the polynomial basis too, that is we rewrite \\(f(x + y) = g(x; y)\\) there \\(g\\) is a polynomial in \\(x\\). This has the effect of moving fixed information \\(y\\) out of the functions argument and into the polynomial coefficients.\nHere \\(\\tilde M_k(x_1; x_2)\\) is a \\(k\\) degree polynomial in \\(x_1\\). \\(x_2\\) is treated as a parameter that is absorbed into the coefficients. Next we’d like to do the same for general polynomials. So we’d like to find \\(\\tilde a\\) such that \\(f(x_1 + x_2) = \\sum a_k A_k(x_1 + x_2) = \\sum_k \\tilde a(x_2) A_k(x_1)\\).\nLet \\(m_{kj} = 0 \\; \\forall k < j\\), and \\(M(x_2)\\) be the upper triangular matrix \\(M(x_2) = \\begin{bmatrix} m_{kj}(x_2)\\end{bmatrix}_{k=0, \\dots K,\\\\; j=0,\\dots,K}\\)\nThen we can find the coordinates\n\\[\n\\tilde a(x_2) = (A^{(k)})^{-1}M(x_2)A^{(k)} a\n\\]\nand\n\\[\n\\mathbb E_{ q(x_2) } \\left[  \\tilde a(x_2)  \\right] =  (A^{(k)})^{-1}\\mathbb E_{ q(x_2) } \\left[  M(x_2)  \\right]A^{(k)} a\n\\]\n\n\nThings to look into\nClenshaw algorithm and Horner’s method. these are recursive methods for evaluating polynomials in the Chebyshev and monomial basis respectively.\nHamming and Salzer develop algorithms for converting polynomials between different basis representations.\nWe may not be able to use these techniques, unless we can get an expression for each coefficient, because we need to evaluate the expected value of the terms."
  },
  {
    "objectID": "research/polynomial_approximation/index.html#variational-approximation",
    "href": "research/polynomial_approximation/index.html#variational-approximation",
    "title": "Polynomial approximation for variational Bayes",
    "section": "Variational approximation",
    "text": "Variational approximation\nHere we present the variational approximation for SuSiE and explore how to perform coordinate ascent variational inference with this approximation and our polynomial likelihoods.\n\nEvidence lower bound, and a polynomial approximation\nWe can write the log likelihood for a single observation as a function \\(y_i\\) of a function of the linear predictor \\(\\psi_i = \\sum_{l} \\psi_{li}\\), then we will approximate that likelihood with a polynomial in \\(\\psi_i\\)\n\\[\n\\log p (y_i | \\psi_i) = l_i(\\psi_i) \\approx \\sum_{k=0}^K m_{ik} \\psi_i^k =: \\hat l_i^{(K)}(\\psi_i)\n\\]\nThen we can approximate the ELBO\n\\[\n\\begin{aligned}\nF_1(q) &= \\mathbb E_{ q } \\left[  \\sum l_i(\\psi_i)  \\right]- KL(q || p) \\\\\n       &\\approx \\mathbb E_{ q } \\left[  \\sum \\hat l_i(\\psi_i)  \\right] - KL(q || p) =: F_2(q)\n\\end{aligned}\n\\]\nWe perform inference by selecting \\(q \\in \\mathcal Q\\) to optimize \\(F_2\\), where \\(\\mathcal Q\\) is a family of distributions.\n\\[\nq^* = \\arg\\max_{q \\in \\mathcal Q} F_2(q)\n\\]\nLet \\({\\bf m}_i = (m_{i0}, \\dots, m_{iK})\\) denote the coefficients for \\(\\hat l_i\\).\nTo perform coordinate ascent variational inference we will need to compute \\(\\mathbb E_{ q_{-l} } \\left[  \\hat l(\\psi_l + \\psi_{-l})  \\right]\\). We will write \\(\\tilde l(\\psi_l; \\psi_{-l}) = \\hat l(\\psi_l + \\psi_{-l})\\) to emphasize that we are treating the likelihood as a function of \\(\\psi_l\\), with \\(psi_{-l}\\) fixed. By expanding the \\((\\psi_l + \\psi_{-l})^k\\) terms and collecting coefficients we can write \\(f(\\psi_l; \\psi_{-l}) = \\sum_k \\tilde m_k(\\psi_{-l}) \\psi_l^k\\). Now \\(\\tilde {\\bf m}(\\psi_{-l}) = (m_k(\\psi_{-l}))_{k=0}^K\\) give the coefficients for a polynomial in \\(\\psi_l\\). Now, taking expectations\n\\[\n\\tilde f(\\psi_l; q_{-l}) := \\mathbb E_{ q_{-l} } \\left[  \\hat l(\\psi_l + \\psi_{-l})  \\right]\n= \\mathbb E_{ q_{-l} } \\left[  \\tilde l(\\psi_l; \\psi_{-l})  \\right]\n= \\sum_k \\mathbb E_{ q_{-l} } \\left[  \\tilde m_k(\\psi_{-l})  \\right] \\psi_l^k\n\\] Noting that we can write \\(\\hat l(\\psi) = \\hat l(\\psi_l + \\psi_{-l})\\)\n\n\nSuSiE variational approximation\nFor SuSiE our latent variables consist of the effect sizes \\(\\set{b_l}\\_{l=1}^L\\) and a set of indicators that select \\(L\\) non-zero effect variables \\(\\set{\\gamma_l}\\_{l=1}^L\\). We select \\(\\mathcal Q\\) to be the family of distributions that factorize over the \\(L\\) single effects, that is\n\\[\nq(\\set{b_l}, \\set{\\gamma_l}) = \\prod_l q(b_l | \\gamma_l)q(\\gamma_l).\n\\]\n\n\nCoordinate ascent\nTo update \\(q_l\\) we need to maximize\nDropping the subscript \\(i\\), for each term in the sum we need to compute\nWe can do this by applying the “expected” shift operator. We can do this by computing the moments of \\(\\psi_{-l}\\) and applying the shift operation once, or by computing the moments of \\(\\psi_m, \\;\\; m\\neq l\\) and performing the shift operation sequentially.\n(TODO: update notation here)\n\\[\n\\tilde a(q_{-l})\n= (A^{(k)})^{-1} \\left(\\prod_{m \\neq l} \\mathbb E_{ q_m } \\left[  M(\\psi_m)  \\right] \\right) A^{(k)} a\n\\]\nA nice feature of the sequential approach is it gives us an easy way of converting between polynomial representations. Let \\(\\Gamma_l = \\mathbb E_{ q_l } \\left[  M(\\psi_l)  \\right]\\) be the matrix for applying the “shifted expectation” operation to the polynomial coefficients for \\(f(\\psi_l)\\), \\({\\bf m}\\). That is \\(\\Gamma_l {\\bf m}\\) gives the coefficients of \\(f(\\psi_{-l}; q_l)\\), which is a polynomial in \\(\\psi_{-l}\\).\nLet \\(\\Gamma = \\prod_l \\Gamma_l\\). Notice that the polynomial with coefficients \\(\\Gamma {\\bf m}\\) evalutated at \\(0\\) give \\(\\mathbb E_{ q } \\left[  f(x)  \\right]\\). Furthermore we can quickly move from \\(f(\\psi_{l}; q_{-l})\\) to \\(f(\\psi_{l+1}; q_{-(l+1)})\\).\nStarting with \\(f(\\psi_{1}; q_{-1})\\) we want the coefficients \\(\\Gamma_{-1} {\\bf m}\\), where \\(\\Gamma_{-1} = \\Gamma_1^{-1} \\Gamma\\). Then, to get \\(f(\\psi_{1}; q_{-1})\\) we need \\(\\Gamma_{-2} {\\bf m}\\), where we can compute \\(\\Gamma_{-2}\\) by\n\\[\n\\Gamma_{-2} = \\Gamma_2^{-1} \\Gamma_{-1} \\Gamma_1.\n\\] We can continue iterating over all \\(L\\) single effects. We note that it is easier to compute the moments of the single effects \\(\\psi_m\\) rather than the moments of all “other” single effects \\(\\psi_{-l}\\). Carrying out the iterated expectations as matrix-vector products in the polynomial coefficients seems like an appealing approach to implementation.\nThis is useful in a coordinate ascent update scheme where we can remove one of the single effect from \\(\\Gamma\\) by a triangular system. Update \\(q_l\\), and then add back the update \\(\\Gamma_l\\) to \\(\\Gamma\\) by a right matrix multiplication.\n\n\nUnconstrained variational posterior\nThe optimal variational approximation looks like\n\\[\nq^*\\_l(b_l | \\gamma_l = j) \\propto e^{f(b_l; q_{-l}, {\\bf x}\\_j)}.\n\\] Where \\(f(b_l; q_{-l}, {\\bf x}\\_j) = \\sum_k \\eta_k b_l^k\\) is a polynomial of degree \\(K\\). Notice that this is an exponential family with sufficient statistics \\(T(b_l) = (b_l^k)\\_{k=0}^K\\) and natural parameters \\({\\bf \\eta}\\). It has a normalizing constant \\(A(\\eta) = \\log \\int \\exp\\{\\langle T(x), \\eta \\rangle\\} dx\\), and \\(\\nabla_{\\eta} A(\\eta) = \\mathbb E_{ q } \\left[  T(x)  \\right]\\). Thus if we can compute (or approximate to satisfactory precision) \\(\\nabla_{\\eta} A(\\eta)\\) we could compute the moments we need for CAVI.\nTo date, I am not really sure how to handle this integral of an exponentiation polynomial. By designing our polynomial approximation correctly, we can ensure that the the exponentiation function will decay and the \\(A\\) will be finite (recall also that \\(\\set{\\eta: A(\\eta) < \\infty}\\) is the natural parameter space).\nOne option is to approximate \\(A(\\eta)\\) by a quadrature rule. We can use automatic differentiation to compute it’s gradient.\n\n\nBest gaussian approximation\nMaybe we don’t know how to compute \\(A(\\eta) = \\log \\int \\exp\\{\\langle T(x), \\eta \\rangle\\} dx\\) Which involves evaluate the integral of an exponentiated polynomial. But perhaps we want to use a Gaussian variational approximation.\n\\[\nq_{l}^*(x) \\propto e^{f(x)}\n\\approx e^{f(\\mu) + f'(\\mu)(x-\\mu) + \\frac{1}{2}f''(\\mu)(x - \\mu)^2}\n\\]\nFor \\(\\mu\\) such that \\(f'(\\mu) = 0\\)\n\\[\ne^{f(\\mu) + f'(\\mu)(x-\\mu) + \\frac{1}{2}f''(\\mu)(x - \\mu)^2} \\propto e^{\\frac{1}{2}f''(\\mu)(x - \\mu)^2} \\propto \\mathcal N (x; \\mu, -\\frac{1}{f''(\\mu)})\n\\]\nIn our case \\(f\\) is a polynomial. Finding \\(\\mu\\) can be achieved by searching over the roots of \\(f'\\) and then \\(f''(\\mu)\\) is computed easily. This is a Laplace approximation to the optimal posterior \\(q_l\\)"
  },
  {
    "objectID": "research/polynomial_approximation/index.html#more-scattered-notes-on-polynomial-approximation-for-susie",
    "href": "research/polynomial_approximation/index.html#more-scattered-notes-on-polynomial-approximation-for-susie",
    "title": "Polynomial approximation for variational Bayes",
    "section": "More scattered notes on polynomial approximation for SuSiE",
    "text": "More scattered notes on polynomial approximation for SuSiE\nAbusing notation a bit, \\(\\phi_l = x_{\\gamma_l} b_l\\).\n\\[\n\\begin{aligned}\nf(\\psi_l)\n&= \\sum \\tilde m(q_{-l}) M_k(\\psi_l) \\\\\\\\\n&= \\sum \\tilde m(q_{-l}) A_k(x_jb_l) \\\\\\\\\n&= \\sum \\hat m_k(q_{-l}, x_j) M_k(b_l) \\\\quad \\hat m_k(q_{-l}, x_j) := \\tilde m_k(q_{-l})x_j^k\n\\end{aligned}\n\\]\n\\[\n\\mathbb E_{ q_{-l} } \\left[  f(\\psi)  \\right]\n= \\mathbb E_{ q_{-l} } \\left[  f(\\psi_l, \\psi_{-l}))  \\right]\n= \\sum \\mathbb E_{ q_{-l} } \\left[  \\tilde {\\bf m}(\\psi_{-l})  \\right] M_k(\\psi_l)\n= \\sum \\left(\\prod_{m \\neq l}\\mathbb E_{ q_m } \\left[  M(\\psi_m))  \\right]\\right){\\bf m}\n\\]\nWe can write\n\\[\n\\Gamma_l = \\mathbb E_{ q_l } \\left[  M(\\psi_l)  \\right]\n\\] \\[\n\\Psi^{(l)} = \\Gamma_{l+1} \\dots \\Gamma_L \\Gamma_1 \\dots \\Gamma_{l-1}\n\\] Then we can compute the coefficients of \\(f(\\psi_l)\\) by a triangular matrix multiplication\n\\[\n\\tilde{\\bf m}\\_l = \\Psi^{(l)}{\\bf m} = \\mathbb E_{ q_{-l} } \\left[  M(\\psi_{-l})  \\right]{\\bf m}\n\\]\nAnd we can compute the next \\(\\Psi\\) by a triangular matrix inversion and two matrix multiplications.\n\\[\n\\Psi^{(l+1)} = \\Gamma_{l+1}^{-1} \\Psi^{(l)} \\Gamma_l\n\\] ### Rescal polynomial\n\\[\nf(bx) = \\sum m_k (bx)^k = \\sum m_k b^kx^k = \\sum (m_k b^k) x^k\n\\]\n\nShift\n\\[\nf(x - c)\n= \\sum m_k (x -c)^k\n= \\sum_k m_k \\sum_{j \\leq k} {k \\choose j} x^j c^{k-j}\n= \\sum_j \\left(\\sum_{k \\geq j} {k \\choose j} c^{k-j}\\right) x^j\n\\]\n\n\nUpdating \\(q_l\\)\nWe’ve written the natural parameters\nFor each observation we can compute\n\\[\n\\hat{{\\bf m}}\\_{li} = \\tilde{\\bf m}(q_{-l}) \\circ (x_i^0, \\dots, x_i^K)\n\\]\nThese are the coefficients in the monomial basis for each observation conditional on effect \\(b_l\\) for covariate \\(x_i\\). \\(f_i(b_l) = \\sum \\tilde {\\bf m}\\_k b_l^k\\). That is, this is the data likelihood as a function of \\(b_l\\), conditional on data \\(x\\), and marginalizing over \\(\\psi_{-l}\\).\nWe can express or approximate our prior with the same polynomial expansion. Suppose we can write our prior\n\\[\n\\log p(b_l) = \\sum \\rho_{kl} b_l^k\n\\]\nThen the posterior distribution is trivially computed with a conjugate computation\n\\[\n{\\bf \\eta}\\_l =\\sum_i \\hat {\\bf m}_{li} + {\\bf \\rho}_l\n\\]\nThe posterior distribution is an exponential family with sufficient statistics \\(T(b_l) = (b_l^0, \\dots, b_l^K)\\) and natural parameters \\(\\eta_l\\).\nIf our original polynomial approximation “goes down” outside the range we care to ensure it is a good approximation, then we should always get a finite log-normalizer/cumulant \\(A(\\eta) = \\log \\int \\exp\\{\\eta^T T(\\psi)\\} d\\psi < \\infty\\). It may be important to ensure that our approximation is good over the range of values of \\(\\psi\\) with high posterior probability. Supposing we have an even degree polynomial assumption, make sure the last coefficient is \\(< 0\\) so that the function is very negative for arguments that are large in absolute value, but the approximation is good for values of small absolute value. Intuitivley, taking expectations over \\(\\psi_{-l}\\) won’t change t\nAdditionally, we ideally want to make sure that our likelihood approximation does not have bad behavior. If our polynomial approximation wildly overestimates the likelihood in some regions that could seriously mess up our inference. There is probably a tradeoff. We can approximate \\(l_i\\) on the interval \\([a, b]\\) with lower error with a polynomial of degree \\(K\\). To approximate \\(l_i\\) on the wider interval \\([a, b] \\subset [A, B]\\) with the same error we need a higher degree \\(K\\).\n\n\nComputing moments of \\(q_l\\)\nAn algorithm would look like this\n\nCompute \\(\\Psi_1\\)\nUpdate update \\(q_1\\)\nCompute \\(\\mathbb E_{ q_1 } \\left[  M(\\psi_1)  \\right]\\)\nCompute \\(\\Psi_2\\) …\n\nNote that \\(\\Psi_l\\) is constructed by taking expectations is a particular order, or multiplying matrices in a particular order. But I think order should not matter. Is it the case that triangular matrix multiplication commutes?"
  },
  {
    "objectID": "research/polynomial_approximation/index.html#discussion",
    "href": "research/polynomial_approximation/index.html#discussion",
    "title": "Polynomial approximation for variational Bayes",
    "section": "Discussion",
    "text": "Discussion\n\nQuality of the global approximation\nTheorem C.3 in [(2)] provides a bound on the Wasserstein distance between the exact posterior and the polynomial approximation to the posterior, “the result depends primarily on the peakedness of the approximate posterior, and the error of the approximate gradients”. Informally, I suspect that the more data we accrue, and the more peaked our approximate posterior becomes, the greater demand we must put on the quality of our approximation to the log density. Imagine a situation where the true likelihood surface is flat, but looks a little bumpy due to error in the polynomial approximation. These small bumps will accumulate over a large sample size leading to a spiky posterior where it should have been flat.\nIn contrast, the local approximations should shine as we accumulate more evidence and the likelihood becomes more peaked. This is because we can tune the local approximation to be tight where the likelihood peaks. The errors in the local approximation matter less because we don’t need to stray far from where the bound is tight. Note: when the likelihood is log-concave, the local variational approximations are also concave [(3,4)]. I’ll need to do some more work to understand this completely for generalized local approximations, but it is certainly the case for the logistic case. In fact, running EM with the JJ bound is a good alternative to using Newton’s method to get point estimates of the MAP. Whereas Newton’s method may diverge for some initialization, JJ should converge for any initialization of the variational parameters (note: check this claim).\nQ: Is logistic regression with a Gaussian prior/L2 penalty on the effects convex? If so, we’re replacing a convex problem with a potentially multimodal one."
  },
  {
    "objectID": "research/polynomial_approximation/index.html#glossary",
    "href": "research/polynomial_approximation/index.html#glossary",
    "title": "Polynomial approximation for variational Bayes",
    "section": "Glossary",
    "text": "Glossary\n\n\n\n\n\n\n\nSymbol\nDescription\n\n\n\n\n\\({\\bf a}_i\\)\ncoefficients for \\(f_i\\) in basis \\(\\mathcal A\\)\n\n\n\\(\\tilde {\\bf a}_i(\\psi_2)\\)\ncoefficients for \\(f_i(\\psi_1; \\psi_2)\\) in the basis \\(\\mathcal A\\)\n\n\n\\(\\hat {\\bf a}_i(\\psi_2, x_j)\\)\ncoefficients for \\(f_i(b_1; x_j, \\psi_2)\\) in the basis \\(\\mathcal A\\)\n\n\n\\(M(\\psi_2)\\)\ntriangular matrix shifts monomial basis, \\(\\tilde {\\bf m} (\\psi_2) = M(\\psi_2) {\\bf m}\\). Gives coefficients of \\(f_i(\\psi_1; \\psi_2)\\) in \\(\\mathcal M\\)\n\n\n\\(A\\)\ntriangular matrix maps to coordinates in monomial basis, \\({\\bf m} = A {\\bf a}\\). Gives coefficients of \\(f_i(\\psi_1; \\psi_2)\\) in \\(\\mathcal M\\)\n\n\n\\(f_i(\\psi_1; \\psi_2)\\)\nA polynomial is \\(\\psi_1\\) such that \\(f_i(\\psi_1; \\psi_2) = f_i(\\psi_1 + \\psi_2); \\\\;\\\\; \\tilde {\\bf m}(\\psi_2) = M(\\psi_2){\\bf m}\\) gives the coordinates in the monomial basis"
  },
  {
    "objectID": "research/polynomial_approximation/index.html#related-work",
    "href": "research/polynomial_approximation/index.html#related-work",
    "title": "Polynomial approximation for variational Bayes",
    "section": "Related work",
    "text": "Related work\n(2) (5)"
  },
  {
    "objectID": "research/polynomial_approximation/index.html#references",
    "href": "research/polynomial_approximation/index.html#references",
    "title": "Polynomial approximation for variational Bayes",
    "section": "References",
    "text": "References\n\n\n1. Galy-Fajou T, Wenzel F, Opper M. Automated Augmented Conjugate Inference for Non-conjugate Gaussian Process Models. In: Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics [Internet]. PMLR; 2020 [cited 2022 Sep 12]. p. 3025–35. Available from: https://proceedings.mlr.press/v108/galy-fajou20a.html\n\n\n2. Huggins J, Adams RP, Broderick T. PASS-GLM: Polynomial approximate sufficient statistics for scalable Bayesian GLM inference. In: Advances in Neural Information Processing Systems [Internet]. Curran Associates, Inc.; 2017 [cited 2023 Feb 23]. Available from: https://proceedings.neurips.cc/paper/2017/hash/07811dc6c422334ce36a09ff5cd6fe71-Abstract.html\n\n\n3. Seeger MW. Sparse linear models: Variational approximate inference and Bayesian experimental design. J Phys: Conf Ser [Internet]. 2009 Dec 1 [cited 2023 Mar 24];197:012001. Available from: https://iopscience.iop.org/article/10.1088/1742-6596/197/1/012001\n\n\n4. Challis E, Barber D. Concave Gaussian Variational Approximations for Inference in Large-Scale Bayesian Linear Models. In: Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics [Internet]. JMLR Workshop and Conference Proceedings; 2011 [cited 2023 Mar 24]. p. 199–207. Available from: https://proceedings.mlr.press/v15/challis11a.html\n\n\n5. Wong L. Orthogonal Polynomials–Quadrature Algorithm (OPQA): A Functional Analytical Approach to Bayesian Inference. Orthogonal Polynomials."
  },
  {
    "objectID": "research/gibss_polygenic_scenario/index.html",
    "href": "research/gibss_polygenic_scenario/index.html",
    "title": "Logistic SER coverage under polygenic model",
    "section": "",
    "text": "We simulate binary \\(X\\) under a markov model. The first column of \\(X\\) are iid \\(Bin(1, p)\\) to generate \\(x_{k+1} | x_{k}\\) we sample \\(\\pi\\) indices and flip the bits. By setting \\(\\pi\\) to a small value we get highly correlated \\(X\\), or by setting \\(\\pi =0.5\\) we get uncorrelated \\(X\\).\nTo simulate our phenotypes, we specify \\(h^2\\) of liability for the trait, and \\(\\rho\\) the fraction of \\(h^2\\) explained by the causal variant in the locus. We fix \\(h^2 = 0.2\\) and vary \\(\\rho\\) from \\(0.02\\) to \\(1.0\\) (corresponding to odds ratios of \\(\\approx 1.1 - 2\\)). For each simulation we set the intercept to achieve the desired fraction of cases \\(k=0.1\\). Note the effect sizes in these simulations are not random, they are computed deterministically.\nThese simulation settings are meant to be a reasonable approximation to what we might see in GWAS. But admitidely, the way we generate \\(X\\) is a little odd.\nWe can improve these simulations by: 1. Simulating more realistic genotype type data 1. Being a bit more thoughtful about how we pick effect sizes e.g. have an MAF dependent prior OR."
  },
  {
    "objectID": "research/gibss_polygenic_scenario/index.html#correlated-x",
    "href": "research/gibss_polygenic_scenario/index.html#correlated-x",
    "title": "Logistic SER coverage under polygenic model",
    "section": "Correlated X",
    "text": "Correlated X\n\n\nCode\ndef pluck(series, val):\n    return series.apply(lambda x: x[val])\n\n\n\n\nCode\n# simulate X\nn, p = 5000, 100\nX = sim_binary_X(n=n, p=p, pi1 = 0.1, rho=0.005)\nx = X[:, 0] # first one is causal\n\nrep = np.arange(50)\nrho = np.concatenate([np.linspace(0.02, 0.1, 5), np.linspace(0.2, 1.0, 5)])\ndf = pd.DataFrame(list(itertools.product(rep, rho)), columns=['rep', 'rho'])\ndf['k'] = 0.1\ndf['h2'] = 0.2\ndf['sim'] = df.apply(lambda row: simulate_case_control(x, k=row.k, h2=row.h2, rho=row.rho), axis=1)\ndf['ybar'] = df.sim.apply(lambda sim: sim['y'].mean())\ndf['logit_var'] = df.sim.apply(lambda sim: np.var(sim['logit']))\n\n\n\n\nCode\n# run simulation\ndf['ser'] = df.sim.apply(lambda sim: fit_ser_aug(X, sim['y']))\n\n\n\n\nCode\n# process simulation\ndf['cs'] = df.ser.apply(lambda fit: compute_cs(fit['alpha'], 0.95))\n\n\n/usr/local/lib/python3.11/site-packages/jax/_src/numpy/lax_numpy.py:3527: UserWarning: 'kind' argument to argsort is ignored; only 'stable' sorts are supported.\n  warnings.warn(\"'kind' argument to argsort is ignored; only 'stable' sorts \"\n\n\nCode\ndf['cs_size'] = df.cs.apply(lambda cs: len(cs))\ndf['covered'] = df.cs.apply(lambda cs: 0 in cs)\ndf['max_z'] = df.ser.apply(lambda fit: np.abs(fit['betahat']/np.sqrt(fit['shat2'])).max())\ndf['lbf_ser'] = df.ser.apply(lambda fit: np.array(fit['lbf_ser'].flatten())[0])\ndf['prior_variance'] = df.ser.apply(lambda fit: np.array(fit['prior_variance'].flatten())[0])\n\n\nAcross all CSs\n\n\nCode\ndf2 = df.groupby('rho').agg({'covered': 'mean', 'cs_size': 'median', 'max_z': 'median', 'lbf_ser': 'median'}).rename(columns={'covered': 'mean_covered', 'cs_size': 'median_cs_size', 'max_z': 'median_max_z', 'lbf_ser': 'median_lbf_ser'}).reset_index()\ndf2\n\n\n    rho  mean_covered  median_cs_size  median_max_z  median_lbf_ser\n0  0.02          0.82            82.5      3.200346        0.730564\n1  0.04          0.94            28.5      4.354254        3.307323\n2  0.06          0.96             9.0      5.859319        9.163818\n3  0.08          1.00             8.0      6.158600       10.446674\n4  0.10          0.98             5.0      7.288703       16.028008\n5  0.20          1.00             2.0     11.153625       43.252552\n6  0.40          0.98             1.0     17.539280      111.461807\n7  0.60          1.00             1.0     22.606627      186.874664\n8  0.80          1.00             1.0     26.861099      277.643341\n9  1.00          1.00             1.0     30.352369      381.481323\n\n\nThat contains CSs for all simulations. But in some cases there isn’t strong evidence for an effect at all, and we would not report the CSs. If we filter to SERs with and \\(\\log BF_{SER} > 1\\), we see improved coverage in the smallest effect size simulation.\n\n\nCode\ndf2 = df[df.lbf_ser > 1].groupby('rho').agg({'covered': 'mean', 'cs_size': 'median', 'max_z': 'median', 'lbf_ser': 'median'}).rename(columns={'covered': 'mean_covered', 'cs_size': 'median_cs_size', 'max_z': 'median_max_z', 'lbf_ser': 'median_lbf_ser'}).reset_index()\ndf2\n\n\n    rho  mean_covered  median_cs_size  median_max_z  median_lbf_ser\n0  0.02      0.913043            59.0      3.975982        3.440657\n1  0.04      0.976190            25.0      4.472989        4.269767\n2  0.06      0.959184             9.0      5.883980        9.222936\n3  0.08      1.000000             8.0      6.158600       10.446674\n4  0.10      0.980000             5.0      7.288703       16.028008\n5  0.20      1.000000             2.0     11.153625       43.252552\n6  0.40      0.980000             1.0     17.539280      111.461807\n7  0.60      1.000000             1.0     22.606627      186.874664\n8  0.80      1.000000             1.0     26.861099      277.643341\n9  1.00      1.000000             1.0     30.352369      381.481323"
  },
  {
    "objectID": "research/polynomial_regression_vb/index.html",
    "href": "research/polynomial_regression_vb/index.html",
    "title": "Implimenting polynomial approximation VB",
    "section": "",
    "text": "Code\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(pracma)\nlibrary(tictoc)\n\n\n\nAttaching package: 'tictoc'\n\n\nThe following objects are masked from 'package:pracma':\n\n    clear, size, tic, toc\n\n\nCode\nset.seed(1)"
  },
  {
    "objectID": "research/polynomial_regression_vb/index.html#computation",
    "href": "research/polynomial_regression_vb/index.html#computation",
    "title": "Implimenting polynomial approximation VB",
    "section": "Computation",
    "text": "Computation\n\nChebyshev approximations\nChebyshev polynomials can be used to approximate functions on the interval \\([-1, 1]\\) (and then also, on any finite interval). \\(f(x) \\approx \\sum c_k T_k(x)\\). We compute the coefficients of \\(c_k\\) for \\(k = 0, \\dots K\\).\nIn the code below we use pracma::polyApprox which implement a scheme of evaluate the coefficients for \\(f\\). This is essentially done via quadrature.\n\n\nCode\nmake_approximation <- function(f, R, n, plot=F){\n  p <- rev(pracma::polyApprox(f, -R, R, n =n)$p)\n  if(plot){\n    S <- R + 2\n    x <- seq(-S, S, by=0.1)\n    plot(f, -S, S)\n    lines(x, polyval2(p, x), col='red', lty='dotted')\n    abline(v=-R); abline(v=R)\n  }\n  return(p)\n}\n\n\n\n\nScaling and shifting polynomials\nCoefficients after a change of variable\n\\[\nf(bx) = \\sum m_k (bx)^k = \\sum m_k b^kx^k = \\sum (m_k b^k) x^k = f_b(x)\n\\]\n\\[\nf(x + c)\n= \\sum m_k (x + c)^k\n= \\sum_k m_k \\sum_{j \\leq k} {k \\choose j} x^j c^{k-j}\n= \\sum_j \\left(\\sum_{k \\geq j} {k \\choose j} c^{k-j}\\right) x^j\n\\]\n\n\nCode\n#' p coefficients of a polynomial in increasing order\n#' @param p polynomial coefficients in INCREASING deree\npolyval2 <- function(p, x){pracma::polyval(rev(p), x)}\n\n#' f(x + c) = f2(x)\nshift_polynomial <- function(p, c){\n  # construct map\n  K <- length(p) - 1\n  M <- matrix(nrow= K+1, ncol=K+1)\n  for(j in 0:K){\n    for(k in 0:K){\n      M[j+1, k+1] <- choose(k, j) * c**(k - j)\n    }\n  }\n  \n  coef_new <- (M %*% p)[, 1]\n  return(coef_new)\n}\n\n# change back to original scale\n# f(bx) = f2(x)\nscale_polynomial <- function(p, b){\n  K <- length(p) - 1\n  coef_new <- p * sapply(0:K, function(k) b**k)\n  return(coef_new)\n}\n\n\n\n\nLaplace approximation to polynomial\n\n\nCode\n#' convert (unnormalized) polynomial density to gaussian approximation\n#' p the coefficients of a polynomial in increasing order p = c(p0, p1, ..., pK)\npoly_to_gaussian <- function(p){\n  p <- rev(p)\n  d <- pracma::polyder(p)\n  d2 <- pracma::polyder(d)\n  \n  #f <- function(x){polyval2(p, x)}\n  #mu <- optimize(f, interval = c(-100, 100), maximum = T)$maximum\n  roots <- Re(pracma::polyroots(d)$root)\n  mu <- roots[which.max(pracma::polyval(p, roots))]\n  \n  var <- - 1 / pracma::polyval(d2, mu)\n  return(list(mu=mu, var=var))\n}\n\n\nTo test it out:\n\n\nCode\ncoef<- c(1, 2, 3)\ncoef2 <- shift_polynomial(coef, -1)\n#f(x-1) = f2(x)\n(polyval2(coef, 3) == polyval2(coef2, 4))\n\n\n[1] TRUE\n\n\nCode\ncoef<- c(1, 2, 3)\ncoef2 <- scale_polynomial(coef, 2)\n# f(2x) = f2(x)\n(polyval2(coef, 6) == polyval2(coef2, 3))\n\n\n[1] TRUE\n\n\n\n\nPlot functions\n\n\nCode\nplot_effect_posterior <- function(q, b, ...){\n  mu_post <- purrr::map_dbl(q, ~purrr::pluck(.x, 'mu'))\n  var_post <- purrr::map_dbl(q, ~purrr::pluck(.x, 'var'))\n  \n  plotrix::plotCI(\n    x = b,\n    y = mu_post,\n    li =  mu_post - 2 * sqrt(var_post),\n    ui =  mu_post + 2 * sqrt(var_post),\n    ...\n  )\n  abline(0, 1, col='red')\n}"
  },
  {
    "objectID": "research/polynomial_regression_vb/index.html#gaussian-linear-regression-mean-field-approximation",
    "href": "research/polynomial_regression_vb/index.html#gaussian-linear-regression-mean-field-approximation",
    "title": "Implimenting polynomial approximation VB",
    "section": "Gaussian linear regression, mean field approximation",
    "text": "Gaussian linear regression, mean field approximation\nHere we illustrate a simple example of our inference technique. We fit a mean field approximation to a bivariate regression problem\n\\[\n\\begin{aligned}\ny &\\sim N(\\sum_j x_j b_j, 1)\\\\\nb_j &\\sim N(0, 1)\\;\\; j = 1, \\dots, p\n\\end{aligned}\n\\]\nWhere \\(x_1\\) and \\(x_2\\) are correlated (inducing dependence in the posterior \\(p(b_1, b_2 | \\mathcal D)\\).\n\nPolynomial representation of Gaussian likelihood\nFor the observations\n\\[\nl(\\psi)\n= C -\\frac{1}{2\\sigma^2}(y - \\psi)^2\n= C -\\frac{1}{2\\sigma^2} y^2 + \\frac{y}{\\sigma^2}\\psi - \\frac{1}{2\\sigma^2}\\psi^2 \\implies {\\bf m}\n= (C -\\frac{y^2}{2 \\sigma^2}, \\frac{y}{\\sigma^2}, -\\frac{1}{2\\sigma^2})\n\\] For the prior\n\\[\n\\log p(b) = C -\\frac{1}{2\\sigma^2} \\left(b - \\mu \\right)^2\n\\implies {\\bf m} = \\left(C - \\frac{\\mu^2}{2\\sigma^2}, \\frac{\\mu}{\\sigma^2}, -\\frac{1}{2 \\sigma^2}\\right)\n\\]\n\n\nMean field variational approximation\n\\[\n\\begin{aligned}\nq({\\bf b}) = \\prod_j q(b_j) \\\\\nq(b_j) = N(\\mu_j, \\sigma^2_j)\n\\end{aligned}\n\\]\n\n\nComputing moment under \\(q\\)\n\\[\n\\mathbb E \\psi_j = \\mu_j x_j\n\\]\n\\[\n\\mathbb E \\psi_j^2 = (\\sigma^2_j + \\mu^2_j) x^2_j\n\\]\n\\[\n\\bar \\psi := \\mathbb E_q[\\psi] = \\sum \\mathbb E_q[\\psi_j]\n\\]\n\\[\n\\bar{\\psi^2} = \\mathbb E_q[\\psi^2] =  \\mathbb E_q[ \\left(\\sum \\psi_j \\right)^2] = Var(\\psi) + \\bar \\psi\n\\]\n\n\nCode\n#' write gaussian log density as polynomial \n#' return a vector c representing polynomial c[3]x^2 + c[2] x + c[1]\nmake_gaussian_coef <- function(y, var=1){\n  c(- 0.5 * y**2/var, y/var, -0.5 / var)\n}\n\n\n\n\nCode\n#' q a p-list of distributions for effect size of each column\n#' X a n x p matrix of covariate\n#' returns a list(mu, mu2) with the means and second moments\ncompute_moments <- function(X, q){\n  p <- ncol(X)\n  mu <- 0\n  var <- 0\n  for (j in 1:p){\n    mu <- mu + (X[,j] * q[[j]]$mu)\n    var <- var + (X[,j]**2 * q[[j]]$var)\n  }\n  mu2 <- var + mu**2\n  return(list(mu=mu, mu2=mu2))\n}\n\n#' compute coeeficient for EXPECTED shift\n#' f2(x) = E(f(x + c))\nshift_polynomial2 <- function(coef, mu, mu2){\n  c <- list(1, mu, mu2)\n  # construct map\n  K <- length(coef) - 1\n  M <- matrix(nrow= K+1, ncol=K+1)\n  for(j in 0:K){\n    for(k in 0:K){\n      M[j+1, k+1] <- choose(k, j) * c[[max(k-j+1, 1)]]\n    }\n  }\n  coef_new <- (M %*% coef)[, 1]\n  return(coef_new)\n}\n\n\n\n\nCode\nsublist <- function(list, j){\n  list[[j]] <- NULL\n  return(list)\n}\n\npolynomial_update2 <- function(m, X, prior_p, q){\n  p <- length(q)\n  n <- nrow(X)\n  for(j in 1:p){\n    # compute moments\n    # right now we just compute two moments, but need higher moments for \n    # higher degree polynomials\n    moments <- compute_moments(X[, -j, drop=F], sublist(q, j))\n    \n    # shift-- new polynomial in terms of \\psi_j\n    m2_tilde <- do.call(rbind, lapply(1:n, function(i) shift_polynomial2(\n      m[i,], moments$mu[i], moments$mu2[i])))\n    \n    # scale-- new polynomial in terms of b_j\n    m2_hat <- do.call(rbind, lapply(1:n, function(i) scale_polynomial(m2_tilde[i,], X[i, j])))\n    \n    # compute posterior polynomial\n    m2_post <- colSums(m2_hat) + prior_p[[j]]\n    \n    # find gaussian approximation\n    q[[j]] <- poly_to_gaussian(m2_post)\n  }\n  \n  return(q)\n}\n\n\n\n\nExample\n\n\nCode\nsimulate <- function(n=500, p=2, lenghtscale = 0.8, prior_variance=5){\n  Z <- matrix(rnorm(n*p), nrow=n)\n  K <- exp(-(outer(1:p, 1:p, '-')/lenghtscale)**2)\n  X <- Z %*% K\n  \n  b <- rnorm(p) * sqrt(prior_variance)\n  y <- (X %*% b)[, 1] + rnorm(n)\n  \n  return(list(y=y, X=X, b=b))\n}\n\nsim <- simulate(p=3)\ny <- sim$y\nX <- sim$X\nb <- sim$b\nprint(b)\n\n\n[1]  1.900755 -2.069063  1.998108\n\n\n\n\nCode\n# observations in polynomial coeeficients\nm <- do.call(rbind, lapply(y, make_gaussian_coef))\np <- ncol(X)\n\n# initialize prior and q\nq <- list()\nprior_p <- list()\nfor(j in 1:p){\n  prior_p[[j]] <- c(0, 0, -0.5)\n  q[[j]] <- list(mu = 0, var=1)\n}\n\n# iteratively update\nparam_history <- list()\nparam_history[[1]] <- q\nfor(i in 1:50){\n  q <- polynomial_update2(m, X, prior_p, q)\n  param_history[[i+1]] <- q\n}\n\nmu_post <- purrr::map_dbl(q, ~purrr::pluck(.x, 'mu'))\nvar_post <- purrr::map_dbl(q, ~purrr::pluck(.x, 'var'))\n\n\nHere we simulate a gaussian linear mode with three covariates. We plot the simulated effects against their posteror mean. It looks like we are able to recover the effects. Nice!\n\n\nCode\nplot_effect_posterior(q, sim$b)\n\n\n\n\n\n\n\nComparison to usual Bayesian computation\nTODO"
  },
  {
    "objectID": "research/polynomial_regression_vb/index.html#logistic-regression",
    "href": "research/polynomial_regression_vb/index.html#logistic-regression",
    "title": "Implimenting polynomial approximation VB",
    "section": "Logistic regression",
    "text": "Logistic regression\nNext we will take a quadratic approximation for logistic regression. We will also test higher degree polynomial approximations. In all cases we continue to use a Gaussian mean-field variational approximation. Computing the moments of the higher degree polynomials may be tricky, so we defer for now. However, if it can be done easily we may benefit from the richer varitional approximation (note: the Gaussian distribution is a degree 2 polynomial exponential family since it’s sufficient statistics are \\(T(x) = [x, x^2]\\))\n\nPolynomial approximation to log likelihood\nFor each data point we want to approximate the log likelihood as a function of the linear predictor\n\\[\n\\log p(y | \\psi) = y\\psi + \\log \\sigma(-\\psi)\n\\]\nHere we plot the \\(\\log p(y=0 | \\psi)\\) and it’s polynomial approximations of degree \\(k=2,4,6,8\\).\nThese polynomial approximations are generated using pracma::polyApprox which use the Chebyshev coefficients of an appropriately rescaled version of the function, to generate a polynomial approximation on the interval \\([a, b]\\). We generate approximations on the interval \\([-R, R]\\) where \\(R = 3\\).\n\n\nCode\n# loglike functions for y=1 and y=0\nloglik1 <- function(psi){\n  psi + log(sigmoid(-psi))\n}\nloglik0 <- function(psi){\n  log(sigmoid(-psi))\n}\n\n# polynomial approximation via pracma\nR <- 3\nll0_p2 <- rev(pracma::polyApprox(loglik0, -R, R, n = 2)$p)\nll0_p4 <- rev(pracma::polyApprox(loglik0, -R, R, n = 4)$p)\nll0_p6 <- rev(pracma::polyApprox(loglik0, -R, R, n = 6)$p)\nll0_p8 <- rev(pracma::polyApprox(loglik0, -R, R, n = 8)$p)\n\n# note: ll0 and ll1 are just reflections over the x axis\n# so we can get ll1 by taking ll0_p2 and flipping the sign of the linear term\nll1_p2 <- rev(pracma::polyApprox(loglik1, -R, R, n = 2)$p)\nll1_p4 <- rev(pracma::polyApprox(loglik1, -R, R, n = 4)$p)\nll1_p6 <- rev(pracma::polyApprox(loglik1, -R, R, n = 6)$p)\nll1_p8 <- rev(pracma::polyApprox(loglik1, -R, R, n = 8)$p)\n\nS <- R + 2\nx <- seq(-S, S, by=0.1)\nplot(loglik0, -S, S)\nlines(x, polyval2(ll0_p2, x), col='red', lty='dotted')\nlines(x, polyval2(ll0_p4, x), col='blue', lty='dotted')\nlines(x, polyval2(ll0_p6, x), col='green', lty='dotted')\nlines(x, polyval2(ll0_p8, x), col='orange', lty='dotted')\nabline(v=R); abline(v=-R)\n\n\n\n\n\n\n\nApproximate date likelihood\nThis just takes a whoe list of vector \\(y\\) and returns a matrix of coefficients\n\n\nCode\n#' get approximate polynomial representation of the data y\nbernoulli_poly_approx <- function(y, R, k){\n  n <- length(y)\n  p0 <- make_approximation(loglik0, R, k)\n  \n  # for y=1 flip the sign of odd coefficients (note: 0 indexing)\n  p1 <- p0\n  p1[seq(2, length(p0), by=2)] <- p1[seq(2, length(p0), by=2)] * -1\n  \n  m <- matrix(nrow = n, ncol = k + 1)\n  for(i in 1:length(y)){\n    if(y[i] == 0){\n      m[i,] <- p0\n    } else{\n      m[i,] <- p1\n    }\n  }\n  return(m)\n}\n\n\n\n\nSimulate logistic regression\n\n\nCode\nsimulate_lr <- function(n=500, p=2, lenghtscale = 0.8, prior_variance=5){\n  Z <- matrix(rnorm(n*p), nrow=n)\n  K <- exp(-(outer(1:p, 1:p, '-')/lenghtscale)**2)\n  X <- Z %*% K\n  \n  b <- rnorm(p) * sqrt(prior_variance)\n  logits <- (X %*% b)[, 1]\n  y <- rbinom(length(logits), 1, sigmoid(logits))\n  return(list(y=y, X=X, b=b, logits=logits))\n}\n\n\n\n\nApproximation with \\(k=2\\)\nWe can reuse our code above, substituting in new “data”, the coefficients for the polynomial approximation to the conditional likelihood.\n\n\nCode\nlogistic_polynomial_approximation_k2 <- function(y, X, R){\n  # observations in polynomial coeeficients\n  m <- bernoulli_poly_approx(y, R, k=2)\n  p <- ncol(X)\n  \n  # initialize prior and q\n  q <- list()\n  prior_p <- list()\n  for(j in 1:p){\n    prior_p[[j]] <- c(0, 0, -0.5)\n    q[[j]] <- list(mu = 0, var=1)\n  }\n  \n  # iteratively update\n  param_history <- list()\n  param_history[[1]] <- q\n  for(i in 1:50){\n    q <- polynomial_update2(m, X, prior_p, q)\n    param_history[[i+1]] <- q\n  }\n  return(param_history)\n}\n\n\nA 2d approximation does not seem to perform very well here.\n\n\nCode\nset.seed(5)\nn <- 500\np <- 3\nlenghtscale <- 0.8\nprior_variance <- 5\n\nZ <- matrix(rnorm(n*p), nrow=n)\nK <- exp(-(outer(1:p, 1:p, '-')/lenghtscale)**2)\nX <- Z %*% K\n\nb <- rnorm(p) * sqrt(prior_variance)\nlogits <- (X %*% b)[, 1]\ny <- rbinom(length(logits), 1, sigmoid(logits))\nprint(b)\n\n\n[1] 3.3319418 0.3807557 5.4429687\n\n\nCode\nqs <- logistic_polynomial_approximation_k2(y, X, R=5)\nq <- tail(qs, 1)[[1]]\nmu_post <- purrr::map_dbl(q, ~purrr::pluck(.x, 'mu'))\nvar_post <- purrr::map_dbl(q, ~purrr::pluck(.x, 'var'))\n\nplotrix::plotCI(\n  x = b,\n  y = mu_post,\n  li =  mu_post - 2 * sqrt(var_post),\n  ui =  mu_post + 2 * sqrt(var_post)\n)\nabline(0, 1, col='red')\n\n\n\n\n\n\nTesting a range of interval widths\nWe can check a few values of \\(R\\). There is a tradeoff here of course– the wider the interval we try to approximate, the worse the approximation will be. But if the interval is too narrow, the polynomial approximate likelihood essentially does not support data that fall far outside the interval. This is because we require the highest odd degree coefficient of our polynomial to be \\(<0\\) otherwise the likelihood grows unbounded outside the interval, and the approximation is not integrable.\n\n\n\n\n\nCode\npar(mfrow = c(1, 3))\nplot_effect_posterior(q_R3, b, main='R=3')\nplot_effect_posterior(q_R5, b, main='R=5')\nplot_effect_posterior(q_R7, b, main='R=7')\n\n\n\n\n\n\n\n\nHigher degree approximations\nNow we will extend our implementation above to handle higher degree approximations. This involves computing higher moments of the effect predictions, which isn’t too hard for Gaussian distributions.\n\n\nCode\n#' Make shift matrix\n#' \n#' Generate matrix that maps coefficients of a polynomial\n#' f(x + y) (represented by coefficients p) to coefficients of\n#' f2(x) = E_{p(y)}[f(x+y)]\n#' @param moments moments of y\nmake_shift_matrix <- function(moments){\n  # construct map\n  K <- length(moments) - 1\n  M <- matrix(nrow= K+1, ncol=K+1)\n  for(j in 0:K){\n    for(k in 0:K){\n      M[j+1, k+1] <- choose(k, j) * moments[[max(k-j+1, 1)]]\n    }\n  }\n  return(M)\n}\n\n#' Transform coefficients of a polynomial f(x + y) (represented by coefficients p)\n#' to coefficients of f2(x) = E_{p(y)}[f(x+y)]\n#' @param p K+1 coefficients of a degree-k polynomial\n#' @param moments moments of y (including E[y^0] = 1)\nshift_polynomial3 <- function(p, moments){\n  M <- make_shift_matrix(moments)\n  p_new <- (M %*% p)[, 1]\n  return(p_new)\n}\n\ncompute_normal_moments <- function(mu, var, k){\n  return(purrr::map_dbl(0:k, ~actuar::mnorm(.x, mu, sqrt(var))))\n}\n\n#' compute k moments for psi = xb, b ~ N(mu, var)\ncompute_psi_moments <- function(x, mu, var, k){\n  normal_moments <- compute_normal_moments(mu, var, k)\n  psi_moments <- do.call(cbind, purrr::map(0:k, ~ (x**.x) * normal_moments[.x + 1]))\n}\n\n#' update q with polynomial approximation of arbitrary degree\npolynomial_update3 <- function(m, X, prior_p, q){\n  K <- ncol(m) - 1\n  p <- ncol(X)\n  n <- nrow(X)\n  for(j in 1:p){\n    m_tilde <- m\n    for(k in (1:p)[-j]){\n      moments <- compute_psi_moments(X[, k], q[[k]]$mu, q[[k]]$var, K)\n      m_tilde <- do.call(rbind, lapply(1:n, function(i) shift_polynomial3(\n        m_tilde[i,], moments[i,])))\n    }\n    \n    # scale-- new polynomial in terms of b_j\n    m_hat <- do.call(rbind, lapply(1:n, function(i) scale_polynomial(\n      m_tilde[i,], X[i, j])))\n    \n    # compute posterior polynomial\n    m_post <- colSums(m_hat) + prior_p[[j]]\n    \n    # find gaussian approximation\n    q[[j]] <- poly_to_gaussian(m_post)\n    q[[j]]$m_post <- m_post\n  }\n  \n  return(q)\n}\n\nlogistic_polynomial_approximation <- function(y, X, R, K=2){\n  # observations in polynomial coeeficients\n  m <- bernoulli_poly_approx(y, R, K)\n  q <- list()\n  prior_p <- list()\n  for(j in 1:p){\n    prior_p[[j]] <- c(c(0, 0, -0.5), rep(0, K-2)) # extend polynomial to agree with m\n    q[[j]] <- list(mu = 0, var=1) # initialize normal posterior\n  }\n  \n  # iteratively update\n  param_history <- list()\n  param_history[[1]] <- q\n  for(i in 1:50){\n    q <- polynomial_update3(m, X, prior_p, q)\n    param_history[[i+1]] <- q\n  }\n  return(param_history)\n}\n\n\n\n\nCompare implimentations\nThe two implimentations agree!\n\n\nCode\nset.seed(12)\nsim <- simulate_lr(p=3)\ny <- sim$y\nX <- sim$X\nb <- sim$b\n\nqs_2 <- logistic_polynomial_approximation_k2(y, X, R=10)\nqs_k2 <- logistic_polynomial_approximation(y, X, R=10, K=2)\n\npar(mfrow=c(1,2))\nplot_effect_posterior(qs_2[[51]], b, main='A')\nplot_effect_posterior(qs_k2[[51]], b, main='B')\n\n\n\n\n\n\n\nHow does the posterior approximation change as we increase degree?\nWe approximate the likelihood on \\([-10, 10]\\) with \\(K=2,6,10,14\\). Note the polynomial approximation cannot be e.g. degree \\(4\\) because these polynomials are unbounded above (\\(c_4 >0\\)), so \\(e^ \\hat f(x)\\) is not integrable over the real line. But \\(c_K < 0\\) for approximations of degree \\(K = 2z + 2\\) for \\(z \\in \\mathbb N\\).\nIn this example it seems that with \\(K=2\\) we tend to over-estimate the effect size, but this is resolved in the higher degree approximations.\n\n\nCode\nset.seed(12)\nsim <- simulate_lr(p=3)\ny <- sim$y\nX <- sim$X\nb <- sim$b\n\nqs_k2 <- logistic_polynomial_approximation(y, X, R=10, K=2)\nqs_k6 <- logistic_polynomial_approximation(y, X, R=10, K=6)\nqs_k10 <- logistic_polynomial_approximation(y, X, R=10, K=10)\nqs_k14 <- logistic_polynomial_approximation(y, X, R=10, K=14)\n\npar(mfrow=c(2,2))\nplot_effect_posterior(qs_k2[[51]], b, main='K=2')\nplot_effect_posterior(qs_k6[[51]], b, main='K=6')\nplot_effect_posterior(qs_k10[[51]], b, main='K=10')\nplot_effect_posterior(qs_k14[[51]], b, main='K=14')"
  },
  {
    "objectID": "research/polynomial_regression_vb/index.html#poisson-regression-example",
    "href": "research/polynomial_regression_vb/index.html#poisson-regression-example",
    "title": "Implimenting polynomial approximation VB",
    "section": "Poisson regression example",
    "text": "Poisson regression example\n\nImpliment\n\n\nCode\npoisson_ll <- function(y){\n  f <- function(psi) dpois(y, exp(psi), log = T)\n  return(f)\n}\n\npoisson_approx <- function(y, R, k, as_function=F){\n  f <- poisson_ll(y)\n  p <- make_approximation(f, R, k, plot = F)\n\n  if(as_function){\n    p2 <- function(x) polyval2(p, x)\n    return(p2)\n  }\n  return(p)\n}\n\npoisson_poly_approx <- function(y, R, k){\n  unique_counts <- unique(y)\n  \n  polynomials <- list()\n  for(yy in unique_counts){\n    polynomials[[yy + 1]] <- poisson_approx(yy, R, k)\n  }\n  \n  m <- do.call(rbind, purrr::map(y, ~polynomials[[.x + 1]]))\n  return(m)\n}\n\npoisson_regression_polynomial_approximation <- function(y, X, R, K=2){\n  # observations in polynomial coeeficients\n  tic()\n  m <- poisson_poly_approx(y, R, K)\n  p <- ncol(X)\n  q <- list()\n  prior_p <- list()\n  for(j in 1:p){\n    prior_p[[j]] <- c(c(0, 0, -0.5), rep(0, K-2)) # extend polynomial to agree with m\n    q[[j]] <- list(mu = 0, var=1) # initialize normal posterior\n  }\n  \n  # iteratively update\n  param_history <- list()\n  param_history[[1]] <- q\n  for(i in 1:50){\n    q <- polynomial_update3(m, X, prior_p, q)\n    param_history[[i+1]] <- q\n  }\n  toc()\n  return(param_history)\n}\n\n\n\n\nVisualize approximation\nWe approximation \\(f(\\psi) = Pois(y=3; \\lambda = e^\\psi)\\) using polynomials of increasing degree on the interval \\([-5, 5]\\)\n\n\nCode\nf <- poisson_ll(3)\n\n# polynomial approximation via pracma\nR <- 5\nf_k2 <- rev(pracma::polyApprox(f, -R, R, n = 2)$p)\nf_k4 <- rev(pracma::polyApprox(f, -R, R, n = 4)$p)\nf_k8 <- rev(pracma::polyApprox(f, -R, R, n = 8)$p)\nf_k16 <- rev(pracma::polyApprox(f, -R, R, n = 16)$p)\n\nS <- R + 2\nx <- seq(-S, S, by=0.1)\nplot(f, -S, S)\nlines(x, polyval2(f_k2, x), col='red', lty='dotted')\nlines(x, polyval2(f_k4, x), col='blue', lty='dotted')\nlines(x, polyval2(f_k8, x), col='green', lty='dotted')\nlines(x, polyval2(f_k16, x), col='orange', lty='dotted')\nabline(v=R); abline(v=-R)\n\n\n\n\n\n\n\nSimulate poisson regression\n\n\nCode\nsimulate_poisson_regression <- function(n=500, p=2, lenghtscale = 0.8, prior_variance=1){\n  Z <- matrix(rnorm(n*p), nrow=n)\n  K <- exp(-(outer(1:p, 1:p, '-')/lenghtscale)**2)\n  X <- Z %*% K\n  \n  b <- rnorm(p) * sqrt(prior_variance)\n  logrates <- (X %*% b)[, 1]\n  y <- rpois(length(logrates), exp(logrates))\n  return(list(y=y, X=X, b=b, logrates=logrates))\n}\n\nsim <- simulate_poisson_regression(p=4, prior_variance = 1)\ny <- sim$y\nX <- sim$X\nb <- sim$b\nprint(b)\n\n\n[1] -1.3296046  0.6149598  0.7740477  0.2643521\n\n\n\n\nExample\n\n\nCode\npois_R5_K2 <- poisson_regression_polynomial_approximation(y, X, R=5, K=2)\n\n\n6.152 sec elapsed\n\n\nCode\npois_R5_K4 <- poisson_regression_polynomial_approximation(y, X, R=5, K=4)\n\n\n9.306 sec elapsed\n\n\nCode\npois_R5_K8 <- poisson_regression_polynomial_approximation(y, X, R=5, K=8)\n\n\n20.818 sec elapsed\n\n\nCode\npois_R5_K16 <- poisson_regression_polynomial_approximation(y, X, R=5, K=16)\n\n\n61.81 sec elapsed\n\n\nCode\npar(mfrow=c(2,2))\nplot_effect_posterior(pois_R5_K2[[51]], b, main='K=2')\nplot_effect_posterior(pois_R5_K4[[51]], b, main='K=4')\nplot_effect_posterior(pois_R5_K8[[51]], b, main='K=8')\nplot_effect_posterior(pois_R5_K16[[51]], b, main='K=16')\n\n\n\n\n\n\n\nCode\nplot_effect_posterior(pois_R5_K2[[51]], sim$b)\n\n\n\n\n\nCode\npurrr::map_dbl(pois_R5_K2[[51]], ~purrr::pluck(.x, 'mu'))\n\n\n[1] -1.51427519  0.50936054  0.92844494  0.08298042"
  },
  {
    "objectID": "research/polynomial_regression_vb/index.html#higher-degree-polynomial-posterior-q",
    "href": "research/polynomial_regression_vb/index.html#higher-degree-polynomial-posterior-q",
    "title": "Implimenting polynomial approximation VB",
    "section": "Higher degree polynomial posterior \\(q\\)",
    "text": "Higher degree polynomial posterior \\(q\\)\nIn the above implementation we compute a polynomial proportional to the posterior density, but reduce this to a Gaussian approximation by taking a Laplace approximation.\nThat is we want to compute\n\\[\n\\mathbb E [b^j] = \\int_{\\mathbb R} b^j q(b)\n\\]\nWhich we approximate by\n\\[\n\\mathbb E [b^j] \\approx \\int_{\\mathbb R} b^j q_{\\text{gauss}}(b)\n\\]\nWhere \\(q_{\\text{gauss}}\\) is the Gaussian distribution that minimizes the divergence to \\(q_{\\text{gauss}} = \\arg \\min_{q_g} KL(q_g ||q)\\).\nIt would be better if we could compute the moments of \\(q(b_l) \\propto \\exp\\{\\sum_k \\eta_k b_l^k\\}\\). We define the log normalizing constant \\(A({\\bf \\eta}) = \\log \\int \\exp\\{\\sum_{k=0}^K \\eta_k b_l^k\\} db\\). Let \\(f(b) = \\sum_{k=0}^K \\eta_k b^k - A(\\eta)\\) so that \\(q(b) = \\exp\\{f(b)}\\).\nBecause \\(q\\) is in an exponential family, we know that \\(\\nabla_{\\eta} A(\\eta) = \\mathbb E[T(x)] = [1, \\mathbb E[b], \\mathbb E[b^2], \\dots, \\mathbb E[b^K]]\\). Is there an easy way to compute the gradient of \\(A\\)?"
  },
  {
    "objectID": "research/gseasusie_working_example/index.html",
    "href": "research/gseasusie_working_example/index.html",
    "title": "Minimal working example for GSEA SuSiE",
    "section": "",
    "text": "This was supposed to be a minimal working example, but when Matthew ran it he got different results than me. When I ran it again this morning I got results that agreed with his. So now this document is meant to diagnose what went wrong!\nThe MLE for the univariate doesn’t exist when the observations. When \\(X\\) is binary (e.g. in GSEA) if all of the genes in the gene set are in the gene list we can always improve the likelihood by increasing the effect size. Similarly if none of the genes in the gene set are in the gene list, we can always improve the likelihood by decreasing the effect size. When we fit the GLM in these cases using fastglm it will return a large effect size (e.g. \\(\\hat \\beta = 14\\)) and very large standard error \\(\\hat s \\approx 20,000\\). Consequently we get a \\(z\\)-score close to zero, but the reported likelihood ratio (LR) can be very large, and may depends on the stopping criteria from the GLM fitting procedure. Also note that when \\(y_i = 0 \\;\\forall i \\text{ s.t. } x_i=0\\) the data are completely separable and the LR becomes unbounded. However, more often in our setting we will have that there is a limiting value for the LR given by the likelihood of the data in the intercept only model excluding the data where \\(x_i=1\\) and including the observations where \\(x_i = 1\\) (I thinks this is correct, but I need to check).\n\nFailing example\n\n\nCode\nlibrary(tictoc)\n\n# devtools::install_github('karltayeb/logisticsusie')\n# devtools::install_github('karltayeb/gseasusie')\n\n##### Minimal working example\n\nf <- paste0('', Sys.glob('cache/resources/C1*')) # clear cash so it can knit\nif(file.exists(f)){file.remove(f)}\n\n\n[1] TRUE\n\n\nCode\nc1 <- gseasusie::load_msigdb_geneset_x('C1')\n\n\nloading gene set from msigdbr: C1\n\n\nAdding missing grouping variables: `geneSet`\n\n\nCode\n# sample random 5k background genes\nset.seed(0)\nbackground <- sample(rownames(c1$X), 5000)\n\n# sample GSs to enrich, picked b0 so list is ~ 500 genes\nenriched_gs <- sample(colnames(c1$X), 3)\nb0 <- -2.2\nb <- 3 *abs(rnorm(length(enriched_gs)))\nlogit <- b0 + (c1$X[background, enriched_gs] %*% b)[,1]\ny1 <- rbinom(length(logit), 1, 1/(1 + exp(-logit)))\nlist1 <- background[y1==1]\n\n# gene set matrix restricted to background, keep non-empty gene sets\nX <- c1$X[background,]\nX <- X[, Matrix::colSums(X) > 1]\n\n\n\n\nCode\n# GIBSS fit\ntic()\nfit <- logisticsusie::generalized_ibss(X, y1, L=10, estimate_prior_variance = F, maxit=10)\n\n\n48.291 sec elapsed\n\n\nCode\ntoc()\n\n\n48.306 sec elapsed\n\n\n\n\nCode\nfit$prior_variance\n\n\n L1  L2  L3  L4  L5  L6  L7  L8  L9 L10 \n  1   1   1   1   1   1   1   1   1   1 \n\n\nCode\nfit$cs\n\n\n$L1\nCS = {66} \n alpha = {1} \n size = 1, coverage = 1, requested = 0.95\n$L2\nCS = {66} \n alpha = {1} \n size = 1, coverage = 1, requested = 0.95\n$L3\nCS = {66} \n alpha = {1} \n size = 1, coverage = 1, requested = 0.95\n$L4\nCS = {66} \n alpha = {1} \n size = 1, coverage = 1, requested = 0.95\n$L5\nCS = {66} \n alpha = {1} \n size = 1, coverage = 1, requested = 0.95\n$L6\nCS = {66} \n alpha = {1} \n size = 1, coverage = 1, requested = 0.95\n$L7\nCS = {66} \n alpha = {1} \n size = 1, coverage = 1, requested = 0.95\n$L8\nCS = {66} \n alpha = {1} \n size = 1, coverage = 1, requested = 0.95\n$L9\nCS = {66} \n alpha = {1} \n size = 1, coverage = 1, requested = 0.95\n$L10\nCS = {66} \n alpha = {1} \n size = 1, coverage = 1, requested = 0.95\n\n\n\n\nData augmenation\nWe append \\((1, 0, 1, 0)\\) to the end of \\(y\\) and \\((a, a, 0, 0)\\) to each column of \\(X\\). We show the augmentation strategy for \\(a = 1, 10\\). Perhaps a better alternative is to add a very small \\(l_2\\) penalty to the effect size \\(\\beta\\). It turns out in this simulation 2 of the causal gene sets are completely enriched.\n\n\nCode\naugment_binary_data <- function(X, y, xval=1){\n  p <- ncol(X)\n\n  Xaug <- rbind(X, matrix(rep(c(xval, xval, 0, 0), p), nrow=4))\n  yaug <- c(y, c(1, 0, 1, 0))\n  return(list(X=Xaug, y = yaug))\n}\n\npar(mfrow=c(1, 3))\n\ncausal_idx <- which(colnames(X) %in% enriched_gs)\nbad_points <- (Matrix::t(X) %*% y1)[, 1] == 0 #(z < 0.001) & (serfit$lr > 1)\ngood_points <- !bad_points\n\nserfit <- logisticsusie::fit_glm_ser(X, y1, estimate_prior_variance = T)\nlabf <- with(serfit, logisticsusie:::compute_log_labf(betahat, shat2, lr, 1.))\nz <- serfit$betahat/sqrt(serfit$shat2)^2\n\nplot(labf[good_points], z[good_points], xlim=range(labf), ylim=range(z), \n     xlab='logBF (Laplace Approximation)', ylab = 'z-score', main='Original Data')\npoints(labf[bad_points], z[bad_points], col='red')\npoints(labf[causal_idx], z[causal_idx], col='blue', pch = 22)\n\n\naugmented_data <- augment_binary_data(X, y1, 1.)\nserfitaug <- with(augmented_data, logisticsusie::fit_glm_ser(X, y, estimate_prior_variance=T))\nlabf <- with(serfitaug, logisticsusie:::compute_log_labf(betahat, shat2, lr, 1.))\nz <- with(serfitaug, betahat/sqrt(shat2)^2)\nplot(labf[good_points], z[good_points], xlim=range(labf), ylim=range(z), \n     xlab='logBF (Laplace Approximation)', ylab = 'z-score', main='Augmented Data, a=1')\npoints(labf[bad_points], z[bad_points], col='red')\npoints(labf[causal_idx], z[causal_idx], col='blue', pch = 22)\n\n\naugmented_data <- augment_binary_data(X, y1, 10.)\nserfitaug <- with(augmented_data, logisticsusie::fit_glm_ser(X, y, estimate_prior_variance=T))\nlabf <- with(serfitaug, logisticsusie:::compute_log_labf(betahat, shat2, lr, 1.))\nz <- with(serfitaug, betahat/sqrt(shat2)^2)\nplot(labf[good_points], z[good_points], xlim=range(labf), ylim=range(z), \n     xlab='logBF (Laplace Approximation)', ylab = 'z-score', main='Augmented Data, a=10')\npoints(labf[bad_points], z[bad_points], col='red')\npoints(labf[causal_idx], z[causal_idx], col='blue', pch = 22)\n\n\n\n\n\n\n\nGIBSS with data augmentation\n\n\nCode\n# GIBSS fit\ntic()\naugmented_data <- logisticsusie::augment_binary_data(X, y1, 1.)\nfit <- with(augmented_data, logisticsusie::generalized_ibss(X, y, L=5, estimate_prior_variance=T, maxit=10))\n\n\nWarning in ibss_from_ser(X, y, L = L, ser_function = ser_fun, ...): Maximum\nnumber of iterations reached\n\n\n108.749 sec elapsed\n\n\nCode\nfit$prior_variance\n\n\n      L1       L2       L3       L4       L5 \n38.32500 28.67109 81.14890  0.00000  0.00000 \n\n\nCode\ntoc()\n\n\n109.752 sec elapsed\n\n\n\n\nCode\ncausal_idx <- which(colnames(X) %in% enriched_gs)\nprint(causal_idx)\n\n\n[1]  16  66 194\n\n\nCode\nfit$cs\n\n\n$L1\nCS = {66} \n alpha = {1} \n size = 1, coverage = 1, requested = 0.95\n$L2\nCS = {16} \n alpha = {1} \n size = 1, coverage = 1, requested = 0.95\n$L3\nCS = {67, 68, 75, 239, 106, 14, ...} \n alpha = {0.11, 0.09, 0.04, 0.04, 0.03, 0.03, ...} \n size = 62, coverage = 0.95, requested = 0.95\n$L4\nCS = {107, 9, 133, 131, 75, 239, ...} \n alpha = {0.01, 0, 0, 0, 0, 0, ...} \n size = 275, coverage = 0.952, requested = 0.95\n$L5\nCS = {107, 9, 133, 131, 75, 239, ...} \n alpha = {0.01, 0, 0, 0, 0, 0, ...} \n size = 275, coverage = 0.952, requested = 0.95\n\n\n\n\nCode\nall(fit$cs$L3$cs %in% which(bad_points))\n\n\n[1] TRUE\n\n\n\nWhen the estimated prior variance is \\(0\\) the Laplace approximation of the BF reduced to the LR, so the variables are just ranked by their LR.\nWe estimate a very large prior variance of the third effect, but the CS is very diffuse. The 3rd effect includes all of the other completely enriched gene sets.\n\n\n\nCode\nknitr::knit_exit()"
  },
  {
    "objectID": "research/logistic_ser_comparisons/indx.html",
    "href": "research/logistic_ser_comparisons/indx.html",
    "title": "Logistic SER comparison",
    "section": "",
    "text": "VEB.Boost: from VEB.Boost package\nJaakkola-Jodan approximation logisticsusie::binser\nUnivariate Jaakkola-Jordan approximation logisticsusie::uvbser\nLaplace approximation"
  },
  {
    "objectID": "research/logistic_ser_comparisons/indx.html#discrepency-between-vebboost-and-my-implementation",
    "href": "research/logistic_ser_comparisons/indx.html#discrepency-between-vebboost-and-my-implementation",
    "title": "Logistic SER comparison",
    "section": "Discrepency between VEBBoost and my implementation",
    "text": "Discrepency between VEBBoost and my implementation\n\n\nCode\n# JJ approximation-- VEB boost\ntic()\nveb_ser <- fit_logistic_susie_veb_boost(X, y1, L=1)\nlogisticsusie::compute_cs(veb_ser$alpha[1,])\ntoc()\n\n# JJ approximation other implementation\ntic()\njj_ser <- logisticsusie::binser(X, y1, estimate_prior_variance = F, prior_variance=veb_ser$prior_variance)\nlogisticsusie::compute_cs(jj_ser$alpha)\ntoc()\n\n# veb and my implementation agree \npar(mfrow = c(1, 2))\nplot(veb_ser$mu, jj_ser$mu); abline(0, 1, col='red')\nplot(log(veb_ser$alpha), log(jj_ser$alpha)); abline(0, 1, col='red')"
  },
  {
    "objectID": "research/logistic_ser_comparisons/indx.html#laplace",
    "href": "research/logistic_ser_comparisons/indx.html#laplace",
    "title": "Logistic SER comparison",
    "section": "Laplace",
    "text": "Laplace\nNote that when \\(\\sigma^2 \\rightarrow 0\\) the Laplace approximation of the BF converges to the likelihood ratio between the MLE and null model, which is \\(\\geq 1\\).\n\n\nCode\n# SER fit with Laplace\nserfit <- logisticsusie::generalized_ibss(X, y1, L=1, estimate_prior_variance = F, maxit=1)\n\ntic()\nserfit2 <- logisticsusie::generalized_ibss(X[, 1:100], y1, L=1, estimate_prior_variance = F, maxit=1)\nserfiteb <- logisticsusie::generalized_ibss(X[, 1:100], y1, L=1, estimate_prior_variance = T, maxit=1)\n\nuvbserfit2 <- logisticsusie::uvbser(X[, 1:100], y1, estimate_prior_variance = T, tol=1e-2)\nuvbserfit <- logisticsusie::uvbser(X[, 1:100], y1, estimate_prior_variance = F)\n\ntoc()\n\nplot(veb_ser$mu, serfit$mu); abline(0, 1, col='red')\n\n# JJ approximation other implementation\ntic()\njj_ser2 <- logisticsusie::binser(X, y1, estimate_prior_variance = F, prior_variance=serfit$prior_variance)\nlogisticsusie::compute_cs(jj_ser2$alpha)\ntoc()\n\n# There is a difference between the GIBSS implementation and VB\npar(mfrow = c(1, 2))\nplot(serfit$mu, jj_ser2$mu); abline(0, 1, col='red')\nplot(log(serfit$alpha), log(jj_ser2$alpha)); abline(0, 1, col='red')\n\n# the ones that don't agree are completely separated-- the MLE does not exist.\n# these may also inflate the estimated prior variance the VB approach?\n# are are we underestimating the prior variance in the Laplace approximation?\nweird <- (abs(serfit$mu[1,]) < 0.001) * (abs(jj_ser2$mu) > 0.001)\nmax((Matrix::t(X[,weird]) %*% y1)[,1])\n\n\n# repeat with augmented data\nXaug <- rbind(X, matrix(rep(c(1, 1, 0, 0), ncol(X)), nrow=4))\ny1aug <- c(y1, c(1, 0, 1, 0))\n\n# JJ approximation-- VEB boost\ntic()\nveb_ser_aug <- fit_logistic_susie_veb_boost(Xaug, y1aug, L=1)\nlogisticsusie::compute_cs(veb_ser_aug$alpha[1,])\ntoc()\n\n# JJ approximation other implementation\ntic()\njj_ser_aug <- logisticsusie::binser(Xaug, y1aug, estimate_prior_variance = F, prior_variance=veb_ser$prior_variance)\nlogisticsusie::compute_cs(jj_ser_aug$alpha)\ntoc()\n\n# veb and my implementation agree \npar(mfrow = c(1, 2))\nplot(veb_ser_aug$mu, jj_ser_aug$mu); abline(0, 1, col='red')\nplot(log(veb_ser_aug$alpha), log(jj_ser_aug$alpha)); abline(0, 1, col='red')\n\n# SER fit with Laplace\nserfit_aug <- logisticsusie::generalized_ibss(Xaug, y1aug, L=1, estimate_prior_variance = F, maxit=1)\n\n# JJ approximation other implementation\ntic()\njj_ser2_aug <- logisticsusie::binser(Xaug, y1aug, estimate_prior_variance = F, prior_variance=serfit$prior_variance)\nlogisticsusie::compute_cs(jj_ser2_aug$alpha)\ntoc()\n\n# There is a difference between the GIBSS implementation and VB\npar(mfrow = c(1, 2))\nplot(serfit_aug$mu, jj_ser2_aug$mu); abline(0, 1, col='red')\nplot(log(serfit_aug$alpha), log(jj_ser2_aug$alpha)); abline(0, 1, col='red')"
  },
  {
    "objectID": "research/mode_matching_rss_vb/index.html",
    "href": "research/mode_matching_rss_vb/index.html",
    "title": "Mode seeking in mean field VB for RSS + sparse prior",
    "section": "",
    "text": "The RSS likelihood relates observed marginal effects to the unobserved effects of a joint model\n\\[\\begin{align}\n\\hat \\beta \\sim \\mathcal N(SRS^{-1} \\beta, SRS) \\\\\n\\beta \\sim g(\\cdot)\n\\end{align}\\]\nWhere we consider the problem of putting an i.i.d. prior on the entries of \\(\\beta\\) and using a mean field approximation for variational inference.\nSpecifically, we put a spike and slab prior on \\(\\beta_j = b_j\\gamma_j\\) for \\(j \\in [p]\\). Where \\(b_j \\sim N(0, \\sigma^2)\\) gives the distribution of non-zero effects, and and \\(\\gamma_j \\sim Bernoulli(\\pi)\\). That is, the effect is non-zero with probability \\(\\pi\\).\nThe problem we demonstrate, is that due to the mode matching behavior of the “reverse” KL divergence, which is minimized in variational inference, the posterior on \\(q(\\gamma_1, \\dots, \\gamma_p)\\) will tend to concentrate instead of accurately representing uncertainty. Furthermore, due to strong dependence among the posterior means.\nWe work with a simplified version of RSS assuming we observe \\(z\\)-scores \\(\\hat z\\).\n\\[\n\\begin{aligned}\n\\hat z &\\sim \\mathcal N(Rz, R) \\\\\nz_i &\\sim \\pi_0 \\delta_0 + \\pi_1 \\mathcal N(0, \\sigma^2)\n\\end{aligned}\n\\]\n\\[\nq(z, \\gamma) = \\prod_j q(z_j, \\gamma_j)\n\\]\n\\[\n\\begin{aligned}\nELBO(q_j) &= \\mathbb E_{q_{-j}} \\squarb{\\log p(\\hat z| z, R) + \\log p(z_j) - \\log q(b_l, \\gamma_l)} + H(q_l) \\\\\n&= \\hat z_j (b_j \\gamma_j) - \\frac{1}{2} \\left[ (b_j \\gamma_j)^2 + 2 (b_j \\gamma_j) \\sum_{i \\neq j} R_{ij} \\mathbb E_{q_{-j}} \\squarb{z_j} \\right] + \\log p(b_l | \\gamma_l) + \\log p(\\gamma_l) + H(q_l) + C\n\\end{aligned}\n\\]\nThen \\(q(b_l | \\gamma_l = 1) = N(\\frac{\\nu_j}{\\tau_j}, \\tau^{-1}_j)\\) Where \\(\\nu_j = \\hat z_j - \\sum_{i\\neq j} R_{ij} \\alpha_i \\mu_i\\), and \\(\\tau_j = 1 + \\sigma^{-2}_0\\).\nIt’s easy to see that the best choice for \\(q(b_l | \\gamma_l = 0)\\) is the prior, since all fo the data terms disappear, also noted here [(1)]\nAnd \\(q(\\gamma_j) = Bernoulli(\\alpha_j)\\), where \\(\\log \\left(\\frac{\\alpha_j}{1 - \\alpha_j}\\right) = \\hat z \\mu_j - \\frac{1}{2} \\left[\\mu^2_j + \\sigma^2_j + 2 \\mu_j \\sum_{i\\neq j} R_{ij} \\mu_i \\alpha_i \\right] + \\log\\left(\\frac{\\pi}{1 - \\pi}\\right)\\).\n\nSimulation\n\n\nCode\n#' @param q q(mu, var, alpha)\n#' @param R LD matrix-- assumes diag(R) = rep(1, p)\n#' @param tau0 prior effect variance\n#' @param prior_logit p-vector with prior log odds for gamma = 1\nrssvb <- function(zhat, q, R, tau0, prior_logit){\n  # unpack\n  mu <- q$mu\n  var <- q$var\n  alpha <- q$alpha\n\n  p <- length(zhat)\n  psi <- (R %*% (mu * alpha))[,1] # prediction\n  for(i in 1:p){\n    # remove effect of this variable\n    psi <- psi - R[i,] * (mu[i]*alpha[i])\n\n    # compute q(beta | gamma = 1)\n    nu <- zhat[i] - psi[i]\n    tau <- 1 + tau0\n    mu[i] <- nu/tau\n    var[i] <- 1/tau\n\n    # logit <- zhat[i] * mu[i]\n    #   - 0.5 * (psi[i] * mu[i] +  mu[i]^2 + var[i])\n    #   -0.5 * tau0 * (mu[i]^2 + var[i]) + prior_logit[i]\n    logit <- 0.5 * (mu[i]^2/var[i] + log(var[i]) + log(tau0)) + prior_logit[i]\n    alpha[i] <- 1/(1 + exp(-logit))\n\n    alpha[i]\n    psi <- psi + R[i,] * (mu[i]*alpha[i])\n  }\n  return(list(mu=mu, var=var, alpha=alpha))\n}\n\n\n\n\nCode\nsim_zscores <- function(n, p){\n  X <- logisticsusie:::sim_X(n=n, p = p, length_scale = 5)\n  R <- cor(X)\n  z <- rep(0, p)\n  z[10] <- 5\n  zhat <- (R %*% z)[,1] + mvtnorm::rmvnorm(1, sigma=R)[1,]\n  return(list(zhat = zhat, z=z, R=R))\n}\n\ninit_q <- function(p){\n  q = list(\n    mu = rep(0, p),\n    var = rep(1, p),\n    alpha = rep(1/p, p)\n  )\n  return(q)\n}\n\nrun_sim <- function(n = 100, p = 50, tau0=1, prior_logit = -3){\n  sim <- sim_zscores(n = n, p = p)\n  q <- init_q(p)\n  prior_logit <- rep(prior_logit, p)\n  for(i in 1:100){\n    q <- with(sim, rssvb(zhat, q, R, tau0, prior_logit))\n  }\n  \n  sim$q <- q\n  return(sim)\n}\n\n\nFor 100 independent simulations, we simulate \\(50\\) dependent \\(z\\)-scores. The true non-zero \\(z\\)-score is at index \\(10\\) with \\(\\mathbb E[\\hat z_{10}] = 5\\). However, over half the time, the VB approximation confidently selects another nearby feature.\n\n\nCode\nset.seed(10)\nsims <- purrr::map(1:100, ~run_sim(tau0=0.01))\nmax_idx <- purrr::map_int(1:100, ~which.max(sims[[.x]]$q$alpha))\n\nalpha10 <- purrr::map_dbl(1:100, ~sims[[.x]]$q$alpha[10])\nhist(alpha10)\n\n\n\n\n\nCode\ntable(max_idx)\n\n\nmax_idx\n 8  9 10 11 12 \n 4 47 36 11  2 \n\n\n\n\nMany small effects vs a few large effects\nThe interpretation of \\(\\sigma_0^2\\) depends a lot on how polygenic the trait is. Even though we only simulate one non-zero effect, if we use a prior \\(\\pi_1 >> 0\\) the model approaches a mean field approximation of ridge regression. Since ridge can estimate many small effects we get less shrinkage than if we enforce sparse architecture with \\(\\pi_1 \\approx 0\\).\n\n\nCode\nposterior_mean <- function(sim){\n  return((sim$R %*% (sim$q$mu * sim$q$alpha))[, 1])\n}\n\nshrinkage_plot <- function(sims, ...){\n  lims <- range(purrr::map(1:length(sims), ~sims[[.x]]$zhat))\n  plot(\n    sims[[1]]$zhat,\n    posterior_mean(sims[[1]]),\n    xlim = c(-4, 7),\n    ylim = c(-4, 7),\n    xlab = 'zhat',\n    ylab = 'posterior mean z',\n    ...\n  )\n  for(i in 1:100){\n    points(sims[[i]]$zhat, posterior_mean(sims[[i]]))\n  }\n  abline(0, 1, col='red')\n}\n\nset.seed(10)\n\nsim_sparse <- purrr::map(1:100, ~run_sim(tau0=0.1, prior_logit = -3))\nsim_poly <- purrr::map(1:100, ~run_sim(tau0=0.1, prior_logit = 3))\n\npar(mfrow=c(1,2))\nshrinkage_plot(sim_sparse, main='Sparse')\nshrinkage_plot(sim_poly, main='Polygenic')\n\n\n\n\n\n\n\n\n\n\nReferences\n\n1. Titsias MK, Lázaro-Gredilla M. Doubly Stochastic Variational Bayes for non-Conjugate Inference."
  },
  {
    "objectID": "research/polynomial_susie/index.html",
    "href": "research/polynomial_susie/index.html",
    "title": "Polynomial approximation SuSiE",
    "section": "",
    "text": "The sum of single effects (SuSiE) regression, is a regression with a SuSiE prior on the effects. For a Gaussian model with identity link, using a variational approximation that factorizes over single effects yields a fast coordinate ascent variational inference scheme which can be optimized by fitting a sequence of single effect regression (SERs). The computational simplicity of this result relies on (1) the variational approximation and (2) the fact the the log likelihood is quadratic in the regression coefficients.\nHere, we consider the more general case where the log-likelihood is a polynomial in the regression coefficients. We can approximate the log-likelihood of models with many choices of likelihood and link function with polynomials, so having a fast inference scheme for this case provides a uniform treatment of extensions of SuSiE to different observation models. We use the same variational approximation as in linear SuSiE. Approximating the log-likelihood as as polynomial is sufficient to"
  },
  {
    "objectID": "research/polynomial_susie/index.html#preliminaries",
    "href": "research/polynomial_susie/index.html#preliminaries",
    "title": "Polynomial approximation SuSiE",
    "section": "Preliminaries",
    "text": "Preliminaries\n\nPolynomial shift\nGiven the coefficients \\({\\bf a}\\) we would like to find the coefficients for the change of variable\n\\[\\phi_{\\bf a}(x + y) = \\phi_{{\\bf b}({\\bf a}, y)}(x)\\]\n\\[\n\\begin{aligned}\n\\phi_{\\bf a}(x + y)\n&= \\sum_{m=0}^M a_m(x + y)^m \\\\\n&= \\sum_{m=0}^M a_m \\sum_{k=0}^m {m \\choose k} x^k y^{m-k} \\\\\n&= \\sum_{k=0}^M \\left(\\sum_{m=k}^M a_m {m \\choose k}y^{m-k} \\right) x^k \\\\\n&= \\sum_{k=0}^M {\\bf b}({\\bf a}, y)_k x^k, \\quad  {\\bf b}({\\bf a}, y)_k := \\left(\\sum_{m=k}^M a_m {m \\choose k}y^{m-k} \\right)\n\\end{aligned}\n\\] Inspecting the expression for \\({\\bf b}({\\bf a}, y)_k\\) we can see that the new coefficient vector \\({\\bf b}({\\bf a}, y)\\) can be written compactly in matrix form\n\\[\n{\\bf b}({\\bf a}, y) = M(y) {\\bf a}, \\quad M_{ij} = {j \\choose i} y^{j -i}\\;  \\forall j \\geq i, 0 \\text{ otherwise}\n\\] Supposing \\(y\\) is random, we will also have need to compute the expected value of \\({\\bf b}({\\bf a}, y)\\). For \\(y \\sim p\\), \\(\\mathcal M(p) = \\mathbb E_p[M(y)]\\), we can compute these expected coefficients\n\\[\n{\\bf c}({\\bf a}, p) = \\mathcal M(p) {\\bf a}\n\\]\n\n\nPolynomial rescaling\n\\[\\phi_{\\bf a}(cx) = \\phi_{{\\bf d}({\\bf a}, c)}(x)\\]\n\\[\n\\sum_m a_m(c x)^m = \\sum_m (a_m c^m) x^m = \\sum_m {\\bf d}({\\bf a}, c)_m x^m\n\\]"
  },
  {
    "objectID": "research/polynomial_susie/index.html#model",
    "href": "research/polynomial_susie/index.html#model",
    "title": "Polynomial approximation SuSiE",
    "section": "Model",
    "text": "Model\n\\[\n\\begin{aligned}\ny_i | \\mu_i &\\sim f(\\mu_i, \\theta) \\\\\n\\mu_i &= g({\\bf x}^T_i \\beta ) \\\\\n\\beta &\\sim \\text{SuSiE}(L, \\{\\sigma_{0l}\\})\n\\end{aligned}\n\\] \\(f\\) is the observation model, parameterized by \\(\\mu\\) and \\(\\theta\\). \\(g\\) is a link function which maps the linear predictions \\(\\psi_i := {\\bf x}_i^T \\beta\\) to the parameter \\(\\mu_i\\).\n\nPolynomial approximation to the log-likelihood\nFor each observation \\(y\\), we can approximate the log-likelihood as a polynomial in the linear prediction. \\(f\\) is a polynomial of degree \\(M\\)\n\\[\n\\log p(y | \\psi) = \\log f(y | g(\\psi)) \\approx f(\\psi)\n\\]\n\\[\nf(\\psi) = \\sum_{m=0}^M a_m \\psi^m\n\\]\nThere are many ways to construct this approximation. At a high level, we want the approximation to be “good” (some measure of error between the approximate log-likelihood and the exact log-likelihood is small) at plausible values of \\(\\psi\\). So far, if have used a truncated Chebyshev series.\nWe write \\(f_i(\\psi)\\) as the polynomial approximation for \\(\\log p(y_i | \\psi)\\). We write it’s coefficients \\(\\left( a_m^{(i)} \\right)_{m \\in [M]}\\). To denote a polynomial with coefficients \\({\\bf a}\\) we will write \\(\\phi_{\\bf a}\\), e.g. \\(f(\\psi) = \\phi_{\\bf a}(\\psi)\\).\nWe have a polynomial approximation for each observations. Let \\({\\bf a}_i\\) denote the coefficients for observations \\(i\\). and \\(A\\) be the \\(n \\times m\\) matrix where each row corresponds to an observation."
  },
  {
    "objectID": "research/polynomial_susie/index.html#single-effect-regression-with-polynomial-approximation",
    "href": "research/polynomial_susie/index.html#single-effect-regression-with-polynomial-approximation",
    "title": "Polynomial approximation SuSiE",
    "section": "Single effect regression with polynomial approximation",
    "text": "Single effect regression with polynomial approximation\n\nSER prior\nThe SER prior \\(\\beta \\sim SER(\\sigma_0^2, \\pi)\\)\n\\[\n\\begin{aligned}\n\\beta = b\\gamma\\\\\nb \\sim N(0, \\sigma_0^2) \\\\\n\\gamma \\sim \\text{Mult}(1, \\pi)\n\\end{aligned}\n\\]\n\n\nPolynomial approximation for SER posterior\nThe SER follows from \\(p\\) univariate regressions\n\\[\n\\begin{aligned}\np(b | {\\bf y}, X, \\gamma=j, \\sigma^2_0)\n  &= p(b | {\\bf x}_j, {\\bf y}, \\sigma^2_0) \\\\\n  &\\propto p({\\bf y}, b| {\\bf x_j}, \\sigma^2_0) \\\\\n  &\\approx \\exp\\{ \\sum_i \\phi_{\\bf a_i}(x_{ij} b) + \\log p(b)\\}\n\\end{aligned}\n\\]\nSupposing \\(\\log p(b) \\approxeq \\phi_{\\bf \\rho}(b)\\) for some coefficients \\(\\rho\\), the unnormalized log posterior density can be written as a degree \\(M\\) polynomial with coefficients \\({\\bf f}(A, {\\bf x}_j, \\rho)\\) defined below:\n\\[\n\\begin{aligned}\n\\sum_i \\phi_{\\bf a_i}(x_{ij} b) + \\phi_{\\bf \\rho}(b) &= \\sum_i \\phi_{{\\bf d}({\\bf a}_i, x_{ij})}(b) + \\phi_{\\bf \\rho}(b), \\\\\n&= \\phi_{{\\bf f}(A, {\\bf x}_j, \\rho)}(b), \\quad {\\bf f}(A, {\\bf x}_j, \\rho) := \\sum_i {\\bf d}({\\bf a}_i, x_{ij}) + \\rho. \\\\\n\\end{aligned}\n\\]\nFor clarity we write \\({\\bf f}_j\\) for \\({\\bf f}(A, {\\bf x}_j, \\rho)\\). The posterior density is \\(q(b) = \\frac{1}{Z_j}\\exp\\{\\phi_{{\\bf f}_j}(b)\\}\\) where \\(Z_j = \\int_{\\mathbb R} \\exp\\{\\phi_{{\\bf f}_j}(b)\\} db\\). The normalizing constant may be computed numerically. Or, we can take a Gaussian approximation for \\(q(b)\\).\n\\[\n\\begin{aligned}\np(\\gamma = j | {\\bf y}, X, \\sigma^2_0)\n&\\propto p({\\bf y} | {\\bf x}_j, \\gamma = j) \\pi_j \\\\\n&= \\left(\\int p({\\bf y}, b | {\\bf x}_j, \\gamma = j) db\\right) \\pi_j \\\\\n&= \\left(\\int \\exp\\{\\sum_i \\phi_{{\\bf a}_i} (x_{ij} b) + \\phi_{\\rho}(b) \\}\\right) \\pi_j \\\\\n&= \\left(\\int \\exp\\{\\phi_{{\\bf f}_j}(b) \\} db\\right) \\pi_j = Z_j \\pi_j \\\\\n\\end{aligned}\n\\]\n\\[\\alpha_j := q(\\gamma = j) = \\frac{Z_j \\pi_j}{\\sum_k Z_k \\pi_k}\\]\n\n\nVariational objective\nIt will be useful to write out the variational objective for the SER:\n\\[\nF_{SER}(q| {\\bf y}, X, \\sigma^2_0) = \\mathbb E_q[\\log p({\\bf y} | X, b, \\gamma, \\sigma^2_0)] - KL[q || p].\n\\] The exact posterior maximizes \\(F_{SER}\\), that is\n\\[\np_{post} = \\arg\\max_q F_{SER}(q| {\\bf y}, X, \\sigma^2_0).\n\\]\nWe can approximate the variational objective by substituting our polynomial approximation\n\\[\n\\hat F_{SER}(q| A, X, \\sigma^2_0) = \\mathbb E_q \\left[\n\\sum_i \\phi_{\\exp\\{{\\bf a}_i}(X (b\\gamma)) + \\log p(b | \\sigma^2_0) + \\log p(\\gamma | \\pi)\\}\n\\right]  - KL[q || p].\n\\]\nAs discussed above, \\(q(b | \\gamma) \\propto \\exp\\{\\phi_{{\\bf f}_j}(b)\\}\\) and \\(q(\\gamma = j) = \\frac{Z_j}{\\sum_k Z_k}\\).\nLet \\(SER(A, X, \\sigma^2_0, \\pi)\\) be a function that returns \\(q\\).\n\n\nPolynomial to Gaussian\nFor the extention to SuSiE, we are going to need to compute \\(\\mu_j^k = \\mathbb E[b^k | \\gamma = j] = \\int_{\\mathbb R} b^k \\exp\\{\\phi_{{\\bf f}(A, {\\bf x}_j, \\rho)}(b)\\} db\\). We can ensure that this function is integrable by making selecting polynomial approximations \\({\\bf a}_i\\) which are of even degree and \\(a_{iM} < 0\\). This ensures that the leading coefficient of \\({\\bf f}\\) is negative and that \\(e^{\\bf f}\\) decays. However, this integral may not be available analytically in general.\nHowever, if \\(\\phi_{\\bf f}\\) is degree \\(2\\), this is a special case where the moments of a Gaussian distribution can be computed analytically. This motivates us to “approximate the approximation”.\nWe take a Laplace approximation to \\(\\phi_{\\bf f}\\). Currently I approximate the degree \\(M\\) polynomial \\(\\phi_{\\bf f}\\) with the degree \\(2\\) polynomial by the following: first we search for the mode of \\(\\phi_{\\bf f}\\) e.g. by finding the roots of \\(\\phi_{\\bf f}^\\prime\\) and selecting the one that maximizes \\(\\phi_{\\bf f}\\) (although there may be cheaper ways to do this?). Then, we take a second order Taylor series expansion around the mode.\nIt’s worth noting that these two levels of approximation result in a different strategy than taking a “global” second order approximation directly, or a Laplace approximation \\(\\log p(y, \\beta) \\rightarrow {\\bf f} \\rightarrow {\\bf g}\\). “Laplace approximation of a degree-\\(M\\) polynomial approximation” is a local approximation around the approximate mode. Directly applying Laplace approximation would result in a local approximation around the exact mode. We would prefer the latter except that (1) the polynomial approximation provides the computational simplifications we need to extend to SuSiE and (2) finding the posterior mode and it’s second derivative may be more expensive in the general case."
  },
  {
    "objectID": "research/polynomial_susie/index.html#polynomial-susie",
    "href": "research/polynomial_susie/index.html#polynomial-susie",
    "title": "Polynomial approximation SuSiE",
    "section": "Polynomial SuSiE",
    "text": "Polynomial SuSiE\n\nVariational approximation\nWe will use the variational approximation that factorizes over single effects. This is the same variational approximation used in linear SuSiE\n\\(\\beta_l = \\gamma_l b_l\\) \\[q(\\beta_1, \\dots, \\beta_L) = \\prod_l q(\\beta_l)\\]\n\n\nVariational objective\nWe want to find \\(q\\) that optimizes the evidence lower bound (ELBO) \\(F\\)\n\\[\nF_{\\text{SuSiE}}(q| {\\bf y}, X, \\{\\sigma^2_{0l}\\}, \\{\\pi_l\\}) = \\mathbb E_q[\\sum \\log p(y_i | {\\bf x}_i, \\beta)] - \\sum KL[q(\\beta_l) || p(\\beta_l | \\sigma^2_{0l}, \\pi_l)]\n\\]\nEvaluating the expectations \\(\\mathbb E_q[\\log p(y_i | {\\bf x}_i, \\beta)]\\) is generally hard, we make progress by substituting the polynomial approximation, we denote the approximate ELBO \\(\\hat F\\).\n\\[\n\\begin{aligned}\n\\hat F_{\\text{SuSiE}}(q| A, X, \\{\\sigma^2_{0l}\\}, \\{\\pi_l\\})\n&= \\mathbb E_q[\\sum_i \\phi_{{\\bf a}_i}({\\bf x}_i^T \\beta)]\n- \\sum KL[q(\\beta_l) || p(\\beta_l | \\sigma^2_{0l}, \\pi_l)]\n\\end{aligned}\n\\]\n\n\nCoordinate ascent in the approximate ELBO\nWith \\(\\psi_{li} := {\\bf x}_i^T \\beta_l\\) and \\(\\psi_i = {\\bf x}_i^T \\beta = \\sum {\\bf x}_i^T \\beta_l = \\sum_l \\psi_{li}\\) and \\(\\psi_{-li} = \\psi_i - \\psi_{li}\\).\nConsider the approximate ELBO just as a function of \\(q_l\\)\n\\[\n\\begin{aligned}\n\\hat F_{\\text{SuSiE},l}(q_l| A, X,  q_{-l}, \\sigma^2_{0l}, \\pi_l)\n= \\mathbb E_{q_l} \\left[\n    \\mathbb E_{q_{-l}} \\left[\n      \\sum_i \\phi_{{\\bf a}_i}(\\psi_{li} + \\psi_{-li})\n    \\right]\n\\right] - KL \\left[p(\\beta_l) || q(\\beta_{-l})\\right] + \\kappa\n\\end{aligned}\n\\]\nWhere \\(\\kappa\\) is a constant that does not depend on \\(q_l\\) (note: it does depend on \\(\\{\\sigma^2_{0l}\\}, \\{\\pi_l\\}\\), but they are not written in the arguments to \\(\\hat F_{\\text{SuSiE},l}\\). We will need to evaluate \\(\\mathbb E_{q_{-l}}[\\phi_{\\bf a_i}(\\psi_{li} + \\psi_{-li})]\\) for each \\(i\\). Focusing on a single observation\n\\[\n\\mathbb E_{q{-l}} \\left[\\phi_{\\bf a_i}(\\psi_{li} + \\psi_{-li})\\right]\n= \\mathbb E_{q_{-l}} \\left[\\phi_{{\\bf b}({\\bf a}, \\psi_{-li})}(\\psi_{li})\\right] = \\phi_{{\\bf c}({\\bf a_i}, q_{-l})}(\\psi_{li})\n\\]\nWe will write \\({\\bf b}_{li} = {\\bf b}({\\bf a}, \\psi_{-li})\\) and \\({\\bf c}_{li} = {\\bf c}({\\bf a_i}, q_{-l})\\), Indicating that they are the coefficient for an \\(M\\) degree polynomial in \\(\\psi_{li}\\).\nRevisiting the approximate ELBO we see\n\\[\n\\begin{aligned}\n\\hat F_{\\text{SuSiE},l}(q_l| A, X,  q_{-l})\n& = \\mathbb E_{q_l} \\left[\n    \\mathbb E_{q_{-l}} \\left[\n      \\sum_i \\phi_{{\\bf a}_i}(\\psi_{li} + \\psi_{-li})\n    \\right]\n\\right] - KL \\left[q(\\beta_l) || p(\\beta_{-l} | \\sigma^2_{0l}, \\pi_l)\\right] + \\kappa \\\\\n&= \\left(\\mathbb E_{q_l} \\left[\n  \\sum_i \\phi_{{\\bf c}_{li}}(\\psi_{li})\n\\right] - KL \\left[q(\\beta_l) || p(\\beta_{-l} | \\sigma^2_{0l}, \\pi_l)\\right] \\right)  + \\kappa \\\\\n&= \\hat F_{\\text{SER}}(q_l | C_{l}, X, \\sigma^2_{0l}, \\pi_l) + \\kappa\n\\end{aligned}\n\\]\nWe can see that with respect to the \\(q_l\\) the SuSiE approximate ELBO is equal, up to a constant, to the SER approximate ELBO, for a new set of coefficients \\(C_l\\), which depend on \\(q_{-l}\\). It follows that the coordinate ascent update for \\(q_l\\) is achieved by fitting the polynomial approximate SER with coefficients \\(C_l\\).\nThis is analagous to how we fit linear SuSiE by a sequence of SERs: we can fit the polynomial approximate SuSiE with a sequence of polynomial approximate SERs. Rather than “residualizing”, we fit the polynomial approximate SER with coefficients \\(C_l\\).\nThe crux of this approach is having a fast way to compute \\(C_l\\).\n\n\nUpdate Rule\nAt iteration \\(t\\) we have the current variational approximation\n\\[\nq^{(t)} = \\prod q_l^{(t)}\n\\]\nDefine \\[\n{\\bf c}_i^{(0)} := \\mathcal M(\\psi_i, q^{(0)}) {\\bf a}_i\n\\]\nNote that \\(\\mathcal M(\\psi_i, q^{(0)})\\) gives the expected shift matrix that removes the entire linear prediction, so that \\({\\bf c}_{i0}^{(0)} = \\phi_{\\bf c_i^{(0)}}(0) = \\mathbb E_{q^{(0)}}[\\log p(y_i | \\psi_i)]\\).\nAt iteration \\(t\\), our coordinate ascent updates require \\({\\bf c}_{li}^{(t)} = \\mathcal M(\\psi_{li}, q_{-l}^{(t)})\\). However, we know that:\n\\[\n{\\bf c}_{i}^{(t)} = \\mathcal M(\\psi_{li}, q_l^{(t)}){\\bf c}_{li}^{(t)} \\implies {\\bf c}_{li}^{(t)} = \\mathcal M(\\psi_{li}, q_l^{(t)})^{-1}{\\bf c}_{i}^{(t)}\n\\]\nWe can use \\(C_l^{(t)}\\) to compute \\(q_l^{(t+1)}\\) and then\n\\[\n{\\bf c}_{i}^{(t)} \\leftarrow \\mathcal M(\\psi_{li}, q_l^{(t + 1)}){\\bf c}_{li}^{(t)}\n\\]\nso, by solving a triangular systems, and multiplying by upper triangular matrices \\(\\mathcal M\\) we can “efficiently” move between the coefficient representations needed for each SER update. I worry that \\(\\mathcal M_l\\) may be poorly conditioned resulting in numerical instability, but I have not seen it in toy examples yet.\n\n\nAlgorithm\nInitialize \\(C^{(0)} = A\\) and \\(q^{(0)} = \\prod q_l^{(0)}\\) such that \\(\\mathbb E[\\psi_l^k] = 0\\;\\; \\forall l, k\\).\nFor \\(t = 1, 2, \\dots\\):\n\n\\(C^{(t)} = C^{(t-1)}\\)\nFor each \\(l \\in [L]\\):\n\nCompute \\(C_l^{(t)}\\) by \\({\\bf c}_{li}^{(t)} \\leftarrow \\mathcal M(\\psi_{li}, q_l^{(t-1)})^{-1} {\\bf c}_i^{(t)}\\) for \\(i \\in [n]\\)\n\\(q_l^{(t)} \\leftarrow \\text{SER}(C_l^{(t)}, X, \\sigma_{0l}^2, \\pi_l)\\)\nUpdate \\(C^{(t)}\\) by \\({\\bf c}_i^{(t)} \\leftarrow \\mathcal M(\\psi_{li}, q^{(t)}_l) {\\bf c}^{(l)}_i\\)\n\n\n\n\nComplexity\nLet’s break down the complexity of the inner loop (that is, updating \\(q_l\\) and the requisite computations for the next run of the loop).\nComputing coefficients \\(C_l\\): Computing \\({\\bf c}_{li}\\) is \\(O(M^2)\\) to construct \\(\\mathcal M_l\\) and to solve the triangular system \\({\\bf c}_i = \\mathcal M_l(\\psi_{li}, q_l) {\\bf c}_{li}\\). This is per observations, so computing \\(C_{l}\\) is \\(O(M^2m)\\)\nFitting SER Once we computed the coefficients we update the SER in \\(O(Mnp)\\). We just sum the coefficients to construct the polynomial posterior for each variable). Then \\(O(pM^3)\\) to perform root finding and make the Gaussian approximation for \\(q_l\\).\nComputing moments We need to evaluate the moments \\(\\mathbb E[\\psi_{li}^k]\\) for \\(k=1, \\dots, M-1\\). \\(O(Mnp)\\)? This is fast if \\(q_l\\) is Gaussian but the scaling factor may be large if we need to compute numerically. If we are making the Gaussian approximation, that costs \\(O(M^3p)\\).\nUpdating \\(C\\) Again \\(O(nM^2)\\). We need to construct \\(\\mathcal M (\\psi_{li}, q_l)\\) and compute the matrix-vector product \\(\\mathcal M_l {\\bf c}\\).\nTotal \\(O(M^2n + Mnp + M^3p)\\)"
  },
  {
    "objectID": "research/logistic_abf_accuracy/index.html",
    "href": "research/logistic_abf_accuracy/index.html",
    "title": "Wakefield ABF Accuracy",
    "section": "",
    "text": "Code\nlibrary(dplyr)\nlibrary(ggplot2)\n\nsigmoid <- function(x){1/(1 + exp(-x))}\n\nsimulate <- function(n, beta0=0, beta=1){\n  x <- rnorm(n)\n  logit <- beta0 + beta*x\n  y <- rbinom(n, 1, sigmoid(logit))\n  return(list(x=x, y=y, beta0=beta0, beta=beta))\n}\n\nfit_glm_no_intercept <- function(sim){\n  fit <- with(sim, glm(y ~ x + 0, family = 'binomial'))\n  tmp <- unname(summary(fit)$coef[1,])\n  betahat <- tmp[1]\n  shat2 <- tmp[2]^2\n  z <- with(sim, (beta - betahat)/sqrt(shat2))\n  res <- list(\n    intercept=0,\n    betahat = betahat,\n    shat2 = shat2,\n    z = z\n  )\n  return(res)\n}\n\nfit_glm <- function(sim, prior_variance=1){\n  fit <- with(sim, glm(y ~ x + 1, family = 'binomial'))\n  tmp <- unname(summary(fit)$coef[2,])\n  betahat <- tmp[1]\n  shat2 <- tmp[2]^2\n  z <- with(sim, (beta - betahat)/sqrt(shat2))\n  res <- list(\n    intercept = unname(fit$coefficients[1]),\n    betahat = betahat,\n    shat2 = shat2,\n    z = z\n  )\n  return(res)\n}\n\ncompute_log_abf1 <- function(betahat, shat2, prior_variance){\n  W <- prior_variance\n  V <- shat2\n  z <- betahat / sqrt(shat2)\n  labf <- 0.5 * log(V /(V + W)) + 0.5 * z^2 * W /(V + W)\n  return(labf)\n}\n\ncompute_log_abf2 <- function(betahat, shat2, prior_variance){\n  dnorm(betahat, 0, sqrt(shat2 + prior_variance), log = T) - \n    dnorm(betahat, 0, sqrt(shat2), log=T)\n}\n\ncompute_log_abf <- function(glm_summary, prior_variance=1){\n  labf <- with(glm_summary, compute_log_abf1(betahat, shat2, prior_variance))\n  return(labf)\n}\n\ncompute_exact_log_abf_no_intercept <- function(sim, prior_variance){\n  glm_fit <- fit_glm_no_intercept(sim)\n  f <- function(b){\n    logits <- sim$x * b\n    sum(dbinom(x=sim$y, size=1, prob=sigmoid(logits), log = T)) + \n      dnorm(b, 0, sqrt(prior_variance), log = T)\n  }\n  max <- f(glm_fit$betahat)\n  \n  f2 <- function(b){purrr::map_dbl(b, ~exp(f(.x) - max))}\n  \n  lower <- glm_fit$betahat - sqrt(glm_fit$shat2) * 6\n  upper <- glm_fit$betahat + sqrt(glm_fit$shat2) * 6\n  quadres <- cubature::pcubature(f2, lower, upper, tol = 1e-10)\n  log_bf <- (log(quadres$integral) + max) - sum(dbinom(sim$y, 1, 0.5, log=T))\n  return(log_bf)\n}\n\ncompute_exact_log_bf <- function(sim, prior_variance){\n  glm_fit <- fit_glm(sim)\n  f <- function(b){\n    logits <- sim$x * b + glm_fit$intercept\n    sum(dbinom(x=sim$y, size=1, prob=sigmoid(logits), log = T)) + \n      dnorm(b, 0, sqrt(prior_variance), log = T)\n  }\n  max <- f(glm_fit$betahat)\n  \n  f2 <- function(b){purrr::map_dbl(b, ~exp(f(.x) - max))}\n  \n  lower <- glm_fit$betahat - sqrt(glm_fit$shat2) * 6\n  upper <- glm_fit$betahat + sqrt(glm_fit$shat2) * 6\n  quadres <- cubature::pcubature(f2, lower, upper, tol = 1e-10)\n  log_bf <- (log(quadres$integral) + max) - sum(dbinom(sim$y, 1, mean(sim$y), log=T))\n  return(log_bf)\n}\n\ncompute_vb_log_bf <- function(sim, prior_variance=1){\n  vb <- with(sim, logisticsusie::fit_univariate_vb(x, y, tau0=1/prior_variance)) \n  log_vb_bf <- tail(vb$elbos, 1) - sum(dbinom(sim$y, 1, mean(sim$y), log=T))\n  return(log_vb_bf)\n}"
  },
  {
    "objectID": "research/logistic_abf_accuracy/index.html#demonstrations",
    "href": "research/logistic_abf_accuracy/index.html#demonstrations",
    "title": "Wakefield ABF Accuracy",
    "section": "Demonstrations",
    "text": "Demonstrations\nHere we simulate \\(x_i \\sim N(0, 1)\\) and \\(y_i \\sim \\text{Bernoulli}(\\sigma(x_i \\beta + \\beta_0))\\)\n\n\\(\\beta = 0.1\\), \\(\\beta_0 = 0\\)\nWe simulate 10 replications at each sample size \\(n = 2^6, 2^7, \\dots, 2^{16}\\). While we see relatively good agreement between the ABF and exact BF on a log-scale, we do see the pattern that the error tends to increase with increasing samples size. Furthermore, these “small” errors on the log scale actually translate to large errors between the BF and ABF.\n\n\nCode\n# range of n, 10 reps\nn <- 2^(6:15)\nbf_comparison_01 <- purrr::map_dfr(n, ~bf_comparison_rep(10, .x, 0.1))\n\n# \"good\" agreement the exact BF and ABF\npar(mfrow=c(1, 2))\nplot(bf_comparison_01$log_bf,\n     bf_comparison_01$log_abf,\n     xlab='log BF', ylab = 'log ABF'); abline(0, 1, col='red')\nplot(bf_comparison_01$log_bf,\n     bf_comparison_01$log_abf - bf_comparison_01$log_bf,\n     xlab = 'log BF', ylab = 'log ABF - log BF'); abline(h=0, col='red')\n\n\n\n\n\nCode\n# bf_comparison_01 %>% \n#   tidyr::pivot_longer(ends_with('bf'), names_to='method', values_to='log_bf') %>%\n#   ggplot(aes(x=as.factor(n), y=log_bf, color=method)) +\n#   geom_boxplot()\n\n\nbf_comparison_01 %>% \n  mutate(\n    abf_rel_error = exp(log_abf - log_bf) - 1,\n    vbf_rel_error = exp(log_vbf - log_bf) - 1) %>%\n  tidyr::pivot_longer(ends_with('error'), names_to='method', values_to='relative_error') %>%\n  ggplot(aes(x=as.factor(n), y=relative_error, color=method)) +\n  geom_boxplot()\n\n\n\n\n\nSurprisingly as \\(n\\) grows, the relative error between the BF and the ABF increases! At \\(n=2^9\\) things look pretty good, but at \\(n=2^15\\) the relative error is quite large! Let’s inspect the \\(z\\)-scores at these simulation settings– do they look normally distributed?\n\nConfirming normality\nBoth \\(n=2^9\\) and \\(n=2^{15}\\) are fairly large sample sizes. We confirm that \\(z = \\frac{\\beta - \\hat\\beta}{\\hat s}\\) looks normal\n\n\nCode\nglm_sim_reps <- function(reps, n, beta){\n  purrr::map_dfr(1:reps, ~simulate(n, beta=beta) %>% fit_glm() %>% data.frame()) %>%\n    mutate(rep = 1:reps)\n}\n\nrep1 <- glm_sim_reps(1000, 2^9, 0.1)\nshapiro.test(rep1$z)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  rep1$z\nW = 0.99826, p-value = 0.4108\n\n\nCode\nks.test(rep1$z, pnorm)\n\n\n\n    One-sample Kolmogorov-Smirnov test\n\ndata:  rep1$z\nD = 0.030303, p-value = 0.3175\nalternative hypothesis: two-sided\n\n\nCode\nrep2 <- glm_sim_reps(1000, 2^15, 0.1)\nshapiro.test(rep2$z)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  rep2$z\nW = 0.99693, p-value = 0.05116\n\n\nCode\nks.test(rep2$z, pnorm)\n\n\n\n    One-sample Kolmogorov-Smirnov test\n\ndata:  rep2$z\nD = 0.027911, p-value = 0.4172\nalternative hypothesis: two-sided\n\n\n\n\nCode\np1 <- ggplot(rep1,aes(x=z)) +\n       geom_line(stat = \"ecdf\")+\n       geom_point(stat=\"ecdf\",size=2) +\n       stat_function(fun=pnorm,color=\"red\") +\n       labs(title=\"eCDF and normal CDF, n = 2^9\")\np2 <- ggplot(rep2,aes(x=z)) +\n       geom_line(stat = \"ecdf\")+\n       geom_point(stat=\"ecdf\",size=2) +\n       stat_function(fun=pnorm,color=\"red\") +\n       labs(title=\"eCDF and normal CDF, n=2^15\")\n\ncowplot::plot_grid(p1, p2)\n\n\n\n\n\n\n\nCode\nsubset_sim <- function(sim, m){\n  sim2 <- sim\n  sim2$x <- head(sim2$x, m)\n  sim2$y <- head(sim2$y, m)\n  return(sim2)\n}\n\nbeta <- 0.1\nsim <- simulate(2^16, beta=beta)\ntitrate <- purrr::map_dfr(5:16, ~sim %>% subset_sim(2^.x) %>% fit_glm() %>% data.frame())\nplotrix::plotCI(\n  x= 5:16,\n  y= titrate$betahat - beta,\n  li = (titrate$betahat - beta) - 2 * sqrt(titrate$shat2),\n  ui = (titrate$betahat - beta) + 2 * sqrt(titrate$shat2)\n); abline(h=0, lty=3)\n\n\n\n\n\nI was wondering if glm applies a conservative correction to it’s estimate of the standard error. The standard errors come from the squareroot of the diagonal of the negative inverse Fisher information matrix since asymptotically.\n\\[\n\\hat \\beta \\sim N(\\beta,  - \\mathcal I(\\beta)^{-1})\n\\] I think peoples use \\(- \\mathcal I(\\hat \\beta)\\) as an estimator for the precision matrix. Ignoring the intercept I just computed \\(I = \\sum_i \\nabla^2_{\\beta}\\log p (y |x, \\beta, \\beta_0)\\), and used \\(s_2 = \\sqrt{I^{-1}}\\) as a new estimate of the standard error. Despite some hadn waiving, it actually agrees very well with the standard errors reported by glm– which is to say this is not the problem.\n\n\nCode\nrecompute_shat2 <- function(betahat, intercept, x){\n  logits <- betahat * x + intercept\n  return(1/sum(sigmoid(logits) * sigmoid(-logits) * x^2))\n}\n\ntitrate <- titrate %>%\n  mutate(m = 2^(5:16)) %>%\n  rowwise() %>%\n  mutate(shat2_recompute = recompute_shat2(betahat, intercept, head(sim$x, m))) %>%\n  ungroup()\n\nplot(titrate$shat2, titrate$shat2_recompute); abline(0, 1, col='red')\n\n\n\n\n\n\n\n\n\\(\\beta = 1\\)\nFor larger \\(\\beta\\) the discrepency between ABF and BF is much more obvious!\n\n\nCode\nn <- 2^(6:15)\nbf_comparison_1 <- purrr::map_dfr(n, ~bf_comparison_rep(10, .x, 1))\n\nplot(bf_comparison_1$log_bf, bf_comparison_1$log_abf); abline(0, 1, col='red')\n\n\n\n\n\nCode\nbf_comparison_1 %>% \n  tidyr::pivot_longer(ends_with('bf'), names_to='method', values_to='log_bf') %>%\n  ggplot(aes(x=as.factor(n), y=log_bf, color=method)) +\n  geom_boxplot()\n\n\n\n\n\nCode\nbf_comparison_1 %>% \n  mutate(\n    abf_rel_error = exp(log_abf - log_bf) - 1,\n    vbf_rel_error = exp(log_vbf - log_bf) - 1) %>%\n  tidyr::pivot_longer(ends_with('error'), names_to='method', values_to='error') %>%\n  ggplot(aes(x=as.factor(n), y=error, color=method)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nCode\nplot(bf_comparison_1$log_vbf, bf_comparison_1$log_abf);abline(0, 1, col='red')\n\n\n\n\n\n\n\nCode\nn <- 2^(6:15)\nbf_comparison_01 <- purrr::map_dfr(n, ~bf_comparison_rep(10, .x, 1))\nbf_comparison_01 %>% \n  mutate(log_abf_error = (log_abf - log_bf), log_vbf_error = (log_vbf - log_bf)) %>%\n  tidyr::pivot_longer(ends_with('error'), names_to='method', values_to='error') %>%\n  ggplot(aes(x=as.factor(n), y=log_bf, color=method)) +\n  geom_boxplot()"
  },
  {
    "objectID": "research/logistic_abf_accuracy/index.html#an-explaination",
    "href": "research/logistic_abf_accuracy/index.html#an-explaination",
    "title": "Wakefield ABF Accuracy",
    "section": "An explaination",
    "text": "An explaination\n\\[\n\\begin{aligned}\nABF &= \\int \\frac{N(\\hat \\beta| \\beta, s^2)}{N(\\hat\\beta | 0, s^2)} p(\\beta) d\\beta\\\\\nBF &= \\int \\frac{p({\\bf y} | {\\bf x}, \\beta)}{p({\\bf y} | \\beta = 0)} p(\\beta) d\\beta\n\\end{aligned}\n\\]\nFirst, here is an example where there is a large discrepancy between the log ABF and log BF– over 20 log-likelihood units! We also note that including/excluding the intercept in glm doesn’t really make a dent.\n\n\nCode\nset.seed(2)\nsim <- simulate(1000)\nfit <- fit_glm(sim, prior_variance=1)\nfit_no_intercept <- fit_glm_no_intercept(sim)\n\n# show there is a discrepency ~20 log-likelihood unit\nwith(fit, compute_log_abf1(betahat, shat2, 1))\n\n\n[1] 71.33497\n\n\nCode\nwith(fit_no_intercept, compute_log_abf1(betahat, shat2, 1))\n\n\n[1] 71.46433\n\n\nCode\ncompute_vb_log_bf(sim, 1)\n\n\n[1] 93.52508\n\n\nUsing this simulated example we plot the likelihood ratio and approximate log-likelihood ratio (with respect to the null model \\(\\beta=0\\)) over the range of \\(\\beta\\). We plot the likelihood ratio in black, and plot the approximate likelihood ratio in red. We see that the likelihood ratios agree at \\(\\beta = 0\\)– this is not surprising because they should both be equal to \\(1\\) (or \\(0\\), on the log scale as shown in the plot).\nThe mode and the curvature of this likelihood ratio approximation are determined by the effect size and standard error, so the approximation is completely specified. We can see that this leaves a gap between the likelihood ratio and the approximate likelihood ratio near the mode. Assuming a flat prior on \\(\\beta\\), this is the region that will contribute most to the integral.\nWhile the likelihood is well approximated by a Gaussian near it’s mode, the quality of the Gaussian approximation is poor in the tails. As the sample size increases, \\(\\beta = 0\\) is more “in the tail”. So we see that the approximation to the likelihood ratio used in ABF is requiring the approximate likelihood ratio to agree with the exact likelihood ratio at some point in the tail– but this is not particularly relevant for getting a good approximation of the BF, as it is the area in the body of the distribution that contributes most to the integral.\nIn order for the ABF to perform well in large samples, the Gaussian approximation would need to improve in the tails faster than the point \\(\\beta=0\\) gets pushed into the tail. E.g. for the Gaussian approximation to be good, we’d want that the curvature estimated at the mode is also the curvature we’d estimate everywhere else (e.g. at \\(\\beta = 0\\)).\n\n\nCode\ncompare_lrs <- function(sim, prior_variance=1, k=1, plot_correction=F){\n  fit <- fit_glm(sim, prior_variance=1)\n  fit_no_intercept <- fit_glm_no_intercept(sim)\n  \n  # plot ll\n  ll <- function(b){with(sim, sum(y*x*b - log(1 + exp(x*b))))}\n  ll_vec <- function(b){purrr::map_dbl(b, ~ll(.x))}\n  asymptotic_ll <- function(b){dnorm(b, mean=fit$betahat, sd=sqrt(fit$shat2), log=T)}\n\n  ll0 <- ll(0)\n  asymptotic_ll0 <- asymptotic_ll(0)\n  \n  ll_mle <- ll(fit$betahat)\n  asymptotic_ll_mle <- asymptotic_ll(fit$betahat)\n  \n  mle = fit$betahat\n  xs <- seq(mle - k* abs(mle), mle + k* abs(mle), by=0.1)\n  \n  diff = (ll_mle - asymptotic_ll_mle) - ll0\n  ll_xs <- ll_vec(xs)\n  asymptotic_ll_xs <- asymptotic_ll(xs)\n  ll_prior <- dnorm(xs, 0, sd = sqrt(prior_variance), log = T)\n  \n  ylim <- range(asymptotic_ll_xs - asymptotic_ll0, ll_xs - ll0)\n  plot(xs,ll_xs - ll0, type='l', ylim = ylim,\n       xlab= 'beta', ylab='log likelihood ratio')\n  lines(xs, asymptotic_ll_xs - asymptotic_ll0, col='red')\n  \n  if(plot_correction){\n    lines(xs, asymptotic_ll_xs - asymptotic_ll_mle + (ll_mle - ll0), col='blue')\n  }\n  abline(h=0, lty=3)\n  abline(v=0, lty=3)\n  abline(v=fit$betahat)\n}\n\ncompare_lrs(sim, prior_variance = 1, k=1.5)\n\n\n\n\n\n\n\nCode\ncompare_lrs(sim, prior_variance = 1, k=50)\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1, 4))\nsimulate(100, beta0 = 0.1) %>% compare_lrs(k=1.5)\nsimulate(1000, beta0 = 0.1) %>% compare_lrs(k=1.5)\nsimulate(10000, beta0 = 0.1) %>% compare_lrs(k=1.5)\nsimulate(100000, beta0 = 0.1) %>% compare_lrs(k=1.5)\n\n\n\n\n\n\nFamily of LR approximations\nWrite the likelihood ratio as \\[\nLR(\\beta_1, \\beta_2) = \\frac{\\mathcal {L}(\\beta_1)}{\\mathcal {L}(\\beta_2)}\n\\]\nLet \\(\\widehat {LR}(\\beta_1, \\beta_2)\\) be the approximate likelihood ratio under the normal approximation \\(\\beta \\sim N(\\hat \\beta, s^2)\\).\nThe approximation to the likelihood ratio against the null used in ABF is simply\n\\[\n\\widehat{LR}_{ABF}(\\beta) = \\widehat{LR}(\\beta, 0)\n\\] However if we note that\n\\[\nLR(\\beta, 0) = LR(\\beta, \\beta^*) \\times LR(\\beta^*, 0)\n\\]\nwe can derive a family of approximations for the likelihood ratio against the null\n\\[\n\\widehat{LR}_{\\beta^*}(\\beta) = \\widehat{LR}(\\beta, \\beta^*) LR(\\beta^*,0)\n\\] And note that \\(\\widehat{LR}_{ABF}\\) is a special case where \\(\\beta^*=0\\). We’ve seen that this choice makes the approximation good in the tail– which isn’t very important for getting an accurate approximate BF. Perhaps the best choice would be make the approximate LR match the exact LR at the MAP estimate. But a second good choice, and one that does not require knowledge of the prior could be to set \\(\\beta^* = \\beta_{MLE}\\). This choice is shown in blue on the updated plot\n\n\nCode\ncompare_lrs(sim, prior_variance = 1, k=1.5, plot_correction=T)\n\n\n\n\n\nWe can use the"
  },
  {
    "objectID": "research/random_intercepts/index.html",
    "href": "research/random_intercepts/index.html",
    "title": "Random intercepts",
    "section": "",
    "text": "Code\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(kableExtra)\nlibrary(ggplot2)\nWhat happens when you ignore the randomness in a random intercept model? Here is a quick demonstration to show that the behavior is very different for linear regression and logistic regression."
  },
  {
    "objectID": "research/random_intercepts/index.html#linear-regression-random-intercept",
    "href": "research/random_intercepts/index.html#linear-regression-random-intercept",
    "title": "Random intercepts",
    "section": "Linear regression (random intercept)",
    "text": "Linear regression (random intercept)\nFor linear models if the intercept \\(b_0 \\sim N(\\mu_0, \\sigma^2_0)\\) then fitting the linear regression will just have the effect of having a larger residual variance. Standard statistical software estimates the residual variance, and for modest sample size compared to the number of features this can be done quite well.\n\\[\ny = b_0 + b x + \\epsilon;\\; \\epsilon \\sim N(0, \\sigma^2)\n\\]\n\\[\ny = \\mu_0 + b x + \\tilde\\epsilon;\\; \\tilde\\epsilon \\sim N(0, \\sigma^2_0 + \\sigma^2)\n\\]\nSo we should expect OLS to give us unbiased estimate of the intercept and effect. That is what we see. Here we simulate 100 replicates of a simulation with \\(n = 100\\), \\(\\mu_0 = -1\\) and \\(b=1\\). We vary \\(\\sigma_0\\). Here we show the coverage of the 95% confidence intervals across 100 simulations. As expected, we achieve coverage for all settings of \\(\\sigma_0\\) in the Gaussian linear model.\n\n\nCode\nlist2tibble <- function(l){\n    dplyr::tibble(!!!l)\n}\n\nsimulation_linear <- function(n, b0, b, std0, std){\n    std <- sqrt(std0^2 + std^2)\n    x <- rnorm(n)\n    y <- b0 + x * b + rnorm(n) * std\n    fit <- lm(y ~ x)\n    coef <- summary(fit)$coef\n    res <-list(n=n, b0 = b0, b=b, std0 = std0, b0hat = coef[1, 1], b0_shat=coef[1, 2], bhat = coef[2,1], shat = coef[2, 2]) |> list2tibble()\n    return(res)\n}\n\nsimrep_linear <- function(k, std0){\n    purrr::map_dfr(1:k, ~simulation_linear(100, -1, 1, std0, 1))\n}\n\nmake_forrest <- function(res, column_name='std0'){\n    df <- res %>%\n        mutate(\n            Error = bhat -b,\n            Lower = Error - 2 * shat,\n            Upper = Error + 2 * shat,\n            Covered = ifelse(Lower < 0 & Upper > 0, \"yes\", \"no\")) %>%\n        arrange(Error) %>%\n        group_by(.data[[column_name]]) %>%\n        mutate(Rank = 1:n()) %>%\n        ungroup()\n    # Create the forest plot\n    ggplot(df, aes(x = Rank, y = Error)) +\n    # geom_hline(yintercept = 0, linetype = \"dashed\") +\n    geom_point(position = position_dodge(width = 0.5)) +\n    geom_errorbar(aes(ymin = Lower, ymax = Upper, color=Covered)) +\n    theme_minimal() + facet_wrap(vars(.data[[column_name]]))\n}\n\ncap <- 'The random intercept is not a problem for linear regression, the effect estimates will remain unbiased'\n\nstd0 <- c(0, 0.25, 0.5, 1.0, 2.0, 4.0)\nres <- purrr::map_dfr(std0, ~simrep_linear(100, .x))\nres %>%\n    rowwise() %>%\n    mutate(covered = (b < bhat + 2*shat) & (b > bhat - 2*shat)) %>%\n    ungroup() %>%\n    group_by(std0) %>%\n    summarize(coverage=mean(covered)) %>%\n    kbl(caption=cap) %>%\n    kable_classic(full_width = T, html_font = \"Cambria\")\n\n\n\n\nThe random intercept is not a problem for linear regression, the effect estimates will remain unbiased\n \n  \n    std0 \n    coverage \n  \n \n\n  \n    0.00 \n    0.93 \n  \n  \n    0.25 \n    0.94 \n  \n  \n    0.50 \n    0.95 \n  \n  \n    1.00 \n    0.98 \n  \n  \n    2.00 \n    0.91 \n  \n  \n    4.00 \n    0.94 \n  \n\n\n\n\n\n\n\nCode\nres %>%\n    ggplot(aes(x=bhat)) + \n    geom_histogram() + \n    geom_vline(xintercept=1) +\n    facet_wrap(vars(std0), nrow=2) +\n    theme_bw()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nEffect estimates are unbiased\n\n\n\n\n\n\nCode\nres %>% make_forrest() \n\n\n\n\n\nEffect estimates are unbiased"
  },
  {
    "objectID": "research/random_intercepts/index.html#logistic-regression-random-intercept",
    "href": "research/random_intercepts/index.html#logistic-regression-random-intercept",
    "title": "Random intercepts",
    "section": "Logistic regression (random intercept)",
    "text": "Logistic regression (random intercept)\nFor logistic regression, and GLMs generally, it is a different story. As the variance of the random intercept grows, we see worse behavior of the effect estimates.\n\n\nCode\nsimulation_logistic <- function(n, b0, b, std0){\n    x <- rnorm(n)\n    logit <- b0 + x * b + rnorm(n) * std0\n    y <- rbinom(n, 1, 1 / (1 + exp(-logit)))\n    fit <- glm(y ~ x, family='binomial')\n    coef <- summary(fit)$coef\n    res <-list(n=n, b0 = b0, b=b, std0 = std0, b0hat = coef[1, 1], b0_shat=coef[1, 2], bhat = coef[2,1], shat = coef[2, 2]) |> list2tibble()\n    return(res)\n}\n\nsimrep <- function(k, std0){\n    purrr::map_dfr(1:k, ~simulation_logistic(100, -1, 1, std0))\n}\n\nstd0 <- c(0, 0.25, 0.5, 1.0, 2.0, 4.0)\n\nres <- purrr::map_dfr(std0, ~simrep(100, .x))\n\ncap <- 'As the variance of the random intercept increases, the coverage rate of the 95% C.I. decreases'\n\nres %>%\n    rowwise() %>%\n    mutate(covered = (b < bhat + 2*shat) & (b > bhat - 2*shat)) %>%\n    ungroup() %>%\n    group_by(std0) %>%\n    summarize(coverage=mean(covered)) %>%\n    kbl(caption=cap) %>%\n    kable_classic(full_width = T, html_font = \"Cambria\")\n\n\n\n\nAs the variance of the random intercept increases, the coverage rate of the 95% C.I. decreases\n \n  \n    std0 \n    coverage \n  \n \n\n  \n    0.00 \n    0.97 \n  \n  \n    0.25 \n    0.96 \n  \n  \n    0.50 \n    0.94 \n  \n  \n    1.00 \n    0.95 \n  \n  \n    2.00 \n    0.70 \n  \n  \n    4.00 \n    0.16 \n  \n\n\n\n\n\n\n\nCode\nres %>%\n    ggplot(aes(x=bhat)) + \n    geom_histogram() + \n    geom_vline(xintercept=1) +\n    facet_wrap(vars(std0), nrow=2) +\n    theme_bw()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nEffect estimates are biased towards 0– attentuation bias\n\n\n\n\n\n\nCode\nres %>% make_forrest()\n\n\n\n\n\nEffect estimates are unbiased"
  },
  {
    "objectID": "research/random_intercepts/index.html#logistic-regression-polygenic-model-marginal-regression",
    "href": "research/random_intercepts/index.html#logistic-regression-polygenic-model-marginal-regression",
    "title": "Random intercepts",
    "section": "Logistic regression (polygenic model, marginal regression)",
    "text": "Logistic regression (polygenic model, marginal regression)\nClosely related, we simulate from a model with multiple unobserved covariates. We simulate \\(p\\) covariates $[{}_1, , \\({\\bf x}_p]\\) but only regress onto the first covariate \\({\\bf x}_1\\) To make things simple we simulate the entries of the \\(n\\times p\\) design matrix \\(X\\) as i.i.d. \\(N(0, 1)\\). Then, the columns of \\(X\\) are independent.\nThis is really just the random intercept model with extra steps, but maybe showing this makes it explicit that we should be aware of this in e.g. GWAS.\n\n\nCode\nsimulation_logistic_multi <- function(n, p, b0, b){\n    X <- rnorm(n*p) |> matrix(nrow=n)\n    x <- X[, 1]\n    logit <- b0 + Matrix::rowSums(X) * b\n    y <- rbinom(n, 1, 1 / (1 + exp(-logit)))\n    fit <- glm(y ~ x, family='binomial')\n    coef <- summary(fit)$coef\n    res <-list(n=n, b0 = b0, b=b, p = p, b0hat = coef[1, 1], b0_shat=coef[1, 2], bhat = coef[2,1], shat = coef[2, 2]) |> list2tibble()\n    return(res)\n}\n\nsimrep <- function(k, p){\n    purrr::map_dfr(1:k, ~simulation_logistic_multi(100, p, -1, 1))\n}\n\np <- c(1, 2, 5, 10, 20)\n\nres <- purrr::map_dfr(p, ~simrep(100, .x))\n\n\n\n\nCode\ncap <- 'As the number of unobserved variables increases, the coverage drops off.'\nres %>%\n    rowwise() %>%\n    mutate(covered = (b < bhat + 2*shat) & (b > bhat - 2*shat)) %>%\n    ungroup() %>%\n    group_by(p) %>%\n    summarize(coverage=mean(covered)) %>%\n    kbl(caption=cap) %>%\n    kable_classic(full_width = T, html_font = \"Cambria\")\n\n\n\n\nAs the number of unobserved variables increases, the coverage drops off.\n \n  \n    p \n    coverage \n  \n \n\n  \n    1 \n    0.96 \n  \n  \n    2 \n    0.94 \n  \n  \n    5 \n    0.61 \n  \n  \n    10 \n    0.36 \n  \n  \n    20 \n    0.23 \n  \n\n\n\n\n\n\n\nCode\nres %>%\n    ggplot(aes(x=bhat)) + \n    geom_histogram() + \n    geom_vline(xintercept=1) +\n    facet_wrap(vars(p), nrow=1) +\n    theme_bw()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nEstimated effects are biased in simulation scenarios resembling a polygentic model\n\n\n\n\n\n\nCode\nres %>% make_forrest(column_name='p') + facet_wrap(vars(p), nrow=1)\n\n\n\n\n\nEffect estimates are unbiased\n\n\n\n\nThere are two key differences between the behavior of the logistic regression effect estimates and the linear regression effect estimates\n\nWhile the linear regression effect estimates are unbiased, the logistic regression effect estimates are increasingly biased as the variance of the random intercept increases (or the number of unobserved variables increases in our simply polygenic simulation)\nWe see that the effect estimates across simulations flatten out but usual C.I. still have good coverage. This is because the standard errors increasing with \\(\\sigma^2_0\\). In contrast, the effect estimates from logistic regression don’t seem to be flattened out. Probably, the likelihood surface should be less peaked than it is. The linear regression can accomplish this by increasing the estimated residual variance, the logisitic regression can’t do that.\n\nWe are observinAt the end of the day, the likelihood should be much flatter w.r.t. the parameter of interest to account for the fact that there is so much uncertainty in the background risk.\nThe linear model has a mechanism for flattening the likelihood– by increasing the residual variance. However in the case of logistic regression there is not such option."
  },
  {
    "objectID": "research/random_intercepts/index.html#quick-check",
    "href": "research/random_intercepts/index.html#quick-check",
    "title": "Random intercepts",
    "section": "Quick check",
    "text": "Quick check\nWe should be concerned that we are underestimating the standard errors. Here we compare the MLE under the standard logistic model and the random intercept model. We approximate the random intercept model by sampling the intercept \\(m=100\\) times for each obsevation, and weighting each observation in the new regression by \\(w=0.01\\).\nUnsurprisingly, we see better behavior from the (approximate) random intercept model. Across several simulation scenarios I’ve checked, I see that the standard error is ~\\(40\\%\\) larger compared to the fixed intercept GLM fit. That would have huge implications for the BFs we calculate.\n\n\nCode\nset.seed(2)\n\nn <- 1000\nb0 <- -1\nb <- 1\nstd0 <- 4\n\nx <- rnorm(n)\nlogit <- b0 + x * b + rnorm(n) * std0\ny <- rbinom(n, 1, 1 / (1 + exp(-logit)))\n\nfit <- glm(y ~ x, family='binomial')\ncoef <- summary(fit)$coef\n\nfit <- glm(y ~ x, family='binomial')\ncoef <- summary(fit)$coef\n\nm <- 100\ny2 <- rep(y, m)\nx2 <- rep(x, m)\no <- rnorm(n*m) * std0\nw <- rep(1/m, n*m)\nfit2 <- glm(y2 ~ x2 + offset(o), weights = w, family='binomial')\n\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n\nCode\ncoef2 <- summary(fit2)$coef\n\n\n\n\nCode\ncoef %>%\n    kbl(caption=cap) %>%\n    kable_classic(full_width = T, html_font = \"Cambria\") \n\n\n\n\nAs the number of unobserved variables increases, the coverage drops off.\n \n  \n      \n    Estimate \n    Std. Error \n    z value \n    Pr(>|z|) \n  \n \n\n  \n    (Intercept) \n    -0.3697446 \n    0.0651748 \n    -5.673125 \n    0e+00 \n  \n  \n    x \n    0.3165497 \n    0.0645630 \n    4.902957 \n    9e-07 \n  \n\n\n\n\n\nCode\ncoef2 %>%\n    kbl(caption=cap) %>%\n    kable_classic(full_width = T, html_font = \"Cambria\") \n\n\n\n\nAs the number of unobserved variables increases, the coverage drops off.\n \n  \n      \n    Estimate \n    Std. Error \n    z value \n    Pr(>|z|) \n  \n \n\n  \n    (Intercept) \n    -1.006124 \n    0.1071652 \n    -9.388533 \n    0 \n  \n  \n    x2 \n    0.853390 \n    0.1061993 \n    8.035738 \n    0"
  },
  {
    "objectID": "research/random_intercepts/index.html#a-slighly-more-calibrated-example-for-genetics",
    "href": "research/random_intercepts/index.html#a-slighly-more-calibrated-example-for-genetics",
    "title": "Random intercepts",
    "section": "A slighly more calibrated example for genetics",
    "text": "A slighly more calibrated example for genetics\nSuppose we had a trait with a heritability of liability of \\(0.2\\). That is, the proportion of variance in liability explained by genotype is \\(20\\%\\). Suppose that we are considering a locus with one causal variant that explains \\(1\\%\\) of the heritable component of variation.\nWe can see that the total variance of the genetic component is \\[\n\\nu = \\frac{\\pi^2}{3} \\frac{h^2}{1 - h^2}\n\\]\nWe will assume that our genotype is normalized, and that the normalized effect is \\(b \\sim N(0, \\sigma^2)\\) where \\(\\sigma^2 = 0.01 \\times \\nu\\).\n\n\nCode\nsimulation_logistic_poly <- function(n, b0, h2, rho){\n    nu <- pi^2/3 * h2/(1-h2)\n    sigma2 <- 0.01 * nu \n    x <- rnorm(n)\n    b <- rnorm(1, sd= sqrt(sigma2))\n    eps <- rnorm(n, sd = sqrt(nu - sigma2))\n    logit <- b0 + x * b + eps\n    y <- rbinom(n, 1, 1 / (1 + exp(-logit)))\n    fit <- glm(y ~ x, family='binomial')\n    coef <- summary(fit)$coef\n    res <-list(n=n, h2=h2, rho=rho, b0 = b0, b=b, b0hat = coef[1, 1], b0_shat=coef[1, 2], bhat = coef[2,1], shat = coef[2, 2]) |> list2tibble() %>% mutate(z = bhat/shat)\n    return(res)\n}\n\nsimrep <- function(k, h2){\n    purrr::map_dfr(1:k, ~simulation_logistic_poly(10000, -1, h2, 0.01))\n}\n\nh2 <- c(0.1, 0.2, 0.5, 0.9)\nres <- purrr::map_dfr(h2, ~simrep(50, .x))\n\n\n\n\nCode\ncap <- 'As the number of unobserved variables increases, the coverage drops off.'\nres %>%\n    rowwise() %>%\n    mutate(covered = (b < bhat + 2*shat) & (b > bhat - 2*shat)) %>%\n    ungroup() %>%\n    group_by(h2, rho) %>%\n    summarize(coverage=mean(covered)) %>%\n    kbl(caption=cap) %>%\n    kable_classic(full_width = T, html_font = \"Cambria\")\n\n\n`summarise()` has grouped output by 'h2'. You can override using the `.groups`\nargument.\n\n\n\n\nAs the number of unobserved variables increases, the coverage drops off.\n \n  \n    h2 \n    rho \n    coverage \n  \n \n\n  \n    0.1 \n    0.01 \n    0.94 \n  \n  \n    0.2 \n    0.01 \n    0.86 \n  \n  \n    0.5 \n    0.01 \n    0.46 \n  \n  \n    0.9 \n    0.01 \n    0.10 \n  \n\n\n\n\n\n\n\nCode\nres %>% make_forrest(column_name='h2') + facet_wrap(vars(h2), nrow=1)\n\n\n\n\n\nEffect estimates are unbiased\n\n\n\n\n\n\nCode\n# simulation where sigma2 is fixed, but h2 changes\nsigma2 <- 0.01\nh2 <- c(0.1, 0.2, 0.5, 0.9)\nnu <- pi^2/3 * h2/(1-h2)\nrho <- sigma2 / nu\n\nsimrep <- function(k, h2, rho){\n    purrr::map_dfr(1:k, ~simulation_logistic_poly(10000, -1, h2, rho))\n}\n\nres <- purrr::map2_dfr(h2,rho, ~simrep(50, .x, .y))\n\n\n\n\nCode\nres %>%\n    ggplot(aes(x=z)) + geom_histogram() + facet_wrap(vars(h2))\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe standard logistic distribution has a variance of \\(\\pi^2/3\\)\n\n\nCode\ncap <- 'As the number of unobserved variables increases, the coverage drops off.'\nres %>%\n    rowwise() %>%\n    mutate(covered = (b < bhat + 2*shat) & (b > bhat - 2*shat)) %>%\n    ungroup() %>%\n    group_by(h2, rho) %>%\n    summarize(coverage=mean(covered)) %>%\n    kbl(caption=cap) %>%\n    kable_classic(full_width = T, html_font = \"Cambria\")\n\n\n`summarise()` has grouped output by 'h2'. You can override using the `.groups`\nargument.\n\n\n\n\nAs the number of unobserved variables increases, the coverage drops off.\n \n  \n    h2 \n    rho \n    coverage \n  \n \n\n  \n    0.1 \n    0.0273567 \n    0.94 \n  \n  \n    0.2 \n    0.0121585 \n    0.98 \n  \n  \n    0.5 \n    0.0030396 \n    0.42 \n  \n  \n    0.9 \n    0.0003377 \n    0.04 \n  \n\n\n\n\n\n\n\nCode\nres %>% make_forrest(column_name='h2') + facet_wrap(vars(h2), nrow=1)\n\n\n\n\n\nEffect estimates are unbiased"
  },
  {
    "objectID": "research/constrained_multinomial_stickbreaking/index.html",
    "href": "research/constrained_multinomial_stickbreaking/index.html",
    "title": "Constrained multinomial stick-breaking",
    "section": "",
    "text": "Code\nsigmoid <- function(x){1/(1 + exp(-x))}\n\ntilde_pi2pi <- function(tpi){\n  tmp <- c(1, head(cumprod(1 - tpi), -1))\n  pi <- tmp * tpi\n  pi <- c(pi, (1 - sum(tmp * pi)))\n  return(pi)\n}\n\nmake_pi <- function(K, b0, b, x){\n  psi <-  do.call(cbind, purrr::map(b0, ~b + x + .x))\n  tilde_pi <- sigmoid(psi)\n  tpi <- tilde_pi[12, ]\n  pi <- do.call(rbind, purrr::map(1:nrow(tilde_pi), ~tilde_pi2pi(tilde_pi[.x,])))\n  return(pi)\n}\n\nplot_pi <- function(pi, idx, x){\n  par(mfrow = c(1, length(idx)))\n  K <- ncol(pi)\n  for(i in idx){\n    plot(1:K, pi[i,], type = 'b', xlab = 'K', ylab = 'prob', main=paste0('x = ', x[i]))\n  }\n}\n\n\n\nShared\n\\(\\psi_k \\equiv \\psi\\; \\forall\\  k \\in[0, K-1]\\)\n\n\nCode\nK <- 20\nb0 <- rep(0, K)\nb <- 1\nx <- seq(-3, 3.2, by=0.2)\npi <- make_pi(K, b0, b, x)\nplot_pi(pi, c(6, 11, 21, 26), x)\n\n\n\n\n\n\n\nFixed prediction, seperate intercept\n\n\nCode\nK <- 10\nb0 <- rep(0, K)\nb <- 1\nx <- seq(-3, 3, by=0.2)\npi <- make_pi(K, b0, b, x)\nplot_pi(pi, c(6, 11, 21, 26), x)\n\n\n\n\n\n\n\nCode\nK <- 10\nb0 <- rnorm(10)\nb <- 1\nx <- seq(-3, 3, by=0.2)\npi <- make_pi(K, b0, b, x)\nplot_pi(pi, c(6, 11, 21, 26), x)\n\n\n\n\n\n\n\nCode\nK <- 10\nb0 <- 1:K\nb <- 1\nx <- seq(-5, 5, by=0.1)\npi <- make_pi(K, b0, b, x)\nplot(pi[40,])\n\n\n\n\n\n\n\nCode\nK <- 10\nb0 <- rnorm(K)\nb <- 1\nx <- seq(-5, 5, by=0.1)\npi <- make_pi(K, b0, b, x)\n\npar(mfrow = c(2, 3))\nplot(pi[10,])\nplot(pi[20,])\nplot(pi[30,])\nplot(pi[40,])\nplot(pi[50,])\nplot(pi[60,])"
  },
  {
    "objectID": "research/multinomial_stickbreaking/index.html",
    "href": "research/multinomial_stickbreaking/index.html",
    "title": "Multinomial stick-breaking",
    "section": "",
    "text": "We’re interested in modelling multinomial or categorical data in the case where the probability of each category depends on side information. For \\(\\pi: \\mathcal X \\rightarrow \\Delta^{K-1}\\)\n\\[\n{\\bf y} \\sim \\text{Multinomial}(n, {\\bf \\pi}({\\bf x})) \\\\\n\\]\nCommonly \\(\\pi({\\bf x})\\) is written as a composition \\(\\pi = \\sigma \\circ \\eta\\), where \\(\\sigma: \\mathbb R^K \\rightarrow \\Delta^{K-1}\\) is the softmax functions defined element wise as \\(\\sigma({\\bf \\eta})_i = \\left(\\frac{e^{\\eta_i}}{\\sum_{j=1}^K e^{\\eta_j}}\\right)\\), and \\(\\eta:\\mathcal X \\rightarrow \\mathbb R^K\\) is some other function mapping the covariates \\({\\bf x}\\) to a set of unormalized log probabilities.\nThe trouble with this formulation is it is not easy to express uncertainty in the map \\(\\eta\\). As a simple example consider multinomial linear regression where \\(\\eta(z)_k = \\beta_k^T z\\) for some \\(\\beta_k \\in \\mathbb R^d\\). \\(\\pi = \\sigma \\circ \\eta\\) is differential, and point estimates of \\(B =\\{\\beta_k\\}_{k=1, \\dots, K}\\) could be obtained through gradient based optimization. In contrast if we take a Bayesian approach and specify a prior on \\(\\beta_k \\sim g_k\\; k \\in [K]\\) obtaining the posterior distribution over \\(B\\) involves evaluating a nasty integral of the soft max.\n\\[\n\\int_{B} \\sigma(\\eta(z ; B)) dB\n\\]\nThere is plenty of work on bounding softmax with functions that are easier to integrate (1,2), but it is hard problem to get anlytic bounds that are easy to work.\nThere is also quite a bit of work developing bounds for the sigmoid function (softmax with \\(K=2\\), usually people describe softmax as a generalization of sigmoid to \\(K > 2\\)). In particular, techniques for constructing local approximations are popular in variational inference (3,4). These local approximations are tight at a point, but the quality of the bound decays as you get far from that point. Thus, these approximation techniques require selecting/optimizing at what point the bound is tight.\nWe’re operating under the assumption that it is easier to construct good bounds for the sigmoid function compared to the softmax function. We are going to explore a construction of the Categorical/Multinomial distribution that let us utilize these bounds."
  },
  {
    "objectID": "research/multinomial_stickbreaking/index.html#multinomial-stick-breaking",
    "href": "research/multinomial_stickbreaking/index.html#multinomial-stick-breaking",
    "title": "Multinomial stick-breaking",
    "section": "Multinomial stick breaking",
    "text": "Multinomial stick breaking\nThe the multinomial logit construction \\(\\eta\\) is a set of unnormalized log probabilities This is not the only way to construct a multinomial distribution. We can also use a stick breaking construction. In stick breaking we start with a “stick” of length \\(1\\). At the first step we break off a fraction of the stick \\(p_1\\). The remainder of the stick is now length \\(1 - p_1\\). At each successive step we break off a fraction of the remaining stick. After \\(K-1\\) breaks we have broken the stick into \\(K\\) pieces, giving a discrete probability distribution over \\(K\\) categories. Clearly, we can use this process to construct and distribution \\(\\pi\\) over \\(K\\) categories where\n\\[\n\\begin{aligned}\n\\pi_1 &= p_1 \\\\\n\\pi_k &= p_k \\prod_{j < k}(1 - p_k)\n\\end{aligned}\n\\]\nNoting that \\(\\left(1 - \\sum_{j < k} \\pi_j \\right)\\) is the length of the remaining stick after \\(k-1\\) breaks, we can also write\n\\[\n\\begin{aligned}\n\\pi_k &= p_k \\left(1 - \\sum_{j < k} \\pi_j \\right)\n\\end{aligned}\n\\]\nIn the stick breaking construction, \\(\\nu_k,\\; k \\in[K-1]\\) will be a set of log odds such that \\(p_k = \\sigma(\\nu_k)\\) gives the proportion of the stick broken off at step \\(k\\). Using the stick breaking constructiong we can write the multinational pmf as a product of binomial pmfs.\n\\[\n\\text{Multinomial}({\\bf y}; n, \\pi) = \\prod_{k=1}^{K-1} \\text{Binomial}(y_k; n_k, p_k)\n\\]\nWhere \\(n_k = n - \\sum_{j < k} y_j\\) counts the number of remaining trials, conditional on the first \\(k-1\\) draws. This constructing is not new, it has been proposed by several authors (5,6).\nTo do multinomial regression we will write \\(\\nu_k = \\beta_k^T {\\bf z}\\). \\(nu_k\\) gives the log odds of selecting category \\(k\\) given that we did not select category \\(1, \\dots, k-1\\)."
  },
  {
    "objectID": "research/multinomial_stickbreaking/index.html#stick-breaking-for-variational-inference",
    "href": "research/multinomial_stickbreaking/index.html#stick-breaking-for-variational-inference",
    "title": "Multinomial stick-breaking",
    "section": "Stick breaking for variational inference",
    "text": "Stick breaking for variational inference\nThe stick breaking construction is particular useful for variational inference. The multinomial log likelihood can be written as a sum of \\(K-1\\) terms, each a binomial log-likelihood. By selecting a variational approximation where the \\(\\nu_k\\) factorize, the variational objective can be optimized in an embarrassingly parallel fashion– the multinomial regression reduces to a set of \\(K-1\\) independent binomial regression problems. Each of these problems still requires additional approximation of the sigmoid function for tractable inference, but these can be dealt with more easily."
  },
  {
    "objectID": "research/multinomial_stickbreaking/index.html#a-distribution-of-pi",
    "href": "research/multinomial_stickbreaking/index.html#a-distribution-of-pi",
    "title": "Multinomial stick-breaking",
    "section": "A distribution of \\(\\pi\\)",
    "text": "A distribution of \\(\\pi\\)\nWhile stick breaking can be used to construct any discrete distribution, we should take note that the distribution on \\(\\pi\\) is dependent on the distribution we specify for the breakpoints and\nThe Dirichlet \\(Dir((\\alpha_1, \\dots, \\alpha_K))\\) can be constructed through stick breaking, where the break points are\n\\[p_k \\sim Beta(\\alpha_k, \\sum_{j > k } \\alpha_j)\\]\nAgain \\(\\pi_1 = p_1\\), and \\(p_k = (1 - \\sum_{j < k} \\pi_j) p_k\\). If \\(\\alpha_i = \\alpha\\; \\forall i \\in [K]\\) then then the Dirichlet is said to be symmetric– permuting category labels won’t change the likelihood of the sample. Notice that in this case \\(p_k \\sim Beta (\\alpha, (K- k) \\alpha)\\). We should expect to break off smaller fractions of the stick for small \\(k\\) than for large \\(k\\). This makes sense. A necessary condition for the Dirichlet to be exchangeable is that the stick lengths have the same marginal distribution. In order for the stick lengths to have the same marginal distribution, at each successive step we need to balance out the fact that the stick is getting shorter by taking larger fraction of the stick at each step (ultimately \\(\\mathbb E[p_{K-1}] = \\frac{1}{2}\\)).\nIn the code below we simulate the Dirichlet distribution using stick breaking with a Beta distribution. We see that across 10000 simulations each category is equally likely to show up on top.\n\n\nCode\n#' Sample from a Dirichlet distribution using the stick breaking construction\ndirichlet_from_beta_stick_breaking <- function(alpha, K){\n  if(length(alpha) == 1){ \n    alpha <- rep(alpha, K)\n  }\n  beta <- rev(cumsum(rev(alpha))) - alpha # sum {j < k} \\alpha_j\n  p <- rbeta(K, alpha, beta)\n  tmp <- c(1, head(cumprod(1 - p), -1))\n  pi <- c(p * tmp)\n  return(pi)\n}\n\n# each component equally likely to have the most probability mass\ntable(purrr::map_int(1:10000, ~which.max(\n  dirichlet_from_beta_stick_breaking(1, 4))))\n\n\n\n   1    2    3    4 \n2458 2539 2492 2511 \n\n\nTODO: sample for \\(K=3\\)\nQ: What distribution of \\(\\nu_k\\) would give an exchangeable distribution for \\(\\pi\\) (basically, what is the stick-breaking construction for a symmetric Dirichlet?)"
  },
  {
    "objectID": "research/multinomial_stickbreaking/index.html#ordering-of-the-categories",
    "href": "research/multinomial_stickbreaking/index.html#ordering-of-the-categories",
    "title": "Multinomial stick-breaking",
    "section": "Ordering of the categories",
    "text": "Ordering of the categories\nSuccessive categories seem to have less and less information, as \\(n_k \\leq n_j\\) for \\(k > j\\). It seems odd that permuting the category labels would change how certain we are about each \\(\\nu_k\\). Can we make sense of this?"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research Notes",
    "section": "",
    "text": "Citations with quarto\n\n\n\n\n\nA quick demo on how I set up citations on this website\n\n\n\n\n\n\nJan 3, 2024\n\n\nKarl Tayeb\n\n\n\n\n\n\n\n\nConstrained multinomial stick-breaking\n\n\n\n\n\nMultinomial stickbreaking is an easy\n\n\n\n\n\n\nApr 1, 2023\n\n\nKarl Tayeb\n\n\n\n\n\n\n\n\nImplementation polynomial approximation VB\n\n\n\n\n\nRough implementation of of approximate VB regression, where we approximate the data likelihood with a polynomial. We implement a mean field gaussian variational approximation. We demonstrate the method on Gaussian linear model (no approximation), and Bernoulli-logit (logistic regression) and Poisson-log (poisson regression) models with varying approximation degree.\n\n\n\n\n\n\nMar 15, 2023\n\n\nKarl Tayeb\n\n\n\n\n\n\n\n\nLogistic SER comparison\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nLogistic SER coverage under polygenic model\n\n\n\n\n\nWe are interested in knowing if the inferences under the logistic SER are biased when phenotypes are simulated under a polygenic model. In these simulations we simulate one causal variant in the locus, and model unlinked genetic effects with a random intercept. The SER fits the ordinary/marginal regression model at each step, without accounting for this extra variability in liability. Nonetheless, we see good coverage of the credible sets.\n\n\n\n\n\n\nJan 9, 2024\n\n\n\n\n\n\n\n\nMinimal working example for GSEA SuSiE\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nMode seeking in mean field VB for RSS + sparse prior\n\n\n\n\n\nUsing polynomial approximations to perform Bayesian regression\n\n\n\n\n\n\nApr 1, 2023\n\n\nKarl Tayeb\n\n\n\n\n\n\n\n\nMultinomial stick-breaking\n\n\n\n\n\nExploration of multinomial stickbreaking\n\n\n\n\n\n\nMar 19, 2023\n\n\nKarl Tayeb\n\n\n\n\n\n\n\n\nPolynomial approximation SuSiE\n\n\n\n\n\nWe extend the sum of single effects regression to support arbitrary likelihood and link function by representing (approximately) the log likelihood of each observations as a polynomial in the linear prediction. Once this approximation is made we can treat inference in multiple likelihoods with a uniform inference procedure. Furthermore, we can make the polynomial approximation arbitrarily precise by increasing the degree of the approximation.\n\n\n\n\n\n\nMar 15, 2023\n\n\nKarl Tayeb\n\n\n\n\n\n\n\n\nPolynomial approximation for variational Bayes\n\n\n\n\n\nUsing polynomial approximations to perform Bayesian regression\n\n\n\n\n\n\nMar 15, 2023\n\n\nKarl Tayeb\n\n\n\n\n\n\n\n\nRandom intercepts\n\n\n\n\n\nDemonstrate the behavior of fitting linear and logistic regression to data generated from a random intercept mode. OLS gives unbiased estimates for the Gaussian linear model, but the estimated effects are biased in the GLM.\n\n\n\n\n\n\nDec 21, 2023\n\n\n\n\n\n\n\n\nWakefield ABF Accuracy\n\n\n\n\n\nWe noticed that the error in Wakefield’s ABF grows with increasing sample size. This is surprising, because at these large sample sizes asymptotic normality kicks in. The issue can be more clearly seen by considering the ABF as an integral over an approximation to the likelihood ratio. The approximation used in the ABF ensure that the approximate likelihood ratio at \\(0\\) is 1, but that the curvature and mode of the approximation are determined by the asymptotic distribution of the MLE. This leads to a gap in the approximate likelihood ratio and the exact likelihood ratio around the mode. As the normal approximation becomes more peaked, the accuracy of the ABF is increasingly determined by the quality of the likelihood ratio approximation in a small neighborhood around the mode which is off by this gap, which appears to grow with sample size.\n\n\n\n\n\n\nApr 25, 2023\n\n\nKarl Tayeb\n\n\n\n\n\n\n\n\nX-shift invariant prior for logistic regresion\n\n\n\n\n\nwhat prior makes the posterior invariant to shifts in the covariates?\n\n\n\n\n\n\nJan 16, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Karl Tayeb’s Website",
    "section": "",
    "text": "This is my research website. For now, that means a collection of notes that are useful to me. Feel free to look around."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! I’m Karl Tayeb. I’m a PhD Student at the University of Chicago working with Matthew Stephens and Yoav Gilad. Before UChicago I was at Johns Hopkins working with Alexis Battle.\nThis is my research website. For now, that means a collection of notes that are useful to me. Feel free to look around."
  },
  {
    "objectID": "presentations/template/presentation.html#plan",
    "href": "presentations/template/presentation.html#plan",
    "title": "Generalizing SuSiE",
    "section": "Plan",
    "text": "Plan\n\nTalk about our current approach, GIBSS\nTalk about (generalized) additive models"
  },
  {
    "objectID": "presentations/template/presentation.html#ibss",
    "href": "presentations/template/presentation.html#ibss",
    "title": "Generalizing SuSiE",
    "section": "IBSS",
    "text": "IBSS\n\nAlgorithm 1 from (1)"
  },
  {
    "objectID": "presentations/template/presentation.html#generalized-ibss",
    "href": "presentations/template/presentation.html#generalized-ibss",
    "title": "Generalizing SuSiE",
    "section": "Generalized IBSS",
    "text": "Generalized IBSS\n\nGLM \\(\\mu(x) = \\mathbb E[Y |X = x]\\). \\(g(\\mu(x)) = \\sum_l \\psi_l = \\sum_l x^T \\beta_l\\)\nIdea: plug in point estimates \\({\\hat\\psi} _j, j\\neq l\\) to estimate \\(\\psi_l\\)"
  },
  {
    "objectID": "presentations/template/presentation.html#variational-objective",
    "href": "presentations/template/presentation.html#variational-objective",
    "title": "Generalizing SuSiE",
    "section": "Variational Objective",
    "text": "Variational Objective\nlog joint:\n\\[\\begin{align}\nF(\\beta_1, \\dots, \\beta_L) &= \\log p(y | \\sum_l(\\psi_l)) + \\sum_{l=1}^L\\log p(\\psi_l), \\\\\n\\psi_l &= X \\beta_l, \\\\\n\\beta_l &\\sim g_l\n\\end{align}\\]\nELBO:\n\\[\\begin{align}\n\\mathcal F(q_1, \\dots, q_L) &= \\mathbb E_q \\left[ F(\\beta_1, \\dots \\beta_L) \\right] + \\sum_l H(q_l) \\\\\nq(\\beta_1, \\dots, \\beta_L) &= \\prod q_l(\\beta_l)\n\\end{align}\\]"
  },
  {
    "objectID": "presentations/template/presentation.html#gibss-as-a-first-order-approximation-to-cavi",
    "href": "presentations/template/presentation.html#gibss-as-a-first-order-approximation-to-cavi",
    "title": "Generalizing SuSiE",
    "section": "GIBSS as a first order approximation to CAVI",
    "text": "GIBSS as a first order approximation to CAVI\nExponential family \\(f(y | \\eta) \\propto \\exp \\{y \\eta - A(\\eta) \\}\\), \\(A\\) convex. Additive model: \\(\\eta = \\sum \\eta_l\\)\n\\[\\begin{align}\n\\mathbb E_q[\\log f(y | \\eta)]\n&= \\mathbb E_{q_l} [\\mathbb E_{q_{-l}}[\\log f(y | \\eta_l + \\eta_{-l})] | \\eta_l] \\\\\n&\\leq \\mathbb E_{q_l} [\\mathbb E_{q_{-l}}[\\log f(y | \\eta_l + \\bar\\eta_{-l}) + \\nabla \\log f_{\\eta_{-l}}(\\bar \\eta_{-l})) (\\eta_{-l} - \\bar \\eta_{-l})] | \\eta_l] \\\\\n&= \\mathbb E_{q_l} [\\log f(y | \\eta_l + \\bar\\eta_{-l})] \\\\\n\\end{align}\\]\nNotes:\n\nFor exponential families, we upper bound the ELBO for each coordinate update, a bit awkward.\nWe use a different approximation at each step, not clear if the algorithm converges."
  },
  {
    "objectID": "presentations/template/presentation.html#generalized-ser",
    "href": "presentations/template/presentation.html#generalized-ser",
    "title": "Generalizing SuSiE",
    "section": "Generalized SER",
    "text": "Generalized SER"
  },
  {
    "objectID": "presentations/template/presentation.html#approximating-generalized-ser",
    "href": "presentations/template/presentation.html#approximating-generalized-ser",
    "title": "Generalizing SuSiE",
    "section": "Approximating Generalized SER",
    "text": "Approximating Generalized SER"
  },
  {
    "objectID": "presentations/template/presentation.html#additive-effects-model",
    "href": "presentations/template/presentation.html#additive-effects-model",
    "title": "Generalizing SuSiE",
    "section": "Additive effects model",
    "text": "Additive effects model\n\\[\\begin{align}\n{\\bf y} | \\mu & \\sim N(\\mu, \\sigma^2) \\\\\n\\mu &= \\sum_l \\mu_l \\\\\n\\mu_l &\\sim g_l\n\\end{align}\\]\nSuSiE is the special case where \\(g_l \\sim SER\\)"
  },
  {
    "objectID": "presentations/template/presentation.html#backfitting-in-the-additive-regression-model",
    "href": "presentations/template/presentation.html#backfitting-in-the-additive-regression-model",
    "title": "Generalizing SuSiE",
    "section": "Backfitting in the additive regression model",
    "text": "Backfitting in the additive regression model\n\n(2)"
  },
  {
    "objectID": "presentations/template/presentation.html#rationale-for-maximum-likelihood-estimation",
    "href": "presentations/template/presentation.html#rationale-for-maximum-likelihood-estimation",
    "title": "Generalizing SuSiE",
    "section": "Rationale for maximum likelihood estimation",
    "text": "Rationale for maximum likelihood estimation\n\nFig 1. reproduced from (3)"
  },
  {
    "objectID": "presentations/template/presentation.html#additive-effects-model-1",
    "href": "presentations/template/presentation.html#additive-effects-model-1",
    "title": "Generalizing SuSiE",
    "section": "Additive effects model",
    "text": "Additive effects model\n\n(1)"
  },
  {
    "objectID": "presentations/template/presentation.html#generalized-additive-model",
    "href": "presentations/template/presentation.html#generalized-additive-model",
    "title": "Generalizing SuSiE",
    "section": "Generalized additive model",
    "text": "Generalized additive model\n\\(g\\) the canonical link function, \\(\\mathbb E[y] = g^{-1}(\\eta)\\)\n\\[\\begin{align}\ny | \\eta &\\sim p_{g^{-1}(\\eta)} \\\\\n\\eta &= \\sum_l \\eta_l \\\\\n\\eta_l &\\sim g_l\n\\end{align}\\]"
  },
  {
    "objectID": "presentations/template/presentation.html#generalized-local-scoring",
    "href": "presentations/template/presentation.html#generalized-local-scoring",
    "title": "Generalizing SuSiE",
    "section": "Generalized local scoring",
    "text": "Generalized local scoring\n\n(2)"
  },
  {
    "objectID": "presentations/template/presentation.html#comparison-to-gibss",
    "href": "presentations/template/presentation.html#comparison-to-gibss",
    "title": "Generalizing SuSiE",
    "section": "Comparison to GIBSS",
    "text": "Comparison to GIBSS\n\nGeneralized local scoring iteratively updates weights, and fits the weighted additive model\nGIBSS performs IRLS for each variable, fixing \\(L-1\\) additive components\nEstimates don’t"
  },
  {
    "objectID": "presentations/template/presentation.html#variational-objective-1",
    "href": "presentations/template/presentation.html#variational-objective-1",
    "title": "Generalizing SuSiE",
    "section": "Variational Objective",
    "text": "Variational Objective\nlog joint:\n\\[\\begin{align}\nF(\\beta_1, \\dots, \\beta_L) &= \\log p(y | \\sum_l(\\psi_l)) + \\sum_{l=1}^L\\log p(\\psi_l), \\\\\n\\psi_l &= X \\beta_l, \\\\\n\\beta_l &\\sim g_l\n\\end{align}\\]\nELBO:\n\\[\\begin{align}\n\\mathcal F(q_1, \\dots, q_L) &= \\mathbb E_q \\left[ F(\\beta_1, \\dots \\beta_L) \\right] + \\sum_l H(q_l) \\\\\nq(\\beta_1, \\dots, \\beta_L) &= \\prod q_l(\\beta_l)\n\\end{align}\\]"
  },
  {
    "objectID": "presentations/template/presentation.html#mode-rather-than-mean",
    "href": "presentations/template/presentation.html#mode-rather-than-mean",
    "title": "Generalizing SuSiE",
    "section": "Mode rather than mean?",
    "text": "Mode rather than mean?\n\\[\\begin{align}\nF(\\beta_1, \\dots, \\beta_L) &= \\log p(y | \\sum_l(\\psi_l)) + \\sum_{l=1}^L\\log p(\\psi_l), \\\\\n\\end{align}\\]\n\\[\\begin{align}\n\\beta_l^* = \\arg\\max_{\\beta_l}  F(\\beta_1, \\dots, \\beta_L)\n\\end{align}\\]\nNotes:\n\nConverges to a mode of the posterior.\nJust iterative stepwise regression with fixed \\(L\\).\nSeems undesirable to include \\(\\beta_l\\) for effects with weak evidence.\nResembles CAVI when \\(q(\\gamma_l)\\) is concentrated on highly correlated variables.\nCan still report SER posteriors for each effect \\(l\\)."
  },
  {
    "objectID": "presentations/template/presentation.html#laplace-vs-wakefield-review",
    "href": "presentations/template/presentation.html#laplace-vs-wakefield-review",
    "title": "Generalizing SuSiE",
    "section": "Laplace vs Wakefield (Review)",
    "text": "Laplace vs Wakefield (Review)\n\nFigure 1: Wakefield’s ABF can be order of magnitude off when the \\(z\\)-score is large"
  },
  {
    "objectID": "presentations/template/presentation.html#problems-with-wakefield-review",
    "href": "presentations/template/presentation.html#problems-with-wakefield-review",
    "title": "Generalizing SuSiE",
    "section": "Problems with Wakefield (Review)",
    "text": "Problems with Wakefield (Review)\n\\[\n\\text{ABF} = \\sqrt{\\frac{V+W}{V}} \\exp (- \\frac{z^2}{2} \\frac{W}{V + W})\n\\]"
  },
  {
    "objectID": "presentations/template/presentation.html#susie-rss-and-the-wakefield-bf",
    "href": "presentations/template/presentation.html#susie-rss-and-the-wakefield-bf",
    "title": "Generalizing SuSiE",
    "section": "SuSiE-RSS and the Wakefield BF",
    "text": "SuSiE-RSS and the Wakefield BF\n\nRecall that Wakefield’s ABF is not accurate when the \\(z\\)-score is large\nApplying SuSiE-RSS to summary statistics from some other model besides Gaussian linear model"
  },
  {
    "objectID": "presentations/template/presentation.html#how-large-do-the-z-scores-need-to-be",
    "href": "presentations/template/presentation.html#how-large-do-the-z-scores-need-to-be",
    "title": "Generalizing SuSiE",
    "section": "How large do the \\(z\\)-scores need to be?",
    "text": "How large do the \\(z\\)-scores need to be?"
  },
  {
    "objectID": "presentations/template/presentation.html#a",
    "href": "presentations/template/presentation.html#a",
    "title": "Generalizing SuSiE",
    "section": "A",
    "text": "A\n\\[\\begin{align}\n\\mathcal F(q_1, \\dots, q_L) = \\mathbb E_q \\left[ F(\\beta_1, \\dots \\beta_L) \\right] + \\sum_l H(q_l)\n\\end{align}\\]\nCoordinate ascent on \\((\\beta_l)\\). For SuSiE, at each step fit an SER, but only return the posterior mode of the SER, rather than the mean. Essentially stepwise selection, but reports a posterior distribution for each effect. ## Backfitting\nMaximize the expected log likelihood\n\\[\n\\begin{align}\n\\mathbb E[ l(\\hat{\\eta}(X), Y)] = \\max_{\\eta} \\mathbb E[l(\\eta(X), Y)]\n\\end{align}\n\\]"
  },
  {
    "objectID": "presentations/template/presentation.html#overview",
    "href": "presentations/template/presentation.html#overview",
    "title": "Generalizing SuSiE",
    "section": "Overview",
    "text": "Overview\n\nPublication plan:\n\nGeneralized SuSiE via IBSS, emphasis on fine-mapping for non-Gaussian models\nGSEA with logistic SuSiE"
  },
  {
    "objectID": "presentations/template/presentation.html#fine-mapping-under-the-multivariate-gaussian-model",
    "href": "presentations/template/presentation.html#fine-mapping-under-the-multivariate-gaussian-model",
    "title": "Generalizing SuSiE",
    "section": "Fine-mapping under the multivariate Gaussian model",
    "text": "Fine-mapping under the multivariate Gaussian model\nMost fine-mapping methods assume summary statistics from marginal association studies are normally distributed, with covariance determined by LD 1\n\\[\\begin{align*}\n\\hat{ {\\bf z} }  \\sim N({\\bf z}, R)\n\\end{align*}\\]\nStatistical property of OLS– what if the marginal effects are coming from somewhere else?\nsee (4), (5), (6), (7)"
  },
  {
    "objectID": "presentations/template/presentation.html#what-do-we-want-to-accomplish",
    "href": "presentations/template/presentation.html#what-do-we-want-to-accomplish",
    "title": "Generalizing SuSiE",
    "section": "What do we want to accomplish?",
    "text": "What do we want to accomplish?\n\nEstablish when there is a problem with fine-mapping with summary stats from non-Gaussian models\n(Hopefully) find that these situations are not uncommon\n(Hopefully) demonstrate that GIBSS offers improvement in these situations\n(Fallback) advise people to fine map with summary statistics from linear models"
  },
  {
    "objectID": "presentations/template/presentation.html#three-horse-race",
    "href": "presentations/template/presentation.html#three-horse-race",
    "title": "Generalizing SuSiE",
    "section": "Three horse race",
    "text": "Three horse race\n\n\n\n\n\n\n\n\nMethod\nNotes\nSummary stats\n\n\n\n\nGeneralized IBSS\n“correct” model, hueristic algorithm\nNo\n\n\nLogistic + RSS\nad-hoc, actually used (8,9)\nYes\n\n\nLinear + RSS\nmis-specified model, correct algorithm\nYes"
  },
  {
    "objectID": "presentations/template/presentation.html#gibss-overview",
    "href": "presentations/template/presentation.html#gibss-overview",
    "title": "Generalizing SuSiE",
    "section": "GIBSS overview",
    "text": "GIBSS overview\n\nCompute univariate effect estimates using regression of choice, must return MLE and stderr\nCompute/approximate BFs and posterior means: Laplace, quadrature, etc.\nUse predictions as fixed offsets when updating next effect\nIterate until convergence (we don’t know if it converges)"
  },
  {
    "objectID": "presentations/template/presentation.html#key-questions",
    "href": "presentations/template/presentation.html#key-questions",
    "title": "Generalizing SuSiE",
    "section": "Key questions",
    "text": "Key questions\n\nGeneralized IBSS vs Logistic RSS\n\n\\(L=1\\) case reduces to IBSS-Laplace vs IBSS-ABF\nGIBSS-Laplace should be strictly better, but by how much, and when?\n\\(L > 1\\) all bets are off with Logistic RSS\n\nLinear SuSiE vs Logistic RSS\n\nHelpful to look at \\(L=1\\) case, Linear + Wakefield vs Logistic + Wakefield\nIn GWAS linear regression is often a good approximation to logistic regression\n\nLinear SuSiE vs Generalized IBSS\n\nWhen does linear SuSiE give reliable results?\nWhen does GIBSS provide and advantage?"
  },
  {
    "objectID": "presentations/template/presentation.html#potential-problems",
    "href": "presentations/template/presentation.html#potential-problems",
    "title": "Generalizing SuSiE",
    "section": "Potential problems",
    "text": "Potential problems\nLogistic + RSS\n\nCovariate of marginal \\(z\\)-scores do not correspond with LD\nUnder appreciated source of “LD Mismatch?”\nFollow-up: what is the correct covariance matrix?\nInherits problems from using ABF– not the most accurate\n\nLogistic GIBSS\n\nMarginal effect estimates are biased when there is a large genetic component"
  },
  {
    "objectID": "presentations/template/presentation.html#correlation-of-marginal-effects-in-logistic-regression",
    "href": "presentations/template/presentation.html#correlation-of-marginal-effects-in-logistic-regression",
    "title": "Generalizing SuSiE",
    "section": "Correlation of marginal effects in logistic regression:",
    "text": "Correlation of marginal effects in logistic regression:\n\nSimulate \\(\\begin{bmatrix}x_1 \\\\ x_2 \\end{bmatrix} \\sim N(\\begin{bmatrix}0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix}1 & \\rho \\\\ \\rho & 1 \\end{bmatrix})\\)\nSimulate \\(y\\) under a logistic model \\(y \\sim Bin(1, \\sigma(\\psi)), \\; \\psi := b_0 + b x_1\\)\n\\(cor(z_1, z_2) \\neq \\rho \\implies\\) LD matrix is the wrong covariance matrix"
  },
  {
    "objectID": "presentations/template/presentation.html#correlation-of-marginal-effects-cont",
    "href": "presentations/template/presentation.html#correlation-of-marginal-effects-cont",
    "title": "Generalizing SuSiE",
    "section": "Correlation of marginal effects (cont)",
    "text": "Correlation of marginal effects (cont)\nAn under-appreciated source of “LD mismatch”?\n\\(n = 500\\), \\(b_0 = -1\\), \\(b = 0, 1, 2, 3\\) 1\n\nmore simulations settings here"
  },
  {
    "objectID": "presentations/template/presentation.html#laplace-vs-wakefield-review-1",
    "href": "presentations/template/presentation.html#laplace-vs-wakefield-review-1",
    "title": "Generalizing SuSiE",
    "section": "Laplace vs Wakefield (Review)",
    "text": "Laplace vs Wakefield (Review)\n\nFigure 2: Wakefield’s ABF can be order of magnitude off when the \\(z\\)-score is large"
  },
  {
    "objectID": "presentations/template/presentation.html#problems-with-wakefield-review-1",
    "href": "presentations/template/presentation.html#problems-with-wakefield-review-1",
    "title": "Generalizing SuSiE",
    "section": "Problems with Wakefield (Review)",
    "text": "Problems with Wakefield (Review)\n!()[resources/abf_biased.png]\n!()[resources/abf_eq.png]"
  },
  {
    "objectID": "presentations/template/presentation.html#susie-rss-and-the-wakefield-bf-1",
    "href": "presentations/template/presentation.html#susie-rss-and-the-wakefield-bf-1",
    "title": "Generalizing SuSiE",
    "section": "SuSiE-RSS and the Wakefield BF",
    "text": "SuSiE-RSS and the Wakefield BF\n\nRecall that Wakefield’s ABF is not accurate when the \\(z\\)-score is large\nApplying SuSiE-RSS to summary statistics from some other model besides Gaussian linear model"
  },
  {
    "objectID": "presentations/template/presentation.html#how-large-do-the-z-scores-need-to-be-1",
    "href": "presentations/template/presentation.html#how-large-do-the-z-scores-need-to-be-1",
    "title": "Generalizing SuSiE",
    "section": "How large do the \\(z\\)-scores need to be?",
    "text": "How large do the \\(z\\)-scores need to be?"
  },
  {
    "objectID": "presentations/template/presentation.html#biased-effect-estimates-and-bfs",
    "href": "presentations/template/presentation.html#biased-effect-estimates-and-bfs",
    "title": "Generalizing SuSiE",
    "section": "Biased effect estimates (and BFs)",
    "text": "Biased effect estimates (and BFs)\nSimulation: one causal variant in the locus that explains \\(1\\%\\) of heritability of liability. \\(h^2 = 0.1, 0.2, 0.5, 0.9\\)\n\\[\\begin{align*}\ny \\sim Bin(1, \\sigma(\\psi)) \\\\\n\\psi = b_0 + b x + \\epsilon \\\\\n\\epsilon \\sim N(0, \\sigma^2)\n\\end{align*}\\]"
  },
  {
    "objectID": "presentations/template/presentation.html#biased-effect-estimates-and-bfs-1",
    "href": "presentations/template/presentation.html#biased-effect-estimates-and-bfs-1",
    "title": "Generalizing SuSiE",
    "section": "Biased effect estimates (and BFs)",
    "text": "Biased effect estimates (and BFs)"
  },
  {
    "objectID": "presentations/template/presentation.html#biased-effect-estimates-and-bfs-2",
    "href": "presentations/template/presentation.html#biased-effect-estimates-and-bfs-2",
    "title": "Generalizing SuSiE",
    "section": "Biased effect estimates (and BFs)",
    "text": "Biased effect estimates (and BFs)\n\n95% C.I. for different \\(h^2\\)"
  },
  {
    "objectID": "presentations/template/presentation.html#biased-effect-estimates-and-bfs-3",
    "href": "presentations/template/presentation.html#biased-effect-estimates-and-bfs-3",
    "title": "Generalizing SuSiE",
    "section": "Biased effect estimates (and BFs)",
    "text": "Biased effect estimates (and BFs)\n\nFor phenotypes with substantial \\(h^2\\) of liability, restricting our attention to a single locus will lead to biased effect estimates\nRemark: linear model doesn’t struggle with this issue because in practice we estimate the residual variance (or set conservatively)\nRemark:basically a random intercept model, but this seems a little different than the usual motivation for mixed model approaches."
  },
  {
    "objectID": "presentations/template/presentation.html#real-data-analsis",
    "href": "presentations/template/presentation.html#real-data-analsis",
    "title": "Generalizing SuSiE",
    "section": "Real data analsis",
    "text": "Real data analsis\n\nQ: if we reun SuSiE, GIBSS, Logistic + RSS on real case-control GWAS do we get qualitatively different results?\nDon’t know ground truth, hard to tell what is performing better\nReplication failure rate (RFR) among PIPs proposed in (10) may support claim that using GIBSS > Linear + RSS > Logistic + RSS\n\n\n\nOther ideas?\nDo you know of imbalanced case-control GWAS, survival GWAS, etc. GWAS on count based phenotypes, etc to test out?"
  },
  {
    "objectID": "presentations/template/presentation.html#simulation",
    "href": "presentations/template/presentation.html#simulation",
    "title": "Generalizing SuSiE",
    "section": "Simulation",
    "text": "Simulation\n\\[\n\\begin{align*}\ny_i &\\sim Bin\\left(1, \\sigma \\left(b_0 + \\sum_{j=1}^q b_j x_{ij} + \\delta\\right)\\right)\\\\\nb &\\sim N(0, \\sigma^2) \\\\\n\\delta &\\sim N(0, \\nu - q \\sigma^2)\\\\\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nValue\nDescription\n\n\n\n\n\\(X\\)\nStandardized genotypes\n\n\n\\(\\sigma^2\\)\nVariance of standardized effects, i.e. \\(b \\sim N(0, \\sigma^2)\\)\n\n\n\\(q\\)\nNumber of causal variants in locus\n\n\n\\(\\rho\\)\nFraction of variance of genetic component in-locus\n\n\n\\(k\\)\nFraction of cases (determines \\(b_0\\))\n\n\n\\(q\\sigma^2\\)\n(Expected) variance of genetic component in-locus\n\n\n\\(\\nu\\)\n\\(q \\sigma^2/\\rho\\), (expected) variance of genetic component\n\n\n\\(h^2\\)\n\\(\\nu / (\\nu + \\pi^2/3)\\), (expected) heritability of liability 1\n\n\n\n\n\n\nLiability threshold model, borrowed from (11)\n\n\nsee (11), and make sure this makes sense!"
  },
  {
    "objectID": "presentations/template/presentation.html#examples-where-susie-is-applied-to-non-gaussian-linear-summary-stats",
    "href": "presentations/template/presentation.html#examples-where-susie-is-applied-to-non-gaussian-linear-summary-stats",
    "title": "Generalizing SuSiE",
    "section": "Examples where SuSiE is applied to non-Gaussian linear summary stats",
    "text": "Examples where SuSiE is applied to non-Gaussian linear summary stats\n(8), Alzheimers meta analysis combining linear and logistic association studies (9) logistic-mixed model SAIGE + SuSiE"
  },
  {
    "objectID": "presentations/template/presentation.html#when-does-logistic-wakefield-perform-poorly",
    "href": "presentations/template/presentation.html#when-does-logistic-wakefield-perform-poorly",
    "title": "Generalizing SuSiE",
    "section": "When does Logistic + Wakefield perform poorly?",
    "text": "When does Logistic + Wakefield perform poorly?\n\n\n\naccidentally wiped the simulations, need to regenerate\napparently"
  },
  {
    "objectID": "presentations/template/presentation.html#dealing-with-intercept-covariates",
    "href": "presentations/template/presentation.html#dealing-with-intercept-covariates",
    "title": "Generalizing SuSiE",
    "section": "Dealing with intercept + covariates",
    "text": "Dealing with intercept + covariates\nA few options:\n\nEstimate in outer loop, treat as a fixed offset while estimating SERs\nRe-estimate covariate effects for each variable\nFound that reestimating intercept was helpful"
  },
  {
    "objectID": "presentations/template/presentation.html#univariate-bf",
    "href": "presentations/template/presentation.html#univariate-bf",
    "title": "Generalizing SuSiE",
    "section": "Univariate BF",
    "text": "Univariate BF\n\nLimiting BF"
  },
  {
    "objectID": "presentations/template/presentation.html#limiting-bf",
    "href": "presentations/template/presentation.html#limiting-bf",
    "title": "Generalizing SuSiE",
    "section": "Limiting BF",
    "text": "Limiting BF\nIdea put a normal prior on all covariates \\(\\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix} \\sim N(\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} I_{p-1} \\tau_0^{-1} & 0 \\\\ 0 & \\tau_1^{-1} \\end{bmatrix}\\) and compute Laplace approximation to the BF. Take \\(\\tau_0 \\rightarrow 0+\\).\n\nQ: How variable is the scaling factor? Can we get away with just using the univariate BF?"
  },
  {
    "objectID": "presentations/template/presentation.html#better-quadrature-rule",
    "href": "presentations/template/presentation.html#better-quadrature-rule",
    "title": "Generalizing SuSiE",
    "section": "Better quadrature rule",
    "text": "Better quadrature rule\nGauss-Hermite quadrature\n\\[\nI = \\int f(x) e^{-x^2} dx \\approx \\sum_{i=1}^n w_i f(x_i)\n\\]\n\\((x_i)_{i=1}^n\\) are the roots of the Hermite polynomial \\(H_n(x)\\), \\(w_i = \\frac{2^{n-1} n! \\sqrt{\\pi}}{n^2 H_{n-1}^2 (x_i)}\\)\n\n\\[\nI = \\int f(x) dx = \\int \\left[\\frac{f(x)}{q(x)} \\right] q(x) dx, \\;\\; q(x) = N(x | \\mu, \\sigma^2)\\;  \\text{s.t.}\\;  \\frac{f}{q} \\approx 1\n\\]\n\nAsymptotically correct\nOtherwise, integrating a function with little variation where the integrand has mass\nUpshot: very accurate integrals with e.g. \\(n = 8, 16\\).\n\n(note: change of variable + scaling factor to apply the \\(n\\) point Hermite quadrature rule)"
  },
  {
    "objectID": "presentations/gilad_lab_january24/gilad_lab_january24.html#overview",
    "href": "presentations/gilad_lab_january24/gilad_lab_january24.html#overview",
    "title": "Another Generalized SuSiE presentation",
    "section": "Overview",
    "text": "Overview\n\nPublication plan:\n\nGeneralized SuSiE via IBSS, emphasis on fine-mapping for non-Gaussian models\nGSEA with logistic SuSiE"
  },
  {
    "objectID": "presentations/gilad_lab_january24/gilad_lab_january24.html#fine-mapping-under-the-multivariate-gaussian-model",
    "href": "presentations/gilad_lab_january24/gilad_lab_january24.html#fine-mapping-under-the-multivariate-gaussian-model",
    "title": "Another Generalized SuSiE presentation",
    "section": "Fine-mapping under the multivariate Gaussian model",
    "text": "Fine-mapping under the multivariate Gaussian model\nMost fine-mapping methods assume summary statistics from marginal association studies are normally distributed, with covariance determined by LD 1\n\\[\\begin{align*}\n\\hat{ {\\bf z} }  \\sim N({\\bf z}, R)\n\\end{align*}\\]\nStatistical property of OLS– what if the marginal effects are coming from somewhere else?\nsee (1), (2), (3), (4)"
  },
  {
    "objectID": "presentations/gilad_lab_january24/gilad_lab_january24.html#what-do-we-want-to-accomplish",
    "href": "presentations/gilad_lab_january24/gilad_lab_january24.html#what-do-we-want-to-accomplish",
    "title": "Another Generalized SuSiE presentation",
    "section": "What do we want to accomplish?",
    "text": "What do we want to accomplish?\n\nEstablish when there is a problem with fine-mapping with summary stats from non-Gaussian models\n(Hopefully) find that these situations are not uncommon\n(Hopefully) demonstrate that GIBSS offers improvement in these situations\n(Fallback) advise people to fine map with summary statistics from linear models"
  },
  {
    "objectID": "presentations/gilad_lab_january24/gilad_lab_january24.html#three-horse-race",
    "href": "presentations/gilad_lab_january24/gilad_lab_january24.html#three-horse-race",
    "title": "Another Generalized SuSiE presentation",
    "section": "Three horse race",
    "text": "Three horse race\n\n\n\n\n\n\n\n\nMethod\nNotes\nSummary stats\n\n\n\n\nGeneralized IBSS\n“correct” model, hueristic algorithm\nNo\n\n\nLogistic + RSS\nad-hoc, actually used (5,6)\nYes\n\n\nLinear + RSS\nmis-specified model, correct algorithm\nYes"
  },
  {
    "objectID": "presentations/gilad_lab_january24/gilad_lab_january24.html#gibss-overview",
    "href": "presentations/gilad_lab_january24/gilad_lab_january24.html#gibss-overview",
    "title": "Another Generalized SuSiE presentation",
    "section": "GIBSS overview",
    "text": "GIBSS overview\n\nCompute univariate effect estimates using regression of choice, must return MLE and stderr\nCompute/approximate BFs and posterior means: Laplace, quadrature, etc.\nUse predictions as fixed offsets when updating next effect\nIterate until convergence (we don’t know if it converges)"
  },
  {
    "objectID": "presentations/gilad_lab_january24/gilad_lab_january24.html#key-questions",
    "href": "presentations/gilad_lab_january24/gilad_lab_january24.html#key-questions",
    "title": "Another Generalized SuSiE presentation",
    "section": "Key questions",
    "text": "Key questions\n\nGeneralized IBSS vs Logistic RSS\n\n\\(L=1\\) case reduces to IBSS-Laplace vs IBSS-ABF\nGIBSS-Laplace should be strictly better, but by how much, and when?\n\\(L > 1\\) all bets are off with Logistic RSS\n\nLinear SuSiE vs Logistic RSS\n\nHelpful to look at \\(L=1\\) case, Linear + Wakefield vs Logistic + Wakefield\nIn GWAS linear regression is often a good approximation to logistic regression\n\nLinear SuSiE vs Generalized IBSS\n\nWhen does linear SuSiE give reliable results?\nWhen does GIBSS provide and advantage?"
  },
  {
    "objectID": "presentations/gilad_lab_january24/gilad_lab_january24.html#potential-problems",
    "href": "presentations/gilad_lab_january24/gilad_lab_january24.html#potential-problems",
    "title": "Another Generalized SuSiE presentation",
    "section": "Potential problems",
    "text": "Potential problems\nLogistic + RSS\n\nCovariate of marginal \\(z\\)-scores do not correspond with LD\nUnder appreciated source of “LD Mismatch?”\nFollow-up: what is the correct covariance matrix?\nInherits problems from using ABF– not the most accurate\n\nLogistic GIBSS\n\nMarginal effect estimates are biased when there is a large genetic component"
  },
  {
    "objectID": "presentations/gilad_lab_january24/gilad_lab_january24.html#correlation-of-marginal-effects-in-logistic-regression",
    "href": "presentations/gilad_lab_january24/gilad_lab_january24.html#correlation-of-marginal-effects-in-logistic-regression",
    "title": "Another Generalized SuSiE presentation",
    "section": "Correlation of marginal effects in logistic regression:",
    "text": "Correlation of marginal effects in logistic regression:\n\nSimulate \\(\\begin{bmatrix}x_1 \\\\ x_2 \\end{bmatrix} \\sim N(\\begin{bmatrix}0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix}1 & \\rho \\\\ \\rho & 1 \\end{bmatrix})\\)\nSimulate \\(y\\) under a logistic model \\(y \\sim Bin(1, \\sigma(\\psi)), \\; \\psi := b_0 + b x_1\\)\n\\(cor(z_1, z_2) \\neq \\rho \\implies\\) LD matrix is the wrong covariance matrix"
  },
  {
    "objectID": "presentations/gilad_lab_january24/gilad_lab_january24.html#correlation-of-marginal-effects-cont",
    "href": "presentations/gilad_lab_january24/gilad_lab_january24.html#correlation-of-marginal-effects-cont",
    "title": "Another Generalized SuSiE presentation",
    "section": "Correlation of marginal effects (cont)",
    "text": "Correlation of marginal effects (cont)\nAn under-appreciated source of “LD mismatch”?\n\\(n = 500\\), \\(b_0 = -1\\), \\(b = 0, 1, 2, 3\\) 1\n\nmore simulations settings here"
  },
  {
    "objectID": "presentations/gilad_lab_january24/gilad_lab_january24.html#laplace-vs-wakefield-review",
    "href": "presentations/gilad_lab_january24/gilad_lab_january24.html#laplace-vs-wakefield-review",
    "title": "Another Generalized SuSiE presentation",
    "section": "Laplace vs Wakefield (Review)",
    "text": "Laplace vs Wakefield (Review)\n\nFigure 1: Wakefield’s ABF can be order of magnitude off when the \\(z\\)-score is large"
  },
  {
    "objectID": "presentations/gilad_lab_january24/gilad_lab_january24.html#problems-with-wakefield-review",
    "href": "presentations/gilad_lab_january24/gilad_lab_january24.html#problems-with-wakefield-review",
    "title": "Another Generalized SuSiE presentation",
    "section": "Problems with Wakefield (Review)",
    "text": "Problems with Wakefield (Review)\n!()[resources/abf_biased.png]\n!()[resources/abf_eq.png]"
  },
  {
    "objectID": "presentations/gilad_lab_january24/gilad_lab_january24.html#susie-rss-and-the-wakefield-bf",
    "href": "presentations/gilad_lab_january24/gilad_lab_january24.html#susie-rss-and-the-wakefield-bf",
    "title": "Another Generalized SuSiE presentation",
    "section": "SuSiE-RSS and the Wakefield BF",
    "text": "SuSiE-RSS and the Wakefield BF\n\nRecall that Wakefield’s ABF is not accurate when the \\(z\\)-score is large\nApplying SuSiE-RSS to summary statistics from some other model besides Gaussian linear model"
  },
  {
    "objectID": "presentations/gilad_lab_january24/gilad_lab_january24.html#how-large-do-the-z-scores-need-to-be",
    "href": "presentations/gilad_lab_january24/gilad_lab_january24.html#how-large-do-the-z-scores-need-to-be",
    "title": "Another Generalized SuSiE presentation",
    "section": "How large do the \\(z\\)-scores need to be?",
    "text": "How large do the \\(z\\)-scores need to be?"
  },
  {
    "objectID": "presentations/gilad_lab_january24/gilad_lab_january24.html#biased-effect-estimates-and-bfs",
    "href": "presentations/gilad_lab_january24/gilad_lab_january24.html#biased-effect-estimates-and-bfs",
    "title": "Another Generalized SuSiE presentation",
    "section": "Biased effect estimates (and BFs)",
    "text": "Biased effect estimates (and BFs)\nSimulation: one causal variant in the locus that explains \\(1\\%\\) of heritability of liability. \\(h^2 = 0.1, 0.2, 0.5, 0.9\\)\n\\[\\begin{align*}\ny \\sim Bin(1, \\sigma(\\psi)) \\\\\n\\psi = b_0 + b x + \\epsilon \\\\\n\\epsilon \\sim N(0, \\sigma^2)\n\\end{align*}\\]"
  },
  {
    "objectID": "presentations/gilad_lab_january24/gilad_lab_january24.html#biased-effect-estimates-and-bfs-1",
    "href": "presentations/gilad_lab_january24/gilad_lab_january24.html#biased-effect-estimates-and-bfs-1",
    "title": "Another Generalized SuSiE presentation",
    "section": "Biased effect estimates (and BFs)",
    "text": "Biased effect estimates (and BFs)"
  },
  {
    "objectID": "presentations/gilad_lab_january24/gilad_lab_january24.html#biased-effect-estimates-and-bfs-2",
    "href": "presentations/gilad_lab_january24/gilad_lab_january24.html#biased-effect-estimates-and-bfs-2",
    "title": "Another Generalized SuSiE presentation",
    "section": "Biased effect estimates (and BFs)",
    "text": "Biased effect estimates (and BFs)\n\n95% C.I. for different \\(h^2\\)"
  },
  {
    "objectID": "presentations/gilad_lab_january24/gilad_lab_january24.html#biased-effect-estimates-and-bfs-3",
    "href": "presentations/gilad_lab_january24/gilad_lab_january24.html#biased-effect-estimates-and-bfs-3",
    "title": "Another Generalized SuSiE presentation",
    "section": "Biased effect estimates (and BFs)",
    "text": "Biased effect estimates (and BFs)\n\nFor phenotypes with substantial \\(h^2\\) of liability, restricting our attention to a single locus will lead to biased effect estimates\nRemark: linear model doesn’t struggle with this issue because in practice we estimate the residual variance (or set conservatively)\nRemark:basically a random intercept model, but this seems a little different than the usual motivation for mixed model approaches."
  },
  {
    "objectID": "presentations/gilad_lab_january24/gilad_lab_january24.html#real-data-analsis",
    "href": "presentations/gilad_lab_january24/gilad_lab_january24.html#real-data-analsis",
    "title": "Another Generalized SuSiE presentation",
    "section": "Real data analsis",
    "text": "Real data analsis\n\nQ: if we reun SuSiE, GIBSS, Logistic + RSS on real case-control GWAS do we get qualitatively different results?\nDon’t know ground truth, hard to tell what is performing better\nReplication failure rate (RFR) among PIPs proposed in (7) may support claim that using GIBSS > Linear + RSS > Logistic + RSS\n\n\n\nOther ideas?\nDo you know of imbalanced case-control GWAS, survival GWAS, etc. GWAS on count based phenotypes, etc to test out?"
  },
  {
    "objectID": "presentations/gilad_lab_january24/gilad_lab_january24.html#simulation",
    "href": "presentations/gilad_lab_january24/gilad_lab_january24.html#simulation",
    "title": "Another Generalized SuSiE presentation",
    "section": "Simulation",
    "text": "Simulation\n\\[\n\\begin{align*}\ny_i &\\sim Bin\\left(1, \\sigma \\left(b_0 + \\sum_{j=1}^q b_j x_{ij} + \\delta\\right)\\right)\\\\\nb &\\sim N(0, \\sigma^2) \\\\\n\\delta &\\sim N(0, \\nu - q \\sigma^2)\\\\\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nValue\nDescription\n\n\n\n\n\\(X\\)\nStandardized genotypes\n\n\n\\(\\sigma^2\\)\nVariance of standardized effects, i.e. \\(b \\sim N(0, \\sigma^2)\\)\n\n\n\\(q\\)\nNumber of causal variants in locus\n\n\n\\(\\rho\\)\nFraction of variance of genetic component in-locus\n\n\n\\(k\\)\nFraction of cases (determines \\(b_0\\))\n\n\n\\(q\\sigma^2\\)\n(Expected) variance of genetic component in-locus\n\n\n\\(\\nu\\)\n\\(q \\sigma^2/\\rho\\), (expected) variance of genetic component\n\n\n\\(h^2\\)\n\\(\\nu / (\\nu + \\pi^2/3)\\), (expected) heritability of liability 1\n\n\n\n\n\n\nLiability threshold model, borrowed from (8)\n\n\nsee (8), and make sure this makes sense!"
  },
  {
    "objectID": "presentations/gilad_lab_january24/gilad_lab_january24.html#examples-where-susie-is-applied-to-non-gaussian-linear-summary-stats",
    "href": "presentations/gilad_lab_january24/gilad_lab_january24.html#examples-where-susie-is-applied-to-non-gaussian-linear-summary-stats",
    "title": "Another Generalized SuSiE presentation",
    "section": "Examples where SuSiE is applied to non-Gaussian linear summary stats",
    "text": "Examples where SuSiE is applied to non-Gaussian linear summary stats\n(5), Alzheimers meta analysis combining linear and logistic association studies (6) logistic-mixed model SAIGE + SuSiE"
  },
  {
    "objectID": "presentations/gilad_lab_january24/gilad_lab_january24.html#when-does-logistic-wakefield-perform-poorly",
    "href": "presentations/gilad_lab_january24/gilad_lab_january24.html#when-does-logistic-wakefield-perform-poorly",
    "title": "Another Generalized SuSiE presentation",
    "section": "When does Logistic + Wakefield perform poorly?",
    "text": "When does Logistic + Wakefield perform poorly?\n\n\n\naccidentally wiped the simulations, need to regenerate\napparently"
  },
  {
    "objectID": "presentations/gilad_lab_january24/gilad_lab_january24.html#dealing-with-intercept-covariates",
    "href": "presentations/gilad_lab_january24/gilad_lab_january24.html#dealing-with-intercept-covariates",
    "title": "Another Generalized SuSiE presentation",
    "section": "Dealing with intercept + covariates",
    "text": "Dealing with intercept + covariates\nA few options:\n\nEstimate in outer loop, treat as a fixed offset while estimating SERs\nRe-estimate covariate effects for each variable\nFound that reestimating intercept was helpful"
  },
  {
    "objectID": "presentations/gilad_lab_january24/gilad_lab_january24.html#univariate-bf",
    "href": "presentations/gilad_lab_january24/gilad_lab_january24.html#univariate-bf",
    "title": "Another Generalized SuSiE presentation",
    "section": "Univariate BF",
    "text": "Univariate BF\n\nLimiting BF"
  },
  {
    "objectID": "presentations/gilad_lab_january24/gilad_lab_january24.html#limiting-bf",
    "href": "presentations/gilad_lab_january24/gilad_lab_january24.html#limiting-bf",
    "title": "Another Generalized SuSiE presentation",
    "section": "Limiting BF",
    "text": "Limiting BF\nIdea put a normal prior on all covariates \\(\\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix} \\sim N(\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} I_{p-1} \\tau_0^{-1} & 0 \\\\ 0 & \\tau_1^{-1} \\end{bmatrix}\\) and compute Laplace approximation to the BF. Take \\(\\tau_0 \\rightarrow 0+\\).\n\nQ: How variable is the scaling factor? Can we get away with just using the univariate BF?"
  },
  {
    "objectID": "presentations/gilad_lab_january24/gilad_lab_january24.html#better-quadrature-rule",
    "href": "presentations/gilad_lab_january24/gilad_lab_january24.html#better-quadrature-rule",
    "title": "Another Generalized SuSiE presentation",
    "section": "Better quadrature rule",
    "text": "Better quadrature rule\nGauss-Hermite quadrature\n\\[\nI = \\int f(x) e^{-x^2} dx \\approx \\sum_{i=1}^n w_i f(x_i)\n\\]\n\\((x_i)_{i=1}^n\\) are the roots of the Hermite polynomial \\(H_n(x)\\), \\(w_i = \\frac{2^{n-1} n! \\sqrt{\\pi}}{n^2 H_{n-1}^2 (x_i)}\\)\n\n\\[\nI = \\int f(x) dx = \\int \\left[\\frac{f(x)}{q(x)} \\right] q(x) dx, \\;\\; q(x) = N(x | \\mu, \\sigma^2)\\;  \\text{s.t.}\\;  \\frac{f}{q} \\approx 1\n\\]\n\nAsymptotically correct\nOtherwise, integrating a function with little variation where the integrand has mass\nUpshot: very accurate integrals with e.g. \\(n = 8, 16\\).\n\n(note: change of variable + scaling factor to apply the \\(n\\) point Hermite quadrature rule)"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#aim-0",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#aim-0",
    "title": "Committee Meeting",
    "section": "Aim 0",
    "text": "Aim 0\nDevelop general approaches for applying SuSiE to a wide class of non-Gaussian models.\n\n\nNeed good version of logistic SuSiE (and multinomial SuSiE) for GSEA and covariate moderated FDR application (Aim 1 and 2)\nWe identified serious limitations in our old approach\nTwo new approaches, both extend beyond the logistic case"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#sum-of-single-effects-regression-susie",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#sum-of-single-effects-regression-susie",
    "title": "Committee Meeting",
    "section": "Sum of single effects regression (SuSiE)",
    "text": "Sum of single effects regression (SuSiE)\n\\({\\bf y} \\in \\mathbb R^n\\), \\(X \\in \\mathbb R^{n \\times p}\\)\nSum of single effects: there are at most \\(L\\) non-zero effects\n\\[\\begin{align}\n{\\bf y} &\\sim N(X\\beta, {\\bf I}_n \\sigma^2) \\\\\n\\beta &= \\sum_{l=1}^L b_l \\gamma_l \\\\\nb_l &\\sim N(0, \\sigma^2_0) \\\\\n\\gamma_l &\\sim \\text{Multinomial}(1, \\pi)\n\\end{align}\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#what-makes-susie-work",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#what-makes-susie-work",
    "title": "Committee Meeting",
    "section": "What makes SuSiE work?",
    "text": "What makes SuSiE work?\nWe will quickly review variational inference for SuSiE.\nTwo key ingredients:\n\nVariational approximation \\(q(\\beta) = \\prod_l q(\\beta_l)\\)\nGaussian likelihood reduces coordinate updates to solving a SER"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#single-effect-regression-ser",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#single-effect-regression-ser",
    "title": "Committee Meeting",
    "section": "Single Effect Regression (SER)",
    "text": "Single Effect Regression (SER)\n\n\nModel\nSingle effect regression assumes there is exactly one non-zero effect.\n\\({\\bf y} \\in \\mathbb R^n\\), \\(X \\in \\mathbb R^{n \\times p}\\) \\[\n\\begin{aligned}\n{\\bf y} &\\sim N(X\\beta, {\\bf I}_p \\sigma^2_0) \\\\\n\\beta &= b \\gamma \\\\\nb &\\sim N(0, \\sigma^2_0)\\\\\n\\gamma &\\sim \\text{Multinomial}(1, \\pi)\n\\end{aligned}\n\\]\n\nInference is easy\n\nCompute the posterior for \\(j = 1,\\dots , p\\) separate univariate regression \\[p(b | X, {\\bf y}, \\gamma_j = 1)\\]\nCompute posterior inclusion probabilities\n\n\\[\np(\\gamma_j = 1 | {\\bf y}, X) \\propto BF_j \\pi_j\n\\]\n\\[\nBF_j = \\frac{p({\\bf y} | X, \\gamma_j=1 )}{p({\\bf y}  | \\beta=0)}\n\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#variational-inference",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#variational-inference",
    "title": "Committee Meeting",
    "section": "Variational inference",
    "text": "Variational inference\nInference as an optimization problem:\n\\[\\begin{align}\np(\\beta | {\\bf y}, X) = \\arg \\max_q F(q),\n\\quad F(q) := \\mathbb E_q \\left[ \\log p ({\\bf y} | X, \\beta) \\right] - KL[q || p]\n\\end{align}\\]\n\n\\[\\begin{align}\nF(q)\n&= \\mathbb E_q \\left[ \\log p ({\\bf y} | X, \\beta) \\right] - KL[q || p] \\\\\n&= \\mathbb E_q \\left[ \\log p ({\\bf y},  \\beta | X) \\right] + H(q) \\\\\n&= \\left(\\mathbb E_q \\left[ \\log p ({\\bf y},  \\beta | X) - Z \\right] + H(q) \\right) + Z \\\\\n&= - KL[q || p_{post}] + Z\n\\end{align}\\]\n\n\nWe’ll write the variational objective for SER \\(F_{SER}(q; {\\bf y}, X, \\sigma_0)\\)"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#susie-model",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#susie-model",
    "title": "Committee Meeting",
    "section": "SuSiE: model",
    "text": "SuSiE: model\n\\[\n\\begin{aligned}\ny &\\sim N(X\\beta, \\sigma^2_0) \\\\\n\\beta &= \\sum_l b_l \\gamma_l \\\\\nb_l &\\sim N(0, \\sigma^2_0)\\\\\n\\gamma_l &\\sim \\text{Multinomial}(1, \\pi)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#susie-variational-approximation",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#susie-variational-approximation",
    "title": "Committee Meeting",
    "section": "SuSiE: variational approximation",
    "text": "SuSiE: variational approximation\nRestrict \\(q\\) to some family \\(\\mathcal Q\\)\n\\[\\begin{align}\nq^*(\\beta) = \\arg \\max_{q \\in \\mathcal Q} F(q)\n\\end{align}\\]\nSuSiE uses \\(\\mathcal Q = \\{q : q(\\beta) = \\prod_l q_l(\\beta_l)\\}\\)\n\n\\[\nF_{SuSiE}(q; X, {\\bf y}) = \\mathbb E_{q} \\left[\n      -\\frac{1}{2\\sigma^2}||{\\bf y} - {\\bf X} \\beta_{-l} - X\\beta_l||^2_2)\n  \\right] + KL[q || p_{SuSiE}]\n\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#susie-coordinate-ascent-ibss",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#susie-coordinate-ascent-ibss",
    "title": "Committee Meeting",
    "section": "SuSiE: coordinate ascent (IBSS)",
    "text": "SuSiE: coordinate ascent (IBSS)\nDefine the residual \\({\\bf r}_l = {\\bf y} - X\\beta_{-l}\\)\n\n\\[\n\\begin{aligned}\nF_{SuSiE}(q_l; q_{-l}, X, {\\bf y})\n&= \\mathbb E_{q_l} \\left[\n  \\mathbb E_{q_{-l}} \\left[\n      -\\frac{1}{2\\sigma^2}||{\\bf y} - {\\bf X} \\beta_{-l} - X\\beta_l||^2_2)\n  \\right]\n\\right] - KL[q || p] \\\\\n&= \\mathbb E_{q_l} \\left[\n  \\mathbb E_{q_{-l}} \\left[\n      -\\frac{1}{2\\sigma^2}|| {\\bf r}_l - X\\beta_l||^2_2)\n  \\right]\n\\right] - KL[q_l || p_l]  + C_1 \\\\\n&= \\mathbb E_{q_l} \\left[ \\color{blue}{\n    -\\frac{1}{2\\sigma^2} \\left(|| \\mathbb E_{q_{-l}} {\\bf r}_l- X\\beta_l||^2_2 + \\mathbb V ||r_l||^2_2 \\right)}\n\\right] - KL[q_l || p_l]  + C_1\\\\\n&= \\mathbb E_{q_l} \\left[\n    -\\frac{1}{2\\sigma^2} || \\mathbb E_{q_{-l}} {\\bf r}_l- X\\beta_l||^2_2]\n\\right] - KL[q_l || p_l]  + C_2  \\\\\n&= F_{SER}(q_l; X, \\mathbb E_{q{-l}}{\\bf r}_l) + C_2\n\\end{aligned}\n\\]\n\n\nKey: coordinate updates in \\(\\mathcal Q\\) reduce to solving an SER"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#generalizing-susie",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#generalizing-susie",
    "title": "Committee Meeting",
    "section": "Generalizing SuSiE",
    "text": "Generalizing SuSiE\nSimplification depends on properties of the Gaussian log likelihood\n\\[\n\\begin{aligned}\n\\log p(y | {\\bf x}, \\beta) = y ({\\bf x}^T \\beta) - \\log(1 + \\exp({\\bf x}^T \\beta))\n\\end{aligned}\n\\]\n\nNo simplification when taking expectations over \\(q_{-l}\\)\n\\[\n\\begin{aligned}\n\\mathbb E_{q_{-l}} \\left[ \\log p(y | {\\bf x}, \\beta) \\right]\n= y ({\\bf x}^T \\beta_l + {\\bf x}^T  \\bar \\beta_{-l}) - \\color{red}{\\mathbb E_{q_{-l}}\\left[\\log(1 + \\exp({\\bf x}^T \\beta)) \\right]}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#jaakkola-jordan-bound",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#jaakkola-jordan-bound",
    "title": "Committee Meeting",
    "section": "Jaakkola-Jordan bound",
    "text": "Jaakkola-Jordan bound\nIdea: construct local approximation to the log-likelihood. Tune the approximation to be tight near the posterior mode.\n\nFor all \\(\\xi \\in \\mathbb R\\), \\(\\psi = {\\bf x}^T \\beta\\)\n\\[\n\\log p(y | \\psi) \\geq \\frac{1}{2} \\log \\sigma(\\xi) +  \\frac{1}{2} \\left((2y -1) \\psi - \\xi\\right) - \\frac{1}{4\\xi}\\tanh(\\frac{\\xi}{2}) (\\psi^2 - \\xi^2)\n\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#jj-bound-is-bad-for-variable-selection",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#jj-bound-is-bad-for-variable-selection",
    "title": "Committee Meeting",
    "section": "JJ Bound is bad for variable selection",
    "text": "JJ Bound is bad for variable selection\n\n\nNaive application of JJ bound to SER uses very loose ELBO for most variables\n\n\n\n\n\n\n\n\n\nWinner take all: Coverage of 90% credible sets across 1k SER simulations\n\n\nmethod\ncoverage\n\n\n\n\nuvb_ser\n0.945\n\n\nvb_ser\n0.845"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#simulations-vb-logistic-susie-l5",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#simulations-vb-logistic-susie-l5",
    "title": "Committee Meeting",
    "section": "Simulations: VB logistic SuSiE \\(L=5\\)",
    "text": "Simulations: VB logistic SuSiE \\(L=5\\)\nVarying effect sizes, correlation structure, and num. of non-zero effects"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#overview",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#overview",
    "title": "Committee Meeting",
    "section": "Overview",
    "text": "Overview\n\n\nHeuristic approach for applying SuSiE to non-Gaussian models\nIdea: apply IBSS, using expected predictions as fixed offsets\n\n\n\n\\[\\begin{align}\nF_{SuSiE}(q)\n&= \\mathbb E_q[\\log p({\\bf y} | X, \\beta_l, \\beta_{-l})] - \\sum_l KL[q_l || p_l] \\\\\n&= \\mathbb E_{q_l}[ \\mathbb E_{q_{-l}}[\\log p({\\bf y} | X, \\beta_l, \\beta_{-l})]] - \\sum_l KL[q_l || p_l] + C_1 \\\\\n&\\color{purple}{\\approx \\mathbb E_{q_l}[\\log p({\\bf y} | X, \\beta_l, \\bar \\beta_{-l})] - KL[q_l || p_l] + C_2} \\\\\n&= F_{SER}(q_l; {\\bf y}, X, \\bar \\beta_{-l}) + C_2\n\\end{align}\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#why-should-this-work",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#why-should-this-work",
    "title": "Committee Meeting",
    "section": "Why should this work?",
    "text": "Why should this work?\n\\[\\begin{align}\nF_{SuSiE}(q)\n&= \\mathbb E_q[\\log p({\\bf y} | X, \\beta_l, \\beta_{-l})] - \\sum_l KL[q_l || p_l] \\\\\n&\\color{purple}{\\approx \\mathbb E_{q_l}[\\log p({\\bf y} | X, \\beta_l, \\bar \\beta_{-l})] - KL[q_l || p_l] + C_2} \\\\\n\\end{align}\\]\n\n\nIf \\(\\log p( {\\bf y} | X, \\beta)\\) is quadratic in \\(\\beta\\) (Gaussian) there \\(\\color{purple}\\approx\\) is \\(=\\)\nInformally, approximation is good if \\(\\log p( {\\bf y} | X, \\beta)\\) is well approximated by a quadratic function in \\(\\beta\\).\nReally we care about \\(\\psi = X\\beta\\)"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#algorithm-single-effect-regression",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#algorithm-single-effect-regression",
    "title": "Committee Meeting",
    "section": "Algorithm: Single effect regression",
    "text": "Algorithm: Single effect regression\nRequire a function \\(G\\) which computes the BF and posterior mean of a Bayesian univariate regression\n\\[\\begin{align}\n{\\bf y} \\sim 1 + {\\bf x} + {\\bf o} \\\\\nb \\sim N(0, \\sigma^2)\n\\end{align}\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#algorithm-gibss",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#algorithm-gibss",
    "title": "Committee Meeting",
    "section": "Algorithm: GIBSS",
    "text": "Algorithm: GIBSS"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#simulations-logistic-gibss-l5",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#simulations-logistic-gibss-l5",
    "title": "Committee Meeting",
    "section": "Simulations: logistic-GIBSS \\(L=5\\)",
    "text": "Simulations: logistic-GIBSS \\(L=5\\)\nMuch better performance compared to direct VB approach"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#advantage-and-limitations",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#advantage-and-limitations",
    "title": "Committee Meeting",
    "section": "Advantage and limitations",
    "text": "Advantage and limitations\n\n\nAdvantages\n\nSeems to work well!\nHighly modular: just need to implement the univariate regression\n\n\n\n\nDisadvantages\n\nHeuristic– not optimizing a clear objective function\nDoes not properly account for uncertainty in \\(\\beta_{-l}\\), only uses posterior mean\n\n\n\n\nOpportunities\n\nCan we analyze the approximation error?\nCan we make guarantees on GIBSS performance asymptotically?"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#status",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#status",
    "title": "Committee Meeting",
    "section": "Status",
    "text": "Status\n\nImplemented in logisticsusie package github rep\nYunqi was able to use GIBSS code to help with survival SuSiE project\nPossibly implement GIBSS for GLMs in susieR package"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#overview-1",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#overview-1",
    "title": "Committee Meeting",
    "section": "Overview",
    "text": "Overview\n\nGIBSS requires computing the BF and posterior mean for each variable\nStandard statistical software for computing MLE\nFor large sample sizes, we can leverage asymptotic normality of the MLE to approximate posterior mean and BF."
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#posterior-mean",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#posterior-mean",
    "title": "Committee Meeting",
    "section": "Posterior mean",
    "text": "Posterior mean\n\\[\n\\begin{aligned}\n\\hat \\beta | \\beta, s^2 &\\sim N(\\beta, s^2) \\\\\n\\beta &\\sim N(0, \\sigma^2)\n\\end{aligned}\n\\]\n\n\\[\n\\beta | \\hat \\beta, s^2 \\sim N \\left(\n\\frac{\\sigma^2}{s^2 + \\sigma^2} \\hat\\beta, \\left(\\frac{1}{s^2} + \\frac{1}{\\sigma^2}\\right)^{-1}\n\\right)\n\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#approximating-the-the-bf",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#approximating-the-the-bf",
    "title": "Committee Meeting",
    "section": "Approximating the the BF",
    "text": "Approximating the the BF\n\\[BF = \\int \\color{red}{\\frac{ p(\\mathcal {\\bf y} | {\\bf x}, \\beta)}{p({\\bf y} | \\beta = 0)}} N(\\beta | 0, \\sigma^2) d\\beta\\]\nApproximate the likelihood ratio in a way that’s easy to integrate\n\\[\nLR(\\beta_1, \\beta_2) = \\frac{p(\\mathcal {\\bf y} | {\\bf x}, \\beta)}{p({\\bf y} | \\beta=0)}\n\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#wakefields-asymptotic-bf",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#wakefields-asymptotic-bf",
    "title": "Committee Meeting",
    "section": "Wakefields asymptotic BF",
    "text": "Wakefields asymptotic BF\nAsymptotically, \\(\\hat \\beta | \\beta, s^2 \\sim N(\\beta, s^2)\\). Then\n\\[\nLR(\\beta, 0)\n= \\frac{p(y | \\hat\\beta, \\beta) p(\\hat\\beta | \\beta)}{p(y | \\hat\\beta, \\beta = 0) p(\\hat\\beta | \\beta = 0)}\n\\approx \\color{red}{\\frac{p(y | \\hat\\beta)}{p(y | \\hat\\beta)}} \\frac{p(\\hat\\beta | \\beta)}{ p(\\hat\\beta | \\beta = 0)} \\approx \\frac{N(\\hat\\beta| \\beta, s^2)}{N(\\hat\\beta| 0, s^2)} = \\widehat{LR}_{ABF}(\\beta, 0).\n\\]\n\nIntegrating over the prior gives Wakefield’s asymptotic Bayes Factor (ABF)\n\\[\nABF = \\int \\widehat{LR}_{ABF}(\\beta, 0) N(\\beta | 0, \\sigma^2_0) d\\beta = \\frac{N(\\hat\\beta | 0, s^2 + \\sigma^2_0)}{N(\\hat\\beta | 0, s^2)}\n\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#a-problem-with-abf",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#a-problem-with-abf",
    "title": "Committee Meeting",
    "section": "A problem with ABF",
    "text": "A problem with ABF\n\nThe asymptotic approximation may not be a good in the tails\nAn issue for \\(\\hat\\beta/s >> 0\\)"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#adjusting-the-abf",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#adjusting-the-abf",
    "title": "Committee Meeting",
    "section": "Adjusting the ABF",
    "text": "Adjusting the ABF\nIdea: use the asymptotic approximation where it is good\n\\[\n\\begin{aligned}\nLR(\\beta, 0)\n&= LR(\\beta, \\hat\\beta) LR(\\hat\\beta, 0) \\\\\n&\\approx \\widehat{LR}_{ABF}(\\beta, \\hat\\beta)LR(\\hat\\beta, 0) \\\\\n&= \\widehat{LR}_{Lap}(\\hat\\beta, 0)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#corrected-abflaplace-approximation",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#corrected-abflaplace-approximation",
    "title": "Committee Meeting",
    "section": "Corrected ABF/Laplace approximation",
    "text": "Corrected ABF/Laplace approximation\n\nRequires knowledge of the LR of the MLE against the null.\nCan dramatically improve approximation of the BF"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#comparison-of-logbf",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#comparison-of-logbf",
    "title": "Committee Meeting",
    "section": "Comparison of logBF",
    "text": "Comparison of logBF"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#comparison-of-logbf-1",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#comparison-of-logbf-1",
    "title": "Committee Meeting",
    "section": "Comparison of logBF",
    "text": "Comparison of logBF"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#abf-correction-reorders-pips",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#abf-correction-reorders-pips",
    "title": "Committee Meeting",
    "section": "ABF Correction reorders PIPs",
    "text": "ABF Correction reorders PIPs"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#laplace-approximation",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#laplace-approximation",
    "title": "Committee Meeting",
    "section": "Laplace approximation",
    "text": "Laplace approximation\nFor some non-negative function \\(f: \\mathbb R \\rightarrow \\mathbb R^+\\), want to compute the integral\n\\[I = \\int f(x) dx,\\]\n\nDefine \\(h(x) = \\log f(x)\\), expand around the maximizer of \\(h\\), \\(x^*\\)\n\\[\\hat h (x) = h(x^*) + \\frac{1}{2} h^{''}(x^*)(x-x^*)^2\\]\n\n\nApproximate with the Gaussian integral\n\\[ \\hat I = \\int \\exp{\\hat h(x))} dx = \\exp{\\hat h(x^*)} \\left(-\\frac{2\\pi}{h^{''}(x^*)}\\right)^{1/2}\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#takeaways",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#takeaways",
    "title": "Committee Meeting",
    "section": "Takeaways",
    "text": "Takeaways\n\nLaplace approximation of the BF better than ABF for variable selection\nAddition information available from standard statistical software for GLMs\nPoor performance of ABF raises concerns about using SuSiE-RSS for summary statistics from non-Gaussian models\nGIBSS + asymptotic approximation looks like a good recipe for GLMs"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#notation",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#notation",
    "title": "Committee Meeting",
    "section": "Notation",
    "text": "Notation\n\\(p\\) an \\(M\\) degree polynomial\n\\({\\bf a} = (a_0, \\dots, a_M)\\) unique coefficients representing \\(p\\) in the monomial basis \\((1, x, \\dots, x^M)\\)\n\\[\\phi_{\\bf a}(x) = \\sum_{k=0}^M a_k x^k = p(x)\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#polynomial-approximation-inference",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#polynomial-approximation-inference",
    "title": "Committee Meeting",
    "section": "Polynomial approximation: inference",
    "text": "Polynomial approximation: inference\n\n\nApproximate the log-likelihood of each observation as a polynomial in linear prediction, \\(\\psi\\). \\({\\bf a}_i\\) s.t. \\(\\log p(y_i | \\psi) \\approx \\phi_{{\\bf a}_i}(\\psi)\\)\nApproximate the log prior density. \\(\\rho\\) s.t. \\(\\log p(\\beta) \\approx \\phi_{\\rho}\\)\nTransform \\({\\bf a}_i\\) so they are a function of \\(\\beta\\). \\({\\bf d}_i\\)\n\\(\\log p(y, \\beta) \\approx \\phi_{\\bf f}(\\beta), \\quad {\\bf f} := \\sum {\\bf d_i} + \\rho\\)"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#polynomial-exponential-family",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#polynomial-exponential-family",
    "title": "Committee Meeting",
    "section": "Polynomial exponential family",
    "text": "Polynomial exponential family\nWith \\({\\bf a} \\in \\mathbb R^M\\), and \\(T(x) = (x, \\dots, x^M)\\)\n\\[\nf(x; {\\bf a}) \\propto \\exp\\{\\langle {\\bf a}, T(x) \\rangle\\}\n\\]\n\nIntercept = normalizing constant. \\(a_0 = - \\log \\int \\exp\\{\\langle {\\bf a}, T(x) \\rangle\\} dx\\)\nHard integral, approximate or handle numerically"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#operations-polynomial-rescaling",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#operations-polynomial-rescaling",
    "title": "Committee Meeting",
    "section": "Operations: polynomial rescaling",
    "text": "Operations: polynomial rescaling\n\\[\\phi_{\\bf a}(cx) = \\phi_{{\\bf d}({\\bf a}, c)}(x)\\]\n\\[\n\\begin{aligned}\n\\sum_m a_m(c x)^m\n&= \\sum_m (a_m c^m) x^m \\\\\n&= \\sum_m {\\bf d}({\\bf a}, c)_m x^m, \\quad {\\bf d}({\\bf a}, c)_m := a_mc^m\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#polynomial-approximate-univariate-regression",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#polynomial-approximate-univariate-regression",
    "title": "Committee Meeting",
    "section": "Polynomial approximate univariate regression",
    "text": "Polynomial approximate univariate regression\n\nFor each \\(i\\), we have \\({\\bf a}_i\\) such that \\(\\log p(y_i | \\psi) \\approx \\phi_{{\\bf a}_i}(\\psi)\\)\nRepresent/approximate the log-prior density with a polynomial with coefficients \\(\\rho\\), i.e. \\(\\log p(\\beta) \\approx \\phi_\\rho(\\beta)\\)\n\n\n\\[\\begin{align}\n\\log p({\\bf y}, \\beta | {\\bf x}, \\beta)\n&\\approx \\sum_i \\phi_{{\\bf a}_i}(x_i \\beta) + \\phi_{\\rho}(\\beta)\\\\\n&= \\sum_i \\phi_{{\\bf d}({\\bf a}_i, x_i)}(\\beta) + \\phi_{\\rho}(\\beta) \\\\\n&= \\phi_{{\\bf f}(A, {\\bf x}, \\rho)}(\\beta), \\quad {\\bf f}(A, {\\bf x}, \\rho) = \\sum_i {\\bf d}({\\bf a}_i, x_i) + \\rho\n\\end{align}\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#example-univariate-regression",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#example-univariate-regression",
    "title": "Committee Meeting",
    "section": "Example: univariate regression",
    "text": "Example: univariate regression"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#increasing-degree-increases-accuracy",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#increasing-degree-increases-accuracy",
    "title": "Committee Meeting",
    "section": "Increasing degree increases accuracy",
    "text": "Increasing degree increases accuracy"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#polynomial-ser",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#polynomial-ser",
    "title": "Committee Meeting",
    "section": "Polynomial SER",
    "text": "Polynomial SER\n\\(A = \\{{\\bf a}_i \\}_{i=1}^n\\), base coefficients\n\\[\n\\hat F_{SER}(q; A, X, \\sigma^2_0) = \\mathbb E_q\\left[\n  \\sum \\phi_{{\\bf a}_i}(\\psi_i)]\\right] - KL[q || p] \\approx F_{SER}(q; {\\bf y}, X, \\sigma^2_0).\n\\]\n\nApproximate the SER by exactly optimizing an approximation of the ELBO\nEach univariate regression is easy to compute from the base coefficient \\(A\\)"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#operations-polynomial-shift",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#operations-polynomial-shift",
    "title": "Committee Meeting",
    "section": "Operations: polynomial shift",
    "text": "Operations: polynomial shift\nFor SuSiE: \\(\\psi = \\psi_l + \\psi_{-l}\\).\n\n\n\\[\\phi_{\\bf a}(x + y) = \\phi_{{\\bf b}({\\bf a}, y)}(x)\\]\n\n\nExpand and collect terms\n\\[\\begin{align}\n\\phi_{\\bf a}(x + y)\n&= \\sum_{m=0}^M a_m(x + y)^m = \\sum_{m=0}^M a_m \\sum_{k=0}^m {m \\choose k} x^k y^{m-k} \\\\\n&= \\sum_{k=0}^M \\left(\\sum_{m=k}^M a_m {m \\choose k}y^{m-k} \\right) x^k \\\\\n&= \\sum_{k=0}^M {\\bf b}({\\bf a}, y)_k x^k, \\quad {\\bf b}({\\bf a}, y)_k := \\left(\\sum_{m=k}^M a_m {m \\choose k}y^{m-k} \\right)\n\\end{align}\\]\n\n\nCan be written as a matrix multiplication \\[\n{\\bf b}({\\bf a}, y) = M(y) {\\bf a}, \\quad M_{ij} = {j \\choose i} y^{j -i}\\;  \\forall j \\geq i, 0 \\text{ otherwise}\n\\]\n\n\nFor random \\(y \\sim p\\), expected polynomial shift\n\\[\n{\\bf c}({\\bf a}, p)  := \\mathbb E_p[{\\bf b}({\\bf a}, y)]  = \\mathcal M(p){\\bf a}, \\quad \\mathcal M(p) := \\mathbb E_p[M(y)]\n\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#polynomial-shift-for-susie",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#polynomial-shift-for-susie",
    "title": "Committee Meeting",
    "section": "Polynomial shift for SuSiE",
    "text": "Polynomial shift for SuSiE\nEach observation must be transformed into a function of \\(\\psi_{l}\\)\nStart with \\({\\bf a}^* = \\mathcal M(q){\\bf a}\\)\n\n“Undo” polynomial shift \\({\\bf a}^{(l)} = M(q_l)^{-1} {\\bf a}^*\\)\nUpdate \\(q_{l, new} \\leftarrow \\arg\\max_{q_l}\\hat F_{SER}(q_l; A^{(l)}, X, \\sigma^2_0)\\)\n\\({\\bf a}^*_{new} \\leftarrow \\mathcal M(q_{l, new}) {\\bf a}^{(l)}\\)\n\n\nThe SER coefficients \\({\\bf a}^{(l)}\\) analogous to expected residuals in IBSS\n\\(\\mathcal M(q_l)\\) requires \\(\\mathbb E_{q_l}\\left[\\psi^k\\right], \\; k = 1, \\dots M\\)"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#coordinate-ascent-in-an-variational-objective",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#coordinate-ascent-in-an-variational-objective",
    "title": "Committee Meeting",
    "section": "Coordinate ascent in an variational objective",
    "text": "Coordinate ascent in an variational objective\n\\[\\begin{align}\n\\hat F_{SuSiE}(q_l; q_{-l}, A, X, \\sigma^2_0)\n&= \\mathbb E_{q_l} \\mathbb E_{q_{-l}} \\sum_i \\phi_{{\\bf a}_i}(\\psi_l + \\psi_{-l}) + KL[q|| p] \\\\\n&= \\mathbb E_{q_l} \\mathbb E_{q_{-l}} \\sum_i \\phi_{{\\bf c}({\\bf a}_i, \\psi_{-l})}(\\psi_l) + KL[q_l|| p_l] + C \\\\\n&= \\mathbb E_{q_l} \\sum_i \\phi_{{\\bf d}({\\bf a}_i,q_{-l})}(\\psi_l) + KL[q_l|| p_l] + C \\\\\n&= \\hat F_{SER} (q_l; D, X, \\sigma^2_0) + C, \\quad D = \\{{\\bf d}({\\bf a}_i,q_{-l})\\}_{i=1}^n\n\\end{align}\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#polynomial-interpolation",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#polynomial-interpolation",
    "title": "Committee Meeting",
    "section": "Polynomial interpolation",
    "text": "Polynomial interpolation\n\n\n\\(f : \\mathbb R \\rightarrow \\mathbb R\\)\n\\(n+1\\) distinct points \\(x_0 < \\dots < x_{n}\\)\nThere exists a unique \\(p \\in P_n\\) such that \\(p\\) interpolates \\(f\\) at \\(x_0, \\dots, x_n\\).\n\\[\\begin{align}\np(x) = \\sum_{i=0}^n f(x_i) L_{n, i}(x), \\quad L_{n, i}(x) = \\prod_{k \\neq i} \\frac{x - x_k}{x_i - x_k}\n\\end{align}\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#chebyshev-interpolation",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#chebyshev-interpolation",
    "title": "Committee Meeting",
    "section": "Chebyshev interpolation",
    "text": "Chebyshev interpolation\n\n\n\n\n\n\nInterpolation at the zeros of Chebyshev polynomials \\(x_k = \\cos \\left(\\frac{2k-1}{2M}\\pi \\right), k = 1, \\dots, M\\)\nFast interpolation via re-scaled DFT of \\(f(x_1), \\dots, f(x_M)\\)\nApproximately minimizes \\(||f - \\hat f||_{\\infty}\\)"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#chebyshev-interpolation-glm-likelihoods",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#chebyshev-interpolation-glm-likelihoods",
    "title": "Committee Meeting",
    "section": "Chebyshev interpolation: GLM likelihoods",
    "text": "Chebyshev interpolation: GLM likelihoods"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#simulations",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#simulations",
    "title": "Committee Meeting",
    "section": "Simulations",
    "text": "Simulations"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#caveats-designing-good-polynomial-approximations",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#caveats-designing-good-polynomial-approximations",
    "title": "Committee Meeting",
    "section": "Caveats: designing good polynomial approximations",
    "text": "Caveats: designing good polynomial approximations\n\nRequire the leading degree of the polynomial approximation to be negative so that \\(\\int \\exp \\phi_{\\bf a} < \\infty\\)\nApproximation interval should contain the prediction for each observation with high probability for the approximation to be accurate \\(\\mathbb P( \\beta^T {\\bf x}_i \\in [L, R] | {\\bf y}, X) \\approx 1\\)\nQuality of the approximation (generally) decreases as approximation interval becomes wider"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#opportunities-1",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#opportunities-1",
    "title": "Committee Meeting",
    "section": "Opportunities",
    "text": "Opportunities\n\nIs there a faster way to perform “expected polynomial shift” operation?\nOther computationally convenient ways to approximation \\(\\log p(y | \\psi)\\)?"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#last-domino-constructing-base-approximation",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#last-domino-constructing-base-approximation",
    "title": "Gilad Lab WIP",
    "section": "Last domino: constructing base approximation",
    "text": "Last domino: constructing base approximation\n\n\nRecall: if we find a good polynomial approximation to the log-likelihood for each observation we can cheaply approximate inference in the non-conjugate model\nPrevious strategy: Chebyshev interpolating polynomials can approximately minimize \\(||f - \\hat f||_{\\infty}\\) on a target interval.\nProblem: interpolating polynomials are not guarunteed to have negative leading coefficient (a requirement for this approach)\nWhat is a general strategy for computing legal polynomial approximations?"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#susie-model",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#susie-model",
    "title": "Gilad Lab WIP",
    "section": "SuSiE: model",
    "text": "SuSiE: model\n\\[\n\\begin{aligned}\ny &\\sim N(X\\beta, \\sigma^2_0) \\\\\n\\beta &= \\sum_l b_l \\gamma_l \\\\\nb_l &\\sim N(0, \\sigma^2_0)\\\\\n\\gamma_l &\\sim \\text{Multinomial}(1, \\pi)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#variational-inference",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#variational-inference",
    "title": "Gilad Lab WIP",
    "section": "Variational inference",
    "text": "Variational inference\nInference as an optimization problem:\n\\[\\begin{align}\np(\\beta | {\\bf y}, X) = \\arg \\max_q F(q),\n\\quad F(q) := \\mathbb E_q \\left[ \\log p ({\\bf y} | X, \\beta) \\right] - KL[q || p]\n\\end{align}\\]\n\n\\[\\begin{align}\nF(q)\n&= \\mathbb E_q \\left[ \\log p ({\\bf y} | X, \\beta) \\right] - KL[q || p] \\\\\n&= \\mathbb E_q \\left[ \\log p ({\\bf y},  \\beta | X) \\right] + H(q) \\\\\n&= \\left(\\mathbb E_q \\left[ \\log p ({\\bf y},  \\beta | X) - Z \\right] + H(q) \\right) + Z \\\\\n&= - KL[q || p_{post}] + Z\n\\end{align}\\]\n\n\nWe’ll write the variational objective for SER \\(F_{SER}(q; {\\bf y}, X, \\sigma_0)\\)"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#susie-variational-approximation",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#susie-variational-approximation",
    "title": "Gilad Lab WIP",
    "section": "SuSiE: variational approximation",
    "text": "SuSiE: variational approximation\nRestrict \\(q\\) to some family \\(\\mathcal Q\\)\n\\[\\begin{align}\nq^*(\\beta) = \\arg \\max_{q \\in \\mathcal Q} F(q)\n\\end{align}\\]\nSuSiE uses \\(\\mathcal Q = \\{q : q(\\beta) = \\prod_l q_l(\\beta_l)\\}\\)\n\n\\[\nF_{SuSiE}(q; X, {\\bf y}) = \\mathbb E_{q} \\left[\n      -\\frac{1}{2\\sigma^2}||{\\bf y} - {\\bf X} \\beta_{-l} - X\\beta_l||^2_2)\n  \\right] + KL[q || p_{SuSiE}] + C(\\sigma)\n\\]"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#susie-coordinate-ascent-ibss",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#susie-coordinate-ascent-ibss",
    "title": "Gilad Lab WIP",
    "section": "SuSiE: coordinate ascent (IBSS)",
    "text": "SuSiE: coordinate ascent (IBSS)\nDefine the residual \\({\\bf r}_l = {\\bf y} - X\\beta_{-l}\\)\n\n\\[\n\\begin{aligned}\nF_{SuSiE}(q_l; q_{-l}, X, {\\bf y})\n&= \\mathbb E_{q_l} \\left[\n  \\mathbb E_{q_{-l}} \\left[\n      -\\frac{1}{2\\sigma^2}||{\\bf y} - {\\bf X} \\beta_{-l} - X\\beta_l||^2_2)\n  \\right]\n\\right] - KL[q || p] \\\\\n&= \\mathbb E_{q_l} \\left[\n  \\mathbb E_{q_{-l}} \\left[\n      -\\frac{1}{2\\sigma^2}|| {\\bf r}_l - X\\beta_l||^2_2)\n  \\right]\n\\right] - KL[q_l || p_l]  + C_1 \\\\\n&= \\mathbb E_{q_l} \\left[ \\color{blue}{\n    -\\frac{1}{2\\sigma^2} \\left(|| \\mathbb E_{q_{-l}} {\\bf r}_l- X\\beta_l||^2_2 + \\mathbb V ||r_l||^2_2 \\right)}\n\\right] - KL[q_l || p_l]  + C_1\\\\\n&= \\mathbb E_{q_l} \\left[\n    -\\frac{1}{2\\sigma^2} || \\mathbb E_{q_{-l}} {\\bf r}_l- X\\beta_l||^2_2]\n\\right] - KL[q_l || p_l]  + C_2  \\\\\n&= F_{SER}(q_l; X, \\mathbb E_{q{-l}}{\\bf r}_l) + C_2\n\\end{aligned}\n\\]\n\n\nKey: coordinate updates in \\(\\mathcal Q\\) reduce to solving an SER"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#generalized-ibss-gibss",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#generalized-ibss-gibss",
    "title": "Gilad Lab WIP",
    "section": "Generalized IBSS (GIBSS)",
    "text": "Generalized IBSS (GIBSS)\n\n\nHeuristic approach for applying SuSiE to non-Gaussian models\nIdea: apply IBSS, using expected predictions as fixed offsets\n\n\n\n\\[\\begin{align}\nF_{SuSiE}(q)\n&= \\mathbb E_q[\\log p({\\bf y} | X, \\beta_l, \\beta_{-l})] - \\sum_l KL[q_l || p_l] \\\\\n&= \\mathbb E_{q_l}[ \\mathbb E_{q_{-l}}[\\log p({\\bf y} | X, \\beta_l, \\beta_{-l})]] - \\sum_l KL[q_l || p_l] + C_1 \\\\\n&\\color{purple}{\\approx \\mathbb E_{q_l}[\\log p({\\bf y} | X, \\beta_l, \\bar \\beta_{-l})] - KL[q_l || p_l] + C_2} \\\\\n&= F_{SER}(q_l; {\\bf y}, X, \\bar \\beta_{-l}) + C_2\n\\end{align}\\]"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#why-should-this-work",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#why-should-this-work",
    "title": "Gilad Lab WIP",
    "section": "Why should this work?",
    "text": "Why should this work?\n\\[\n\\begin{aligned}\nF_{SuSiE}(q)\n&= \\mathbb E_q[\\log p({\\bf y} | X, \\beta_l, \\beta_{-l})] - \\sum_l KL[q_l || p_l] \\\\\n&\\color{purple}{\\approx \\mathbb E_{q_l}[\\log p({\\bf y} | X, \\beta_l, \\bar \\beta_{-l})] - KL[q_l || p_l] + C_2} \\\\\n\\end{aligned}\n\\]\n\n\nIf \\(\\log p( {\\bf y} | X, \\beta)\\) is quadratic in \\(\\beta\\) (Gaussian) there \\(\\color{purple}\\approx\\) is \\(=\\)\nInformally, approximation is good if \\(\\log p( {\\bf y} | X, \\beta)\\) is well approximated by a quadratic function in \\(\\beta\\).\nReally we care about \\(\\psi = X\\beta\\)"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#algorithm-single-effect-regression",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#algorithm-single-effect-regression",
    "title": "Gilad Lab WIP",
    "section": "Algorithm: Single effect regression",
    "text": "Algorithm: Single effect regression\nRequire a function \\(G\\) which computes the BF and posterior mean of a Bayesian univariate regression\n\\[\\begin{align}\n{\\bf y} \\sim 1 + {\\bf x} + {\\bf o} \\\\\nb \\sim N(0, \\sigma^2)\n\\end{align}\\]"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#advantage-and-limitations",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#advantage-and-limitations",
    "title": "Gilad Lab WIP",
    "section": "Advantage and limitations",
    "text": "Advantage and limitations\n\n\nAdvantages\n\nSeems to work well!\nHighly modular: just need to implement the univariate regression\n\n\n\n\nDisadvantages\n\nHeuristic– not optimizing a clear objective function\nDoes not properly account for uncertainty in \\(\\beta_{-l}\\), only uses posterior mean\n\n\n\n\nOpportunities\n\nCan we analyze the approximation error?\nCan we make guarantees on GIBSS performance asymptotically?"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#sources-of-error",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#sources-of-error",
    "title": "Gilad Lab WIP",
    "section": "Sources of error",
    "text": "Sources of error\n\n\nGIBSS with exact SER\n\nTreating random effects as fixed effects (only need to pass posterior means)\n\n\n\n\nGIBSS with asymptotic approximation\n\nAsymptotic approximation of posterior means (only need to compute MLE and std. errors)\nApproximation of BFs"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#algorithm-gibss",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#algorithm-gibss",
    "title": "Gilad Lab WIP",
    "section": "Algorithm: GIBSS",
    "text": "Algorithm: GIBSS"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#simulations-logistic-gibss-l5",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#simulations-logistic-gibss-l5",
    "title": "Gilad Lab WIP",
    "section": "Simulations: logistic-GIBSS \\(L=5\\)",
    "text": "Simulations: logistic-GIBSS \\(L=5\\)\nMuch better performance compared to direct VB approach"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#lasso-simulation",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#lasso-simulation",
    "title": "Gilad Lab WIP",
    "section": "Lasso simulation",
    "text": "Lasso simulation\nFit lasso to each gene list, simulate new gene lists from lasso fit (x20) - Gene sets: MSigDb Hallmark + C2 - ~7500 genes, ~4500 gene sets"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#lasso-simulation-cs-summary",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#lasso-simulation-cs-summary",
    "title": "Gilad Lab WIP",
    "section": "Lasso simulation CS summary",
    "text": "Lasso simulation CS summary\nSome of the CSs are large (uninformative)\n\n\n\n“Confident” discoveries 260/527 vs lasso 460 effects\nSuSiE has good CS coverage ~94%\nLasso has high FP rate ~34%\n\n\n\n\n\n\n\n# Causal x CS size\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n12\n\n\n\n\n\n\n0\n\n\n17\n\n\n3\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n1\n\n\n244\n\n\n6\n\n\n4\n\n\n2\n\n\n2\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n\n\n2\n\n\n0\n\n\n14\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n\n\n3\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#gibss-for-logistic-regression",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#gibss-for-logistic-regression",
    "title": "Gilad Lab WIP",
    "section": "GIBSS for logistic regression",
    "text": "GIBSS for logistic regression\nGoal: demonstrate improvement from using Laplace ABF over ABF in GIBSS for settings commonly found in case-control GWAS\n\nImplication: Naive application of “summary stat” based finemapping methods not appropriate for e.g. logistic regression, GLMs, GLMMs."
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#realistic-simulation-plan",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#realistic-simulation-plan",
    "title": "Gilad Lab WIP",
    "section": "Realistic simulation plan",
    "text": "Realistic simulation plan\nUKBB genotype data\n\nSimulate from UKBB imputed genotypes\nSelect 50k individual per UKBB, keep SNPs with MAF > 1% in sample\n\nSimulation parameters\n\n\n\\[\n\\begin{aligned}\ny_i \\sim Bernoulli(p_i) \\\\\n\\log \\frac{p_i}{1 - p_i} = \\beta_{0i} + \\beta x_i\\\\\n\\beta_{0i} \\sim N(b_0, \\sigma^2_0) \\\\\n\\beta \\sim N(0, \\sigma^2)\n\\end{aligned}\n\\]\n\n\nRandom intercept models genetic contributions at un-linked loci, larger \\(\\sigma^2_0\\) means focal SNP explains smaller fraction of \\(h^2\\)\n\\(\\sigma^2\\) should be selected to reflect un-normalized effect sizes in GWAS\nTune \\(b_0\\), \\(\\sigma^2_0\\) to reflect case-control ratios and polygenecity of trait"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#this-simulation",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#this-simulation",
    "title": "Gilad Lab WIP",
    "section": "This simulation",
    "text": "This simulation\n\n\n\\[\n\\begin{aligned}\ny_i \\sim Bernoulli(p_i) \\\\\n\\log \\frac{p_i}{1 - p_i} = \\beta_{0i} + \\beta x_i\\\\\n\\beta_{0i} \\sim N(b_0, \\sigma^2_0) \\\\\n\\beta \\sim N(0, \\sigma^2)\n\\end{aligned}\n\\]\n\n\nRandom intercept models genetic contributions at un-linked loci, larger \\(\\sigma^2_0\\) means focal SNP explains smaller fraction of \\(h^2\\)\n\\(\\sigma^2\\) should be selected to reflect un-normalized effect sizes in GWAS\nTune \\(b_0\\), \\(\\sigma^2_0\\) to reflect case-control ratios and polygenecity of trait\n\n\n\n\n\\(\\sigma^2_0 = 0\\) (fixed intercept)\nSelect causal SNP to have MAF in \\((0.2, 0.25)\\)\nTuned \\(b_0\\) to give frequency of cases to approx. \\(1 - 10\\%\\)\n50 replicates on one region of chromosome 1."
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#logistic-susie-abf-vs-laplace",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#logistic-susie-abf-vs-laplace",
    "title": "Gilad Lab WIP",
    "section": "Logistic SuSiE ABF vs Laplace",
    "text": "Logistic SuSiE ABF vs Laplace\n\n\n\n95% CS coverage\n\n\nMethod\nCoverage\n\n\n\n\nabf\n0.953\n\n\nlaplace_abf\n0.977\n\n\n\n\n\n{fig-align=“right”, height=10%}"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#gibss-for-survival-analysis",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#gibss-for-survival-analysis",
    "title": "Gilad Lab WIP",
    "section": "GIBSS for survival analysis",
    "text": "GIBSS for survival analysis\n\n\nYunqi has been investigating specifically the application of SuSiE for survival analysis.\n\n‘Non-standard’: optimization of partial likelihood to get MLE and standard error, censored data\nFits within the GIBSS framework!\n\n\n\n\n\n\n\nIndependent\n\n\n\n\n\n\n\nCorrelated (real genotypes)"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#survival-susie-vs-bvsnlp",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#survival-susie-vs-bvsnlp",
    "title": "Gilad Lab WIP",
    "section": "Survival SuSiE vs BVSNLP",
    "text": "Survival SuSiE vs BVSNLP\n\n\n\n\n\nSurvival SuSiE without ABF correction\n\n\n\n\n\n\n\nSurvival SuSiE with ABF correction"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#overview",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#overview",
    "title": "Gilad Lab WIP",
    "section": "Overview",
    "text": "Overview\n\nGIBSS requires computing the BF and posterior mean for each variable\nStandard statistical software for computing MLE\nFor large sample sizes, we can leverage asymptotic normality of the MLE to approximate posterior mean and BF."
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#posterior-mean",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#posterior-mean",
    "title": "Gilad Lab WIP",
    "section": "Posterior mean",
    "text": "Posterior mean\n\\[\n\\begin{aligned}\n\\hat \\beta | \\beta, s^2 &\\sim N(\\beta, s^2) \\\\\n\\beta &\\sim N(0, \\sigma^2)\n\\end{aligned}\n\\]\n\n\\[\n\\beta | \\hat \\beta, s^2 \\sim N \\left(\n\\frac{\\sigma^2}{s^2 + \\sigma^2} \\hat\\beta, \\left(\\frac{1}{s^2} + \\frac{1}{\\sigma^2}\\right)^{-1}\n\\right)\n\\]"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#approximating-the-the-bf",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#approximating-the-the-bf",
    "title": "Gilad Lab WIP",
    "section": "Approximating the the BF",
    "text": "Approximating the the BF\n\\[BF = \\int \\color{red}{\\frac{ p(\\mathcal {\\bf y} | {\\bf x}, \\beta)}{p({\\bf y} | \\beta = 0)}} N(\\beta | 0, \\sigma^2) d\\beta\\]\nApproximate the likelihood ratio in a way that’s easy to integrate\n\\[\nLR(\\beta_1, \\beta_2) = \\frac{p(\\mathcal {\\bf y} | {\\bf x}, \\beta)}{p({\\bf y} | \\beta=0)}\n\\]"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#wakefields-asymptotic-bf",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#wakefields-asymptotic-bf",
    "title": "Gilad Lab WIP",
    "section": "Wakefields asymptotic BF",
    "text": "Wakefields asymptotic BF\nAsymptotically, \\(\\hat \\beta | \\beta, s^2 \\sim N(\\beta, s^2)\\). Then\n\\[\nLR(\\beta, 0)\n= \\frac{p(y | \\hat\\beta, \\beta) p(\\hat\\beta | \\beta)}{p(y | \\hat\\beta, \\beta = 0) p(\\hat\\beta | \\beta = 0)}\n\\approx \\color{red}{\\frac{p(y | \\hat\\beta)}{p(y | \\hat\\beta)}} \\frac{p(\\hat\\beta | \\beta)}{ p(\\hat\\beta | \\beta = 0)} \\approx \\frac{N(\\hat\\beta| \\beta, s^2)}{N(\\hat\\beta| 0, s^2)} = \\widehat{LR}_{ABF}(\\beta, 0).\n\\]\n\nIntegrating over the prior gives Wakefield’s asymptotic Bayes Factor (ABF)\n\\[\nABF = \\int \\widehat{LR}_{ABF}(\\beta, 0) N(\\beta | 0, \\sigma^2_0) d\\beta = \\frac{N(\\hat\\beta | 0, s^2 + \\sigma^2_0)}{N(\\hat\\beta | 0, s^2)}\n\\]"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#a-problem-with-abf",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#a-problem-with-abf",
    "title": "Gilad Lab WIP",
    "section": "A problem with ABF",
    "text": "A problem with ABF\n\n\\(LR(\\beta, 0) \\approx \\widehat {LR}_{ABF}(\\beta, 0)\\)\n\\(LR(0, 0) = \\widehat {LR}_{ABF}(0, 0) = 1\\)\nThe asymptotic approximation may not be a good in the tails, an issue for \\(\\hat\\beta/s >> 0\\)"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#adjusting-the-abf",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#adjusting-the-abf",
    "title": "Gilad Lab WIP",
    "section": "Adjusting the ABF",
    "text": "Adjusting the ABF\nIdea: use the asymptotic approximation where it is good\n\\[\n\\begin{aligned}\nLR(\\beta, 0)\n&= LR(\\beta, \\hat{\\beta}) LR(\\hat{\\beta}, 0) \\\\\n&\\approx \\widehat{LR}_{ABF}(\\beta, \\hat\\beta)LR(\\hat\\beta, 0) \\\\\n&= \\widehat{LR}_{Lap}(\\hat\\beta, 0)\n\\end{aligned}\n\\]\n\nRequires an extra piece of information: \\(LR(\\hat\\beta, 0)\\)"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#corrected-abflaplace-approximation",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#corrected-abflaplace-approximation",
    "title": "Gilad Lab WIP",
    "section": "Corrected ABF/Laplace approximation",
    "text": "Corrected ABF/Laplace approximation\n\nRequires knowledge of the LR of the MLE against the null.\nCan dramatically improve approximation of the BF"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#comparison-of-logbf",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#comparison-of-logbf",
    "title": "Gilad Lab WIP",
    "section": "Comparison of logBF",
    "text": "Comparison of logBF"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#comparison-of-logbf-1",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#comparison-of-logbf-1",
    "title": "Gilad Lab WIP",
    "section": "Comparison of logBF",
    "text": "Comparison of logBF"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#abf-correction-reorders-pips",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#abf-correction-reorders-pips",
    "title": "Gilad Lab WIP",
    "section": "ABF Correction reorders PIPs",
    "text": "ABF Correction reorders PIPs"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#laplace-approximation",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#laplace-approximation",
    "title": "Gilad Lab WIP",
    "section": "Laplace approximation",
    "text": "Laplace approximation\nFor some non-negative function \\(f: \\mathbb R \\rightarrow \\mathbb R^+\\), want to compute the integral\n\\[I = \\int f(x) dx,\\]\n\nDefine \\(h(x) = \\log f(x)\\), expand around the maximizer of \\(h\\), \\(x^*\\)\n\\[\\hat h (x) = h(x^*) + \\frac{1}{2} h^{''}(x^*)(x-x^*)^2\\]\n\n\nApproximate with the Gaussian integral\n\\[ \\hat I = \\int \\exp{\\hat h(x))} dx = \\exp{\\hat h(x^*)} \\left(-\\frac{2\\pi}{h^{''}(x^*)}\\right)^{1/2}\\] . . .\nPotential next step: saddle point approximations?"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#takeaways",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#takeaways",
    "title": "Gilad Lab WIP",
    "section": "Takeaways",
    "text": "Takeaways\n\nLaplace approximation of the BF better than ABF for variable selection\nAddition information available from standard statistical software for GLMs\nPoor performance of ABF raises concerns about using SuSiE-RSS for summary statistics from non-Gaussian models\nGIBSS + asymptotic approximation looks like a good recipe for GLMs"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#fast-implementation",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#fast-implementation",
    "title": "Gilad Lab WIP",
    "section": "Fast implementation",
    "text": "Fast implementation\nProblem\n\nGIBSS requires evaluating many univariate regression\nGLMs fit via IRLS (equivalently, Newton-Raphson with step-size \\(\\gamma = 1\\)). \\(\\beta_t = \\beta_{t-1} - \\gamma H^{-1}_{t-1} g_{t-1}\\)\n\\(p\\) 2d optimization problems that can be carried out in parallel\n\nImplementation\n\nImplemented in Python using Google’s jax (numpy, with automatic differentiation, JIT compilation, and vectorization)\nNewton-Raphson with halving step-size\nEasy to extend to many GLMs (or any regression with twice-differentiable log-likelihood)– just define a log_likelihood function.\nsusiepy package at http://github.com/karltayeb/susiepy\n\nOpen questions for optimizing implementation\n\nCoordinate ascent faster on machine compared to full Newton updates?\nAvoid fitting intercept for each variable separately?\nAvoid checking for likelihood increase\nExploit sparsity in \\(X\\) for faster implementation?\nPartial updates of “inner loop” while fitting GIBBS"
  }
]