[
  {
    "objectID": "research/polynomial_approximation/index.html#overview",
    "href": "research/polynomial_approximation/index.html#overview",
    "title": "Polynomial approximation for variational Bayes",
    "section": "Overview",
    "text": "Overview\nWe propose approximating Bayesian linear regression problems by replacing the conditional likelihood of each observation with a polynomial approximation designed to be close to the true likelihood on an interval, and a lower bound for values far outside the interval. If the log density of our prior distribution can also be expressed (or approximated) as a polynomial the exact posterior of this polynomial approximation is also a polynomial– i.e. conjugate.\nThis is the main benefit of our approach: inference boils down to simple manipulations of the polynomial coefficients, which can be performed in parallel and with a set of simple linear algebra operations (matrix multiplication, solving a triangular system, etc). There may exist more computationally efficient algorithms for carrying out these operations, which we should explore further.\nThis approach can be generalized to many likelihood (e.g. Binomial, Poisson) with various choices of link function (which relate the regression predictions to the distribution parameters)– at least to the extent that we can develop a good polynomial approximation for each combination of likelihood and link.\nTake a look at this rough implimentation."
  },
  {
    "objectID": "research/polynomial_approximation/index.html#global-polynomial-approximations",
    "href": "research/polynomial_approximation/index.html#global-polynomial-approximations",
    "title": "Polynomial approximation for variational Bayes",
    "section": "Global polynomial approximations",
    "text": "Global polynomial approximations\n\nLocal approximations, and trouble with SuSiE\nThis approach was motivated by our work to develop a good variational approximation for logistic SuSiE, that is a logistic regression with the sum of single effects (SuSiE) [(susie?)]. For Gaussian linear models, using a variational approximation that factorizes over single effects produces a lightning fast Bayesian variable selection method. Rather than exploring each configuration of non-zero effect variables, this approximation allows us to update our approximating distribution for one effect while marginalizing over all other effects. Inference in the (Gaussian, linear) SuSiE model reduces to a sequence of embarrassingly parallel univariate linear regression problems.\nIn the case of Gaussian model with identify link (linear regression), the variational approximation is sufficient for computation to simplify, because the log likelihood is quadratic in the regression coefficients \\(\\beta\\). However, the Bernoulli likelihood with a logistic link function (logistic regression), and indeed for many other likelihood-link combinations of interest, do not enjoy the algebraic simplicity of a Gaussian model with identity link (linear regression). The main issue is that it is difficult to marginalize over the other effects. Even if the approximate posterior factorizes, the likelihood computation can combine single effects in a way that complicates the marginalization step.\nWe’ve attempted using local quadratic approximations to the likelihood. The idea is to construct a quadratic lower bound to the likelihood that is “good” in a neighborhood that we care about (contains most of the posterior mass). This is a popular technique for applying variational Bayes to logistic regression (see Jaakkola and Jordan, Polya-Gamma augmentation), and indeed these sorts of “local” approximation schemes can be generalized to super-Gaussian likelihoods [(Galy-Fajou, Wenzel, and Opper 2020)].\nHowever, we find that the local approximation techniques are not well suited to variable selection techniques like SuSiE. In order to use the local approximation techniques, we must optimize a set of variational parameters that essentially dictate where the approximation is good. You only get to select one local approximation per observation. Holding these variational parameters fixed, our inference will systematically favor posterior distributions that place more mass in a neighborhood of these variational parameters. The strong coupling between the local variational parameters and optimal variational approximation make it difficult to navigate the optimization surface via coordinate ascent.\nAdditionally, uncertainty quantification is an important output to SuSiE and other Bayesian variable selection methods. Due to the nature of the local variational approximations we may tend to see a “winner take all” scenario. Assuming we find a good local optimum, the local variational parameters will be tuned to be consistent with the most likely variable configurations. In turn, the objective function will be a tighter lower bound in these regions, and the approximating distribution will place more probability mass on these configurations.\nNote: the Laplace approximation is also a “local” approximation, since we construct a quadratic approximation of the joint log likelihood around the MAP estimate. It is also closely related Gaussian variational approximations.\n\n\nGlobal approximations\nRather than find a “local” approximation which has variational parameters that must be tuned at each iteration, we propose finding a global approximation to the likelihood which is easier to work with.\n“Easier to work with”, in the context of SuSiE means that we can write \\(f(\\psi_l) =\\mathbb E_{q_{-l}}[\\log p(y | \\psi_l + \\psi_{-l})]\\), where \\(f(\\psi_l; q_{-l})\\) is easy to evaluate/optimize over. Notice that if we approximate \\(\\log p(y | \\psi) \\approx f(\\psi) = \\sum_k c_k \\psi^k\\) then taking the expectation of \\(f\\) over \\(q_{-l}\\) results in a polynomial in \\(\\psi_l\\).\n\\[\nE_{q_{-l}}[f(\\psi_l + \\psi_{-l})] = \\hat f(\\psi_l; q_{-l}) = \\sum \\hat c_k(q_{-l}) \\psi_l^k.\n\\]\nHere \\(\\hat c_k(q_{-l})\\) are a transformation of the original coefficients \\(c_k\\) that are achieved by (1) expanding the original polynomial in terms of \\(\\psi_l\\) and \\(\\psi_{-l}\\), (2) marginalizing over \\(\\psi_{-l}\\) w.r.t \\(q_{-l}\\) and then (3) regrouping terms to get coefficients of a polynomial in \\(\\psi_l\\).\nWhile the local variational approximations are only good around a point specified by the variational parameter, we can construct a “global” polynomial approximation that is good on any interval if we allow the degree of the polynomial to be sufficiently high. While we sidestep the issue of needing to tune the local variational parameters, we replace it with the need to select an interval that we care to approximate. There is a trade off here– for a fixed error tolerance, wider intervals will require higher degree approximations to bound the error."
  },
  {
    "objectID": "research/polynomial_approximation/index.html#polynomial-representation",
    "href": "research/polynomial_approximation/index.html#polynomial-representation",
    "title": "Polynomial approximation for variational Bayes",
    "section": "Polynomial representation",
    "text": "Polynomial representation\n\nRepresenting functions with polynomial series\nLet \\(\\left\\{ P_k \\right\\}_{k=1}^{\\infty}\\) be a sequence of polynomials that form a basis for continuous functions on an interval, e.g. \\([-1, 1]\\) (note: that through a change of variable we can stretch this interval to any interval we want). So that for any \\(f: [-1,1] \\\\rightarrow \\mathbb R\\) there are \\(\\left\\{ c_k \\right\\}\\) such that\nWe’ll assume that \\(P_k\\) are ordered by increasing degree, and that \\(deg(P_k) \\leq k\\).\n\n\nChebyshev polynomials\nThese are a family of polynomials on \\([-1, 1]\\) defined by\n\\[T_n(\\cos(\\theta)) = \\cos(n \\theta)\\]\nThey can also be obtained by the reccurrence\n\\[\n\\begin{aligned}\n  T_0(x) &= 1 \\\\\n  T_1(x) &= x \\\\\n  T_{n+1}(x) &= 2x T_n(x) - T_{n-1}(x)\n\\end{aligned}\n\\] The Chebyshev polynomials are orthogonal to eachother, and form a basis for ~a certain family of function~ on the interval \\([-1, 1]\\), so that \\(f(x) = \\sum_{k=0}^\\infty c_k T_k(x)\\). This means that we can get the coefficients by evaluating an inner product\n\\[\n\\langle f, T_k \\rangle = c_k ||T_k||^2\n\\] Through a change of variable, we can approximate functions on any interval. We can obtain a \\(K+1\\) degree polynomial approximation by computing the first \\(K\\) coefficients \\((c_k)_{k=0}^K\\). Each coefficient \\(c_k\\) is effectively computed via a \\(k+1\\) point quadrature (Note: it looks like there is a simple rule for computing the coefficients as linear combinations of \\(f\\) evauated at a set of “nodes”, this looks very closely related to the quadrature rule, but I want to figure this out in more detail)\n\n\nConversion between polynomial basis\nSuppose we have a polynomial basis \\(\\{A_k\\}\\) and \\(\\{B_k\\}\\). Let \\(M_k(x) = x^k\\), \\(\\{M_k\\}\\) is the “monomial basis”, so that we can write \\(A_k(x) = \\sum_{j=0}^k \\alpha_{kj} M_j(x)\\). We can arrange these coefficients for the first \\(K\\) polynomials into the upper triangular matrix:\n\\[\nA^{(k)} =\n\\begin{bmatrix}\n\\alpha_{00} & 0 & 0 &\\dots & 0 \\\\\\\\\n\\alpha_{10} & \\alpha_{11} & 0 & \\dots & 0 \\\\\\\\\n\\alpha_{20} & \\alpha_{21} & \\alpha_{22} &\\dots & 0 \\\\\\\\\n\\dots\\\\\\\\\n\\alpha_{K0} & \\alpha_{K1} & \\alpha_{K2} &\\dots & \\alpha_{KK}\n\\end{bmatrix}\n^T\n\\]\nNow we can see that for \\(f(x) = \\sum_{k=0}^K a_k A_k(x)\\). We can take the vector of coefficients \\(\\vec a = (a_0, \\dots, a_K)\\) and convert back to coefficients in the monomial basis with a simple matrix multiplication\nTo convert from the monomial basis to the basis \\(A\\) invovles solving the triangular system\nTo convert from the monomial basis to the basis \\(A\\) to the basis \\(B\\) involves (1) expanding to the monomial basis and (2) solving for the coefficients in basis \\(B\\).\nApparently there are \\(O(K\\log(K))\\) algorithms for changing basis, it may be worth understanding these. But it’s very easy to see how we can move between different bases for polynomials of degree \\(K\\) by matrix multiplication.\n\n\nShifting and scaling\nWe can “shift” the polynomial basis too, that is we rewrite \\(f(x + y) = g(x; y)\\) there \\(g\\) is a polynomial in \\(x\\). This has the effect of moving fixed information \\(y\\) out of the functions argument and into the polynomial coefficients.\nHere \\(\\tilde M_k(x_1; x_2)\\) is a \\(k\\) degree polynomial in \\(x_1\\). \\(x_2\\) is treated as a parameter that is absorbed into the coefficients. Next we’d like to do the same for general polynomials. So we’d like to find \\(\\tilde a\\) such that \\(f(x_1 + x_2) = \\sum a_k A_k(x_1 + x_2) = \\sum_k \\tilde a(x_2) A_k(x_1)\\).\nLet \\(m_{kj} = 0 \\; \\forall k < j\\), and \\(M(x_2)\\) be the upper triangular matrix \\(M(x_2) = \\begin{bmatrix} m_{kj}(x_2)\\end{bmatrix}_{k=0, \\dots K,\\\\; j=0,\\dots,K}\\)\nThen we can find the coordinates\n\\[\n\\tilde a(x_2) = (A^{(k)})^{-1}M(x_2)A^{(k)} a\n\\]\nand\n\\[\n\\mathbb E_{q(x_2)}\\left[ \\tilde a(x_2) \\right] =  (A^{(k)})^{-1}\\mathbb E_{q(x_2)}\\left[ M(x_2) \\right]A^{(k)} a\n\\]\n\n\nThings to look into\nClenshaw algorithm and Horner’s method. these are recursive methods for evaluating polynomials in the Chebyshev and monomial basis respectively.\nHamming and Salzer develop algorithms for converting polynomials between different basis representations.\nWe may not be able to use these techniques, unless we can get an expression for each coefficient, because we need to evaluate the expected value of the terms."
  },
  {
    "objectID": "research/polynomial_approximation/index.html#variational-approximation",
    "href": "research/polynomial_approximation/index.html#variational-approximation",
    "title": "Polynomial approximation for variational Bayes",
    "section": "Variational approximation",
    "text": "Variational approximation\nHere we present the variational approximation for SuSiE and explore how to perform coordinate ascent variational inference with this approximation and our polynomial likelihoods.\n\nEvidence lower bound, and a polynomial approximation\nWe can write the log likelihood for a single observation as a function \\(y_i\\) of a function of the linear predictor \\(\\psi_i = \\sum_{l} \\psi_{li}\\).\n\\[\n\\log p (y_i | \\psi_i) = l_i(\\psi_i) \\approx \\sum_{k=0}^K a_{yk} A_k(\\psi)\n\\]\nWe will take a polynomial approximation to \\(l(y_i; \\psi_i) \\approx \\sum_k c_{ik} P_k(\\psi_i) = f_i(\\psi_i)\\). Then we can approximate the ELBO\nWe perform inference by selecting \\(q \\in \\mathcal Q\\) to optimize \\(F_2\\), where \\(\\mathcal Q\\) is a family of distributions.\n\\[\nq^* = \\arg\\max_{q \\in \\mathcal Q} F_2(q)\n\\]\n\n\nSuSiE variational approximation\nFor SuSiE our latent variables consist of the effect sizes \\(\\left\\{ b_l \\right\\}\\_{l=1}^L\\) and a set of indicators that select \\(L\\) non-zero effect variables \\(\\left\\{ \\gamma_l \\right\\}\\_{l=1}^L\\). We select \\(\\mathcal Q\\) to be the family of distributions that factorize over the \\(L\\) single effects, that is\n\\[\nq(\\left\\{ b_l \\right\\}, \\left\\{ \\gamma_l \\right\\}) = \\prod_l q(b_l | \\gamma_l)q(\\gamma_l).\n\\]\n\n\nCoordinate ascent\nTo update \\(q_l\\) we need to maximize\nDropping the subscript \\(i\\), for each term in the sum we need to compute\nWe can do this by applying the “expected” shift operator. We can do this by computing the moments of \\(\\psi_{-l}\\) and applying the shift operation once, or by computing the moments of \\(\\psi_m, \\;\\; m\\neq l\\) and performing the shift operation sequentially.\n(TODO: update notation here)\n\\[\n\\tilde a(q_{-l})\n= (A^{(k)})^{-1} \\left(\\prod_{m \\neq l} \\mathbb E_{q_m}\\left[ M(\\psi_m) \\right] \\right) A^{(k)} a\n\\]\nA nice feature of the sequential approach is it gives us an easy way of converting between polynomial representations. Let \\(\\Gamma_l = \\mathbb E_{q_l}\\left[ M(\\psi_l) \\right]\\) be the matrix for applying the “shifted expectation” operation to the polynomial coefficients for \\(f(\\psi_l)\\), \\({\\bf m}\\). That is \\(\\Gamma_l {\\bf m}\\) gives the coefficients of \\(f(\\psi_{-l}; q_l)\\), which is a polynomial in \\(\\psi_{-l}\\).\nLet \\(\\Gamma = \\prod_l \\Gamma_l\\). Notice that the polynomial with coefficients \\(\\Gamma {\\bf m}\\) evalutated at \\(0\\) give \\(\\mathbb E_{q}\\left[ f(x) \\right]\\). Furthermore we can quickly move from \\(f(\\psi_{l}; q_{-l})\\) to \\(f(\\psi_{l+1}; q_{-(l+1)})\\).\nStarting with \\(f(\\psi_{1}; q_{-1})\\) we want the coefficients \\(\\Gamma_{-1} {\\bf m}\\), where \\(\\Gamma_{-1} = \\Gamma_1^{-1} \\Gamma\\). Then, to get \\(f(\\psi_{1}; q_{-1})\\) we need \\(\\Gamma_{-2} {\\bf m}\\), where we can compute \\(\\Gamma_{-2}\\) by\n\\[\n\\Gamma_{-2} = \\Gamma_2^{-1} \\Gamma_{-1} \\Gamma_1.\n\\] We can continue iterating over all \\(L\\) single effects. We note that it is easier to compute the moments of the single effects \\(\\psi_m\\) rather than the moments of all “other” single effects \\(\\psi_{-l}\\). Carrying out the iterated expectations as matrix-vector products in the polynomial coefficients seems like an appealing approach to implementation.\nThis is useful in a coordinate ascent update scheme where we can remove one of the single effect from \\(\\Gamma\\) by a triangular system. Update \\(q_l\\), and then add back the update \\(\\Gamma_l\\) to \\(\\Gamma\\) by a right matrix multiplication.\n\n\nUnconstrained variational posterior\nThe optimal variational approximation looks like\n\\[\nq^*\\_l(b_l | \\gamma_l = j) \\propto e^{f(b_l; q_{-l}, {\\bf x}\\_j)}.\n\\] Where \\(f(b_l; q_{-l}, {\\bf x}\\_j) = \\sum_k \\eta_k b_l^k\\) is a polynomial of degree \\(K\\). Notice that this is an exponential family with sufficient statistics \\(T(b_l) = (b_l^k)\\_{k=0}^K\\) and natural parameters \\({\\bf \\eta}\\). It has a normalizing constant \\(A(\\eta) = \\log \\int \\exp\\{\\langle T(x), \\eta \\rangle\\} dx\\), and \\(\\nabla_{\\eta} A(\\eta) = \\mathbb E_{q}\\left[ T(x) \\right]\\). Thus if we can compute (or approximate to satisfactory precision) \\(\\nabla_{\\eta} A(\\eta)\\) we could compute the moments we need for CAVI.\nTo date, I am not really sure how to handle this integral of an exponentiation polynomial. By designing our polynomial approximation correctly, we can ensure that the the exponentiation function will decay and the \\(A\\) will be finite (recall also that \\(\\left\\{ \\eta: A(\\eta) < \\infty \\right\\}\\) is the natural parameter space).\nOne option is to approximate \\(A(\\eta)\\) by a quadrature rule. We can use automatic differentiation to compute it’s gradient.\n\n\nBest gaussian approximation\nMaybe we don’t know how to compute \\(A(\\eta) = \\log \\int \\exp\\{\\langle T(x), \\eta \\rangle\\} dx\\) Which involves evaluate the integral of an exponentiated polynomial. But perhaps we want to use a Gaussian variational approximation.\n\\[\nq_{l}^*(x) \\propto e^{f(x)}\n\\approx e^{f(\\mu) + f'(\\mu)(x-\\mu) + \\frac{1}{2}f''(\\mu)(x - \\mu)^2}\n\\]\nFor \\(\\mu\\) such that \\(f'(\\mu) = 0\\)\n\\[\ne^{f(\\mu) + f'(\\mu)(x-\\mu) + \\frac{1}{2}f''(\\mu)(x - \\mu)^2} \\propto e^{\\frac{1}{2}f''(\\mu)(x - \\mu)^2} \\propto \\mathcal N (x; \\mu, -\\frac{1}{f''(\\mu)})\n\\]\nIn our case \\(f\\) is a polynomial. Finding \\(\\mu\\) can be achieved by searching over the roots of \\(f'\\) and then \\(f''(\\mu)\\) is computed easily. This is a Laplace approximation to the optimal posterior \\(q_l\\)"
  },
  {
    "objectID": "research/polynomial_approximation/index.html#more-scattered-notes-on-polynomial-approximation-for-susie",
    "href": "research/polynomial_approximation/index.html#more-scattered-notes-on-polynomial-approximation-for-susie",
    "title": "Polynomial approximation for variational Bayes",
    "section": "More scattered notes on polynomial approximation for SuSiE",
    "text": "More scattered notes on polynomial approximation for SuSiE\nAbusing notation a bit, \\(\\phi_l = x_{\\gamma_l} b_l\\).\n\\[\n\\begin{aligned}\nf(\\psi_l)\n&= \\sum \\tilde m(q_{-l}) M_k(\\psi_l) \\\\\\\\\n&= \\sum \\tilde m(q_{-l}) A_k(x_jb_l) \\\\\\\\\n&= \\sum \\hat m_k(q_{-l}, x_j) M_k(b_l) \\\\quad \\hat m_k(q_{-l}, x_j) := \\tilde m_k(q_{-l})x_j^k\n\\end{aligned}\n\\]\n\\[\n\\mathbb E_{q_{-l}}\\left[ f(\\psi) \\right]\n= \\mathbb E_{q_{-l}}\\left[ f(\\psi_l, \\psi_{-l})) \\right]\n= \\sum \\mathbb E_{q_{-l}}\\left[ \\tilde {\\bf m}(\\psi_{-l}) \\right] M_k(\\psi_l)\n= \\sum \\left(\\prod_{m \\neq l}\\mathbb E_{q_m}\\left[ M(\\psi_m)) \\right]\\right){\\bf m}\n\\]\nWe can write\n\\[\n\\Gamma_l = \\mathbb E_{q_l}\\left[ M(\\psi_l) \\right]\n\\] \\[\n\\Psi^{(l)} = \\Gamma_{l+1} \\dots \\Gamma_L \\Gamma_1 \\dots \\Gamma_{l-1}\n\\] Then we can compute the coefficients of \\(f(\\psi_l)\\) by a triangular matrix multiplication\n\\[\n\\tilde{\\bf m}\\_l = \\Psi^{(l)}{\\bf m} = \\mathbb E_{q_{-l}}\\left[ M(\\psi_{-l}) \\right]{\\bf m}\n\\]\nAnd we can compute the next \\(\\Psi\\) by a triangular matrix inversion and two matrix multiplications.\n\\[\n\\Psi^{(l+1)} = \\Gamma_{l+1}^{-1} \\Psi^{(l)} \\Gamma_l\n\\] ### Rescal polynomial\n\\[\nf(bx) = \\sum m_k (bx)^k = \\sum m_k b^kx^k = \\sum (m_k b^k) x^k\n\\]\n\nShift\n\\[\nf(x - c)\n= \\sum m_k (x -c)^k\n= \\sum_k m_k \\sum_{j \\leq k} {k \\choose j} x^j c^{k-j}\n= \\sum_j \\left(\\sum_{k \\geq j} {k \\choose j} c^{k-j}\\right) x^j\n\\]\n\n\nUpdating \\(q_l\\)\nWe’ve written the natural parameters\nFor each observation we can compute\n\\[\n\\hat{{\\bf m}}\\_{li} = \\tilde{\\bf m}(q_{-l}) \\circ (x_i^0, \\dots, x_i^K)\n\\]\nThese are the coefficients in the monomial basis for each observation conditional on effect \\(b_l\\) for covariate \\(x_i\\). \\(f_i(b_l) = \\sum \\tilde {\\bf m}\\_k b_l^k\\). That is, this is the data likelihood as a function of \\(b_l\\), conditional on data \\(x\\), and marginalizing over \\(\\psi_{-l}\\).\nWe can express or approximate our prior with the same polynomial expansion. Suppose we can write our prior\n\\[\n\\log p(b_l) = \\sum \\rho_{kl} b_l^k\n\\]\nThen the posterior distribution is trivially computed with a conjugate computation\n\\[\n{\\bf \\eta}\\_l =\\sum_i \\hat {\\bf m}_{li} + {\\bf \\rho}_l\n\\]\nThe posterior distribution is an exponential family with sufficient statistics \\(T(b_l) = (b_l^0, \\dots, b_l^K)\\) and natural parameters \\(\\eta_l\\).\nIf our original polynomial approximation “goes down” outside the range we care to ensure it is a good approximation, then we should always get a finite log-normalizer/cumulant \\(A(\\eta) = \\log \\int \\exp\\{\\eta^T T(\\psi)\\} d\\psi < \\infty\\). It may be important to ensure that our approximation is good over the range of values of \\(\\psi\\) with high posterior probability. Supposing we have an even degree polynomial assumption, make sure the last coefficient is \\(< 0\\) so that the function is very negative for arguments that are large in absolute value, but the approximation is good for values of small absolute value. Intuitivley, taking expectations over \\(\\psi_{-l}\\) won’t change t\nAdditionally, we ideally want to make sure that our likelihood approximation does not have bad behavior. If our polynomial approximation wildly overestimates the likelihood in some regions that could seriously mess up our inference. There is probably a tradeoff. We can approximate \\(l_i\\) on the interval \\([a, b]\\) with lower error with a polynomial of degree \\(K\\). To approximate \\(l_i\\) on the wider interval \\([a, b] \\subset [A, B]\\) with the same error we need a higher degree \\(K\\).\n\n\nComputing moments of \\(q_l\\)\nAn algorithm would look like this\n\nCompute \\(\\Psi_1\\)\nUpdate update \\(q_1\\)\nCompute \\(\\mathbb E_{q_1}\\left[ M(\\psi_1) \\right]\\)\nCompute \\(\\Psi_2\\) …\n\nNote that \\(\\Psi_l\\) is constructed by taking expectations is a particular order, or multiplying matrices in a particular order. But I think order should not matter. Is it the case that triangular matrix multiplication commutes?"
  },
  {
    "objectID": "research/polynomial_approximation/index.html#glossary",
    "href": "research/polynomial_approximation/index.html#glossary",
    "title": "Polynomial approximation for variational Bayes",
    "section": "Glossary",
    "text": "Glossary\n\n\n\n\n\n\n\nSymbol\nDescription\n\n\n\n\n\\({\\bf a}_i\\)\ncoefficients for \\(f_i\\) in basis \\(\\mathcal A\\)\n\n\n\\(\\tilde {\\bf a}_i(\\psi_2)\\)\ncoefficients for \\(f_i(\\psi_1; \\psi_2)\\) in the basis \\(\\mathcal A\\)\n\n\n\\(\\hat {\\bf a}_i(\\psi_2, x_j)\\)\ncoefficients for \\(f_i(b_1; x_j, \\psi_2)\\) in the basis \\(\\mathcal A\\)\n\n\n\\(M(\\psi_2)\\)\ntriangular matrix shifts monomial basis, \\(\\tilde {\\bf m} (\\psi_2) = M(\\psi_2) {\\bf m}\\). Gives coefficients of \\(f_i(\\psi_1; \\psi_2)\\) in \\(\\mathcal M\\)\n\n\n\\(A\\)\ntriangular matrix maps to coordinates in monomial basis, \\({\\bf m} = A {\\bf a}\\). Gives coefficients of \\(f_i(\\psi_1; \\psi_2)\\) in \\(\\mathcal M\\)\n\n\n\\(f_i(\\psi_1; \\psi_2)\\)\nA polynomial is \\(\\psi_1\\) such that \\(f_i(\\psi_1; \\psi_2) = f_i(\\psi_1 + \\psi_2); \\\\;\\\\; \\tilde {\\bf m}(\\psi_2) = M(\\psi_2){\\bf m}\\) gives the coordinates in the monomial basis"
  },
  {
    "objectID": "research/polynomial_approximation/index.html#related-work",
    "href": "research/polynomial_approximation/index.html#related-work",
    "title": "Polynomial approximation for variational Bayes",
    "section": "Related work",
    "text": "Related work\n(Huggins, Adams, and Broderick 2017) (Wong, n.d.)"
  },
  {
    "objectID": "research/polynomial_approximation/index.html#references",
    "href": "research/polynomial_approximation/index.html#references",
    "title": "Polynomial approximation for variational Bayes",
    "section": "References",
    "text": "References\n\n\nGaly-Fajou, Theo, Florian Wenzel, and Manfred Opper. 2020. “Automated Augmented Conjugate Inference for Non-conjugate Gaussian Process Models.” In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, 3025–35. PMLR. https://proceedings.mlr.press/v108/galy-fajou20a.html.\n\n\nHuggins, Jonathan, Ryan P Adams, and Tamara Broderick. 2017. “PASS-GLM: Polynomial Approximate Sufficient Statistics for Scalable Bayesian GLM Inference.” In Advances in Neural Information Processing Systems. Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2017/hash/07811dc6c422334ce36a09ff5cd6fe71-Abstract.html.\n\n\nWong, Lilian. n.d. “Orthogonal Polynomials–Quadrature Algorithm (OPQA): A Functional Analytical Approach to Bayesian Inference.” Orthogonal Polynomials."
  },
  {
    "objectID": "research/polynomial_regression_vb/index.html",
    "href": "research/polynomial_regression_vb/index.html",
    "title": "Implementing polynomial approximation VB",
    "section": "",
    "text": "Code\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(pracma)\nlibrary(tictoc)\n\n\n\nAttaching package: 'tictoc'\n\n\nThe following objects are masked from 'package:pracma':\n\n    clear, size, tic, toc\n\n\nCode\nset.seed(1)"
  },
  {
    "objectID": "research/polynomial_regression_vb/index.html#computation",
    "href": "research/polynomial_regression_vb/index.html#computation",
    "title": "Implementing polynomial approximation VB",
    "section": "Computation",
    "text": "Computation\n\nChebyshev approximations\nChebyshev polynomials can be used to approximate functions on the interval \\([-1, 1]\\) (and then also, on any finite interval). \\(f(x) \\approx \\sum c_k T_k(x)\\). We compute the coefficients of \\(c_k\\) for \\(k = 0, \\dots K\\).\nIn the code below we use pracma::polyApprox which implement a scheme of evaluate the coefficients for \\(f\\). This is essentially done via quadrature.\n\n\nCode\nmake_approximation <- function(f, R, n, plot=F){\n  p <- rev(pracma::polyApprox(f, -R, R, n =n)$p)\n  if(plot){\n    S <- R + 2\n    x <- seq(-S, S, by=0.1)\n    plot(f, -S, S)\n    lines(x, polyval2(p, x), col='red', lty='dotted')\n    abline(v=-R); abline(v=R)\n  }\n  return(p)\n}\n\n\n\n\nScaling and shifting polynomials\nCoefficients after a change of variable\n\\[\nf(bx) = \\sum m_k (bx)^k = \\sum m_k b^kx^k = \\sum (m_k b^k) x^k = f_b(x)\n\\]\n\\[\nf(x + c)\n= \\sum m_k (x + c)^k\n= \\sum_k m_k \\sum_{j \\leq k} {k \\choose j} x^j c^{k-j}\n= \\sum_j \\left(\\sum_{k \\geq j} {k \\choose j} c^{k-j}\\right) x^j\n\\]\n\n\nCode\n#' p coefficients of a polynomial in increasing order\n#' @param p polynomial coefficients in INCREASING deree\npolyval2 <- function(p, x){pracma::polyval(rev(p), x)}\n\n#' f(x + c) = f2(x)\nshift_polynomial <- function(p, c){\n  # construct map\n  K <- length(p) - 1\n  M <- matrix(nrow= K+1, ncol=K+1)\n  for(j in 0:K){\n    for(k in 0:K){\n      M[j+1, k+1] <- choose(k, j) * c**(k - j)\n    }\n  }\n  \n  coef_new <- (M %*% p)[, 1]\n  return(coef_new)\n}\n\n# change back to original scale\n# f(bx) = f2(x)\nscale_polynomial <- function(p, b){\n  K <- length(p) - 1\n  coef_new <- p * sapply(0:K, function(k) b**k)\n  return(coef_new)\n}\n\n\n\n\nLaplace approximation to polynomial\n\n\nCode\n#' convert (unnormalized) polynomial density to gaussian approximation\n#' p the coefficients of a polynomial in increasing order p = c(p0, p1, ..., pK)\npoly_to_gaussian <- function(p){\n  p <- rev(p)\n  d <- pracma::polyder(p)\n  d2 <- pracma::polyder(d)\n  \n  #f <- function(x){polyval2(p, x)}\n  #mu <- optimize(f, interval = c(-100, 100), maximum = T)$maximum\n  roots <- Re(pracma::polyroots(d)$root)\n  mu <- roots[which.max(pracma::polyval(p, roots))]\n  \n  var <- - 1 / pracma::polyval(d2, mu)\n  return(list(mu=mu, var=var))\n}\n\n\nTo test it out:\n\n\nCode\ncoef<- c(1, 2, 3)\ncoef2 <- shift_polynomial(coef, -1)\n#f(x-1) = f2(x)\n(polyval2(coef, 3) == polyval2(coef2, 4))\n\n\n[1] TRUE\n\n\nCode\ncoef<- c(1, 2, 3)\ncoef2 <- scale_polynomial(coef, 2)\n# f(2x) = f2(x)\n(polyval2(coef, 6) == polyval2(coef2, 3))\n\n\n[1] TRUE\n\n\n\n\nPlot functions\n\n\nCode\nplot_effect_posterior <- function(q, b, ...){\n  mu_post <- purrr::map_dbl(q, ~purrr::pluck(.x, 'mu'))\n  var_post <- purrr::map_dbl(q, ~purrr::pluck(.x, 'var'))\n  \n  plotrix::plotCI(\n    x = b,\n    y = mu_post,\n    li =  mu_post - 2 * sqrt(var_post),\n    ui =  mu_post + 2 * sqrt(var_post),\n    ...\n  )\n  abline(0, 1, col='red')\n}"
  },
  {
    "objectID": "research/polynomial_regression_vb/index.html#gaussian-linear-regression-mean-field-approximation",
    "href": "research/polynomial_regression_vb/index.html#gaussian-linear-regression-mean-field-approximation",
    "title": "Implementing polynomial approximation VB",
    "section": "Gaussian linear regression, mean field approximation",
    "text": "Gaussian linear regression, mean field approximation\nHere we illustrate a simple example of our inference technique. We fit a mean field approximation to a bivariate regression problem\n\\[\n\\begin{aligned}\ny &\\sim N(\\sum_j x_j b_j, 1)\\\\\nb_j &\\sim N(0, 1)\\;\\; j = 1, \\dots, p\n\\end{aligned}\n\\]\nWhere \\(x_1\\) and \\(x_2\\) are correlated (inducing dependence in the posterior \\(p(b_1, b_2 | \\mathcal D)\\).\n\nPolynomial representation of Gaussian likelihood\nFor the observations\n\\[\nl(\\psi)\n= C -\\frac{1}{2\\sigma^2}(y - \\psi)^2\n= C -\\frac{1}{2\\sigma^2} y^2 + \\frac{y}{\\sigma^2}\\psi - \\frac{1}{2\\sigma^2}\\psi^2 \\implies {\\bf m}\n= (C -\\frac{y^2}{2 \\sigma^2}, \\frac{y}{\\sigma^2}, -\\frac{1}{2\\sigma^2})\n\\] For the prior\n\\[\n\\log p(b) = C -\\frac{1}{2\\sigma^2} \\left(b - \\mu \\right)^2\n\\implies {\\bf m} = \\left(C - \\frac{\\mu^2}{2\\sigma^2}, \\frac{\\mu}{\\sigma^2}, -\\frac{1}{2 \\sigma^2}\\right)\n\\]\n\n\nMean field variational approximation\n\\[\n\\begin{aligned}\nq({\\bf b}) = \\prod_j q(b_j) \\\\\nq(b_j) = N(\\mu_j, \\sigma^2_j)\n\\end{aligned}\n\\]\n\n\nComputing moment under \\(q\\)\n\\[\n\\mathbb E \\psi_j = \\mu_j x_j\n\\]\n\\[\n\\mathbb E \\psi_j^2 = (\\sigma^2_j + \\mu^2_j) x^2_j\n\\]\n\\[\n\\bar \\psi := \\mathbb E_q[\\psi] = \\sum \\mathbb E_q[\\psi_j]\n\\]\n\\[\n\\bar{\\psi^2} = \\mathbb E_q[\\psi^2] =  \\mathbb E_q[ \\left(\\sum \\psi_j \\right)^2] = Var(\\psi) + \\bar \\psi\n\\]\n\n\nCode\n#' write gaussian log density as polynomial \n#' return a vector c representing polynomial c[3]x^2 + c[2] x + c[1]\nmake_gaussian_coef <- function(y, var=1){\n  c(- 0.5 * y**2/var, y/var, -0.5 / var)\n}\n\n\n\n\nCode\n#' q a p-list of distributions for effect size of each column\n#' X a n x p matrix of covariate\n#' returns a list(mu, mu2) with the means and second moments\ncompute_moments <- function(X, q){\n  p <- ncol(X)\n  mu <- 0\n  var <- 0\n  for (j in 1:p){\n    mu <- mu + (X[,j] * q[[j]]$mu)\n    var <- var + (X[,j]**2 * q[[j]]$var)\n  }\n  mu2 <- var + mu**2\n  return(list(mu=mu, mu2=mu2))\n}\n\n#' compute coeeficient for EXPECTED shift\n#' f2(x) = E(f(x + c))\nshift_polynomial2 <- function(coef, mu, mu2){\n  c <- list(1, mu, mu2)\n  # construct map\n  K <- length(coef) - 1\n  M <- matrix(nrow= K+1, ncol=K+1)\n  for(j in 0:K){\n    for(k in 0:K){\n      M[j+1, k+1] <- choose(k, j) * c[[max(k-j+1, 1)]]\n    }\n  }\n  coef_new <- (M %*% coef)[, 1]\n  return(coef_new)\n}\n\n\n\n\nCode\nsublist <- function(list, j){\n  list[[j]] <- NULL\n  return(list)\n}\n\npolynomial_update2 <- function(m, X, prior_p, q){\n  p <- length(q)\n  n <- nrow(X)\n  for(j in 1:p){\n    # compute moments\n    # right now we just compute two moments, but need higher moments for \n    # higher degree polynomials\n    moments <- compute_moments(X[, -j, drop=F], sublist(q, j))\n    \n    # shift-- new polynomial in terms of \\psi_j\n    m2_tilde <- do.call(rbind, lapply(1:n, function(i) shift_polynomial2(\n      m[i,], moments$mu[i], moments$mu2[i])))\n    \n    # scale-- new polynomial in terms of b_j\n    m2_hat <- do.call(rbind, lapply(1:n, function(i) scale_polynomial(m2_tilde[i,], X[i, j])))\n    \n    # compute posterior polynomial\n    m2_post <- colSums(m2_hat) + prior_p[[j]]\n    \n    # find gaussian approximation\n    q[[j]] <- poly_to_gaussian(m2_post)\n  }\n  \n  return(q)\n}\n\n\n\n\nExample\n\n\nCode\nsimulate <- function(n=500, p=2, lenghtscale = 0.8, prior_variance=5){\n  Z <- matrix(rnorm(n*p), nrow=n)\n  K <- exp(-(outer(1:p, 1:p, '-')/lenghtscale)**2)\n  X <- Z %*% K\n  \n  b <- rnorm(p) * sqrt(prior_variance)\n  y <- (X %*% b)[, 1] + rnorm(n)\n  \n  return(list(y=y, X=X, b=b))\n}\n\nsim <- simulate(p=3)\ny <- sim$y\nX <- sim$X\nb <- sim$b\nprint(b)\n\n\n[1]  1.900755 -2.069063  1.998108\n\n\n\n\nCode\n# observations in polynomial coeeficients\nm <- do.call(rbind, lapply(y, make_gaussian_coef))\np <- ncol(X)\n\n# initialize prior and q\nq <- list()\nprior_p <- list()\nfor(j in 1:p){\n  prior_p[[j]] <- c(0, 0, -0.5)\n  q[[j]] <- list(mu = 0, var=1)\n}\n\n# iteratively update\nparam_history <- list()\nparam_history[[1]] <- q\nfor(i in 1:50){\n  q <- polynomial_update2(m, X, prior_p, q)\n  param_history[[i+1]] <- q\n}\n\nmu_post <- purrr::map_dbl(q, ~purrr::pluck(.x, 'mu'))\nvar_post <- purrr::map_dbl(q, ~purrr::pluck(.x, 'var'))\n\n\nHere we simulate a gaussian linear mode with three covariates. We plot the simulated effects against their posteror mean. It looks like we are able to recover the effects. Nice!\n\n\nCode\nplot_effect_posterior(q, sim$b)\n\n\n\n\n\n\n\nComparison to usual Bayesian computation\nTODO"
  },
  {
    "objectID": "research/polynomial_regression_vb/index.html#logistic-regression",
    "href": "research/polynomial_regression_vb/index.html#logistic-regression",
    "title": "Implementing polynomial approximation VB",
    "section": "Logistic regression",
    "text": "Logistic regression\nNext we will take a quadratic approximation for logistic regression. We will also test higher degree polynomial approximations. In all cases we continue to use a Gaussian mean-field variational approximation. Computing the moments of the higher degree polynomials may be tricky, so we defer for now. However, if it can be done easily we may benefit from the richer varitional approximation (note: the Gaussian distribution is a degree 2 polynomial exponential family since it’s sufficient statistics are \\(T(x) = [x, x^2]\\))\n\nPolynomial approximation to log likelihood\nFor each data point we want to approximate the log likelihood as a function of the linear predictor\n\\[\n\\log p(y | \\psi) = y\\psi + \\log \\sigma(-\\psi)\n\\]\n\n\nCode\n# loglike functions for y=1 and y=0\nloglik1 <- function(psi){\n  psi + log(sigmoid(-psi))\n}\nloglik0 <- function(psi){\n  log(sigmoid(-psi))\n}\n\n# polynomial approximation via pracma\nR <- 3\nll0_p2 <- rev(pracma::polyApprox(loglik0, -R, R, n = 2)$p)\nll0_p4 <- rev(pracma::polyApprox(loglik0, -R, R, n = 4)$p)\nll0_p6 <- rev(pracma::polyApprox(loglik0, -R, R, n = 6)$p)\nll0_p8 <- rev(pracma::polyApprox(loglik0, -R, R, n = 8)$p)\n\n# note: ll0 and ll1 are just reflections over the x axis\n# so we can get ll1 by taking ll0_p2 and flipping the sign of the linear term\nll1_p2 <- rev(pracma::polyApprox(loglik1, -R, R, n = 2)$p)\nll1_p4 <- rev(pracma::polyApprox(loglik1, -R, R, n = 4)$p)\nll1_p6 <- rev(pracma::polyApprox(loglik1, -R, R, n = 6)$p)\nll1_p8 <- rev(pracma::polyApprox(loglik1, -R, R, n = 8)$p)\n\n\nHere we plot the \\(\\log p(y=0 | \\psi)\\) and it’s polynomial approximations of degree \\(k=2,4,6,8\\).\nThese polynomial approximations are generated using pracma::polyApprox which use the Chebyshev coefficients of an appropriately rescaled version of the function, to generate a polynomial approximation on the interval \\([a, b]\\). We generate approximations on the interval \\([-R, R]\\) where \\(R = 3\\).\n\n\nCode\nS <- R + 2\nx <- seq(-S, S, by=0.1)\nplot(loglik0, -S, S)\nlines(x, polyval2(ll0_p2, x), col='red', lty='dotted')\nlines(x, polyval2(ll0_p4, x), col='blue', lty='dotted')\nlines(x, polyval2(ll0_p6, x), col='green', lty='dotted')\nlines(x, polyval2(ll0_p8, x), col='orange', lty='dotted')\nabline(v=R); abline(v=-R)\n\n\n\n\n\n\n\nApproximate date likelihood\nThis just takes a whoe list of vector \\(y\\) and returns a matrix of coefficients\n\n\nCode\n#' get approximate polynomial representation of the data y\nbernoulli_poly_approx <- function(y, R, k){\n  n <- length(y)\n  p0 <- make_approximation(loglik0, R, k)\n  \n  # for y=1 flip the sign of odd coefficients (note: 0 indexing)\n  p1 <- p0\n  p1[seq(2, length(p0), by=2)] <- p1[seq(2, length(p0), by=2)] * -1\n  \n  m <- matrix(nrow = n, ncol = k + 1)\n  for(i in 1:length(y)){\n    if(y[i] == 0){\n      m[i,] <- p0\n    } else{\n      m[i,] <- p1\n    }\n  }\n  return(m)\n}\n\n\n\n\nSimulate logistic regression\n\n\nCode\nsimulate_lr <- function(n=500, p=2, lenghtscale = 0.8, prior_variance=5){\n  Z <- matrix(rnorm(n*p), nrow=n)\n  K <- exp(-(outer(1:p, 1:p, '-')/lenghtscale)**2)\n  X <- Z %*% K\n  \n  b <- rnorm(p) * sqrt(prior_variance)\n  logits <- (X %*% b)[, 1]\n  y <- rbinom(length(logits), 1, sigmoid(logits))\n  return(list(y=y, X=X, b=b, logits=logits))\n}\n\n\n\n\nApproximation with \\(k=2\\)\nWe can reuse our code above, substituting in new “data”, the coefficients for the polynomial approximation to the conditional likelihood.\n\n\nCode\nlogistic_polynomial_approximation_k2 <- function(y, X, R){\n  # observations in polynomial coeeficients\n  m <- bernoulli_poly_approx(y, R, k=2)\n  p <- ncol(X)\n  \n  # initialize prior and q\n  q <- list()\n  prior_p <- list()\n  for(j in 1:p){\n    prior_p[[j]] <- c(0, 0, -0.5)\n    q[[j]] <- list(mu = 0, var=1)\n  }\n  \n  # iteratively update\n  param_history <- list()\n  param_history[[1]] <- q\n  for(i in 1:50){\n    q <- polynomial_update2(m, X, prior_p, q)\n    param_history[[i+1]] <- q\n  }\n  return(param_history)\n}\n\n\nA 2d approximation does not seem to perform very well here.\n\n\nCode\nn <- 500\np <- 3\nlenghtscale <- 0.8\nprior_variance <- 5\n\nZ <- matrix(rnorm(n*p), nrow=n)\nK <- exp(-(outer(1:p, 1:p, '-')/lenghtscale)**2)\nX <- Z %*% K\n\nb <- rnorm(p) * sqrt(prior_variance)\nlogits <- (X %*% b)[, 1]\ny <- rbinom(length(logits), 1, sigmoid(logits))\nprint(b)\n\n\n[1] -1.2803961  0.6987738 -1.5818567\n\n\nCode\nqs <- logistic_polynomial_approximation_k2(y, X, R=5)\nq <- tail(qs, 1)[[1]]\nmu_post <- purrr::map_dbl(q, ~purrr::pluck(.x, 'mu'))\nvar_post <- purrr::map_dbl(q, ~purrr::pluck(.x, 'var'))\n\nplotrix::plotCI(\n  x = b,\n  y = mu_post,\n  li =  mu_post - 2 * sqrt(var_post),\n  ui =  mu_post + 2 * sqrt(var_post)\n)\nabline(0, 1, col='red')\n\n\n\n\n\n\nTesting a range of interval widths\nWe can check a few values of \\(R\\). There is a tradeoff here of course– the wider the interval we try to approximate, the worse the approximation will be. But if the interval is too narrow, the polynomial approximate likelihood essentially does not support data that fall far outside the interval. This is because we require the highest odd degree coefficient of our polynomial to be \\(<0\\) otherwise the likelihood grows unbounded outside the interval, and the approximation is not integrable.\n\n\n\n\n\nCode\npar(mfrow = c(1, 3))\nplot_effect_posterior(q_R3, b, main='R=3')\nplot_effect_posterior(q_R5, b, main='R=5')\nplot_effect_posterior(q_R7, b, main='R=7')\n\n\n\n\n\n\n\n\nHigher degree approximations\nNow we will extend our implementation above to handle higher degree approximations. This involves computing higher moments of the effect predictions, which isn’t too hard for Gaussian distributions.\n\n\nCode\n#' Make shift matrix\n#' \n#' Generate matrix that maps coefficients of a polynomial\n#' f(x + y) (represented by coefficients p) to coefficients of\n#' f2(x) = E_{p(y)}[f(x+y)]\n#' @param moments moments of y\nmake_shift_matrix <- function(moments){\n  # construct map\n  K <- length(moments) - 1\n  M <- matrix(nrow= K+1, ncol=K+1)\n  for(j in 0:K){\n    for(k in 0:K){\n      M[j+1, k+1] <- choose(k, j) * moments[[max(k-j+1, 1)]]\n    }\n  }\n  return(M)\n}\n\n#' Transform coefficients of a polynomial f(x + y) (represented by coefficients p)\n#' to coefficients of f2(x) = E_{p(y)}[f(x+y)]\n#' @param p K+1 coefficients of a degree-k polynomial\n#' @param moments moments of y (including E[y^0] = 1)\nshift_polynomial3 <- function(p, moments){\n  M <- make_shift_matrix(moments)\n  p_new <- (M %*% p)[, 1]\n  return(p_new)\n}\n\ncompute_normal_moments <- function(mu, var, k){\n  return(purrr::map_dbl(0:k, ~actuar::mnorm(.x, mu, sqrt(var))))\n}\n\n#' compute k moments for psi = xb, b ~ N(mu, var)\ncompute_psi_moments <- function(x, mu, var, k){\n  normal_moments <- compute_normal_moments(mu, var, k)\n  psi_moments <- do.call(cbind, purrr::map(0:k, ~ (x**.x) * normal_moments[.x + 1]))\n}\n\n#' update q with polynomial approximation of arbitrary degree\npolynomial_update3 <- function(m, X, prior_p, q){\n  K <- ncol(m) - 1\n  p <- ncol(X)\n  n <- nrow(X)\n  for(j in 1:p){\n    m_tilde <- m\n    for(k in (1:p)[-j]){\n      moments <- compute_psi_moments(X[, k], q[[k]]$mu, q[[k]]$var, K)\n      m_tilde <- do.call(rbind, lapply(1:n, function(i) shift_polynomial3(\n        m_tilde[i,], moments[i,])))\n    }\n    \n    # scale-- new polynomial in terms of b_j\n    m_hat <- do.call(rbind, lapply(1:n, function(i) scale_polynomial(\n      m_tilde[i,], X[i, j])))\n    \n    # compute posterior polynomial\n    m_post <- colSums(m_hat) + prior_p[[j]]\n    \n    # find gaussian approximation\n    q[[j]] <- poly_to_gaussian(m_post)\n    q[[j]]$m_post <- m_post\n  }\n  \n  return(q)\n}\n\nlogistic_polynomial_approximation <- function(y, X, R, K=2){\n  # observations in polynomial coeeficients\n  m <- bernoulli_poly_approx(y, R, K)\n  q <- list()\n  prior_p <- list()\n  for(j in 1:p){\n    prior_p[[j]] <- c(c(0, 0, -0.5), rep(0, K-2)) # extend polynomial to agree with m\n    q[[j]] <- list(mu = 0, var=1) # initialize normal posterior\n  }\n  \n  # iteratively update\n  param_history <- list()\n  param_history[[1]] <- q\n  for(i in 1:50){\n    q <- polynomial_update3(m, X, prior_p, q)\n    param_history[[i+1]] <- q\n  }\n  return(param_history)\n}\n\n\n\n\nCompare implimentations\nThe two implimentations agree!\n\n\nCode\nsim <- simulate_lr(p=3)\ny <- sim$y\nX <- sim$X\nb <- sim$b\n\nqs_2 <- logistic_polynomial_approximation_k2(y, X, R=10)\nqs_k2 <- logistic_polynomial_approximation(y, X, R=10, K=2)\n\npar(mfrow=c(1,2))\nplot_effect_posterior(qs_2[[51]], b, main='A')\nplot_effect_posterior(qs_k2[[51]], b, main='B')\n\n\n\n\n\n\nVary degree of approximation\nWe approximate the likelihood on \\([-10, 10]\\) with \\(K=2,6,10,14\\). Note the polynomial approximation cannot be e.g. degree \\(4\\) because these polynomials are unbounded above (\\(c_4 >0\\)), so \\(e^ \\hat f(x)\\) is not integrable over the real line. But \\(c_K < 0\\) for approximations of degree \\(K = 2z + 2\\) for \\(z \\in \\mathbb N\\).\n\n\nCode\nqs_k2 <- logistic_polynomial_approximation(y, X, R=10, K=2)\nqs_k6 <- logistic_polynomial_approximation(y, X, R=10, K=6)\nqs_k10 <- logistic_polynomial_approximation(y, X, R=10, K=10)\n#qs_k14 <- logistic_polynomial_approximation(y, X, R=10, K=14)\n\npurrr::map_dbl(qs_k2[[51]], ~purrr::pluck(.x, 'mu'))\n\n\n[1] -1.315997 -2.000712 -1.991923\n\n\nCode\npurrr::map_dbl(qs_k6[[51]], ~purrr::pluck(.x, 'mu'))\n\n\n[1] -1.274430 -1.811244 -1.693130\n\n\nCode\npurrr::map_dbl(qs_k10[[51]], ~purrr::pluck(.x, 'mu'))\n\n\n[1] -1.202289 -1.793740 -1.652252\n\n\nCode\n#purrr::map_dbl(qs_k14[[51]], ~purrr::pluck(.x, 'mu'))\n\n\n\n\nCode\npar(mfrow=c(2,2))\nplot_effect_posterior(qs_k2[[51]], b, main='K=2')\nplot_effect_posterior(qs_k6[[51]], b, main='K=6')\nplot_effect_posterior(qs_k10[[51]], b, main='K=10')\n#plot_effect_posterior(qs_k14[[51]], b, main='K=14')"
  },
  {
    "objectID": "research/polynomial_regression_vb/index.html#poisson-regression-example",
    "href": "research/polynomial_regression_vb/index.html#poisson-regression-example",
    "title": "Implementing polynomial approximation VB",
    "section": "Poisson regression example",
    "text": "Poisson regression example\n\nImpliment\n\n\nCode\npoisson_ll <- function(y){\n  f <- function(psi) dpois(y, exp(psi), log = T)\n  return(f)\n}\n\npoisson_approx <- function(y, R, k, as_function=F){\n  f <- poisson_ll(y)\n  p <- make_approximation(f, R, k, plot = F)\n\n  if(as_function){\n    p2 <- function(x) polyval2(p, x)\n    return(p2)\n  }\n  return(p)\n}\n\npoisson_poly_approx <- function(y, R, k){\n  unique_counts <- unique(y)\n  \n  polynomials <- list()\n  for(yy in unique_counts){\n    polynomials[[yy + 1]] <- poisson_approx(yy, R, k)\n  }\n  \n  m <- do.call(rbind, purrr::map(y, ~polynomials[[.x + 1]]))\n  return(m)\n}\n\npoisson_regression_polynomial_approximation <- function(y, X, R, K=2){\n  # observations in polynomial coeeficients\n  tic()\n  m <- poisson_poly_approx(y, R, K)\n  p <- ncol(X)\n  q <- list()\n  prior_p <- list()\n  for(j in 1:p){\n    prior_p[[j]] <- c(c(0, 0, -0.5), rep(0, K-2)) # extend polynomial to agree with m\n    q[[j]] <- list(mu = 0, var=1) # initialize normal posterior\n  }\n  \n  # iteratively update\n  param_history <- list()\n  param_history[[1]] <- q\n  for(i in 1:50){\n    q <- polynomial_update3(m, X, prior_p, q)\n    param_history[[i+1]] <- q\n  }\n  toc()\n  return(param_history)\n}\n\n\n\n\nVisualize approximation\n\n\nCode\nf <- poisson_ll(3)\np <- make_approximation(f, 10, 12, plot=T)\n\n\n\n\n\nCode\np <- poisson_approx(3, 5, 12, T)\n\n\n\n\nSimulate poisson regression\n\n\nCode\nsimulate_poisson_regression <- function(n=500, p=2, lenghtscale = 0.8, prior_variance=1){\n  Z <- matrix(rnorm(n*p), nrow=n)\n  K <- exp(-(outer(1:p, 1:p, '-')/lenghtscale)**2)\n  X <- Z %*% K\n  \n  b <- rnorm(p) * sqrt(prior_variance)\n  logrates <- (X %*% b)[, 1]\n  y <- rpois(length(logrates), exp(logrates))\n  return(list(y=y, X=X, b=b, logrates=logrates))\n}\n\nsim <- simulate_poisson_regression(p=4, prior_variance = 1)\ny <- sim$y\nX <- sim$X\nb <- sim$b\nprint(b)\n\n\n[1] -1.3296046  0.6149598  0.7740477  0.2643521\n\n\n\n\nExample\n\n\nCode\npois_R5_K2 <- poisson_regression_polynomial_approximation(y, X, R=5, K=2)\n\n\n6.131 sec elapsed\n\n\nCode\npois_R5_K4 <- poisson_regression_polynomial_approximation(y, X, R=5, K=4)\n\n\n9.477 sec elapsed\n\n\nCode\n#pois_R5_K8 <- poisson_regression_polynomial_approximation(y, X, R=5, K=8)\n#pois_R5_K16 <- poisson_regression_polynomial_approximation(y, X, R=5, K=16)\n\n\n\n\nCode\npar(mfrow=c(2,2))\nplot_effect_posterior(pois_R5_K2[[51]], b, main='K=2')\nplot_effect_posterior(pois_R5_K4[[51]], b, main='K=4')\n#plot_effect_posterior(pois_R5_K8[[51]], b, main='K=8')\n#plot_effect_posterior(pois_R5_K16[[51]], b, main='K=16')\n\n\n\n\n\n\n\nCode\nplot_effect_posterior(pois_R5_K2[[51]], sim$b)\n\n\n\n\n\nCode\npurrr::map_dbl(pois_R5_K2[[51]], ~purrr::pluck(.x, 'mu'))\n\n\n[1] -1.51427519  0.50936054  0.92844494  0.08298042"
  },
  {
    "objectID": "research/polynomial_regression_vb/index.html#higher-degree-polynomial-posterior-q",
    "href": "research/polynomial_regression_vb/index.html#higher-degree-polynomial-posterior-q",
    "title": "Implementing polynomial approximation VB",
    "section": "Higher degree polynomial posterior \\(q\\)",
    "text": "Higher degree polynomial posterior \\(q\\)\nIn the above implementation we compute a polynomial proportional to the posterior density, but reduce this to a Gaussian approximation by taking a Laplace approximation.\nThat is we want to compute\n\\[\n\\mathbb E [b^j] = \\int_{\\mathbb R} b^j q(b)\n\\]\nWhich we approximate by\n\\[\n\\mathbb E [b^j] \\approx \\int_{\\mathbb R} b^j q_{\\text{gauss}}(b)\n\\]\nWhere \\(q_{\\text{gauss}}\\) is the Gaussian distribution that minimizes the divergence to \\(q_{\\text{gauss}} = \\arg \\min_{q_g} KL(q_g ||q)\\).\nIt would be better if we could compute the moments of \\(q(b_l) \\propto \\exp\\{\\sum_k \\eta_k b_l^k\\}\\). We define the log normalizing constant \\(A({\\bf \\eta}) = \\log \\int \\exp\\{\\sum_{k=0}^K \\eta_k b_l^k\\} db\\). Let \\(f(b) = \\sum_{k=0}^K \\eta_k b^k - A(\\eta)\\) so that \\(q(b) = \\exp\\{f(b)}\\).\nBecause \\(q\\) is in an exponential family, we know that \\(\\nabla_{\\eta} A(\\eta) = \\mathbb E[T(x)] = [1, \\mathbb E[b], \\mathbb E[b^2], \\dots, \\mathbb E[b^K]]\\). Is there an easy way to compute the gradient of \\(A\\)?\n\nQuick note on Laplace approximation\nThen we can compute the first and second derivatives \\(f'(b) = \\sum_{k=1}^K \\eta_k kb^{k-1}\\) and \\(f''(b) = \\sum_{k=2}^K \\eta_k k(k-1)b^{k-2}\\)\nNext, we can form a quadratic approximation to \\(f\\) around its maximum, \\(x_0\\) (so that \\(f'_0 = 0\\))\n\\[\n\\begin{aligned}\n\\hat f(x)\n&= f_0  + \\frac{1}{2}f''_0(x-x_0)^2 \\\\\n&= \\frac{1}{2}f''_0 x^2 + (- f''_0x_0) x + (\\frac{1}{2}f''_0x_0^2 + f_0)\n\\end{aligned}\n\\]\nIdentifying \\(a = \\frac{1}{2}|f''_0|\\), \\(b = - f''_0x_0\\) and \\(c = \\frac{1}{2}f''_0x_0^2 + f_0\\) we can evaluate the integral\n\\[\n\\int e^{-ax^2 + bx + c} dx = \\sqrt{\\frac{\\pi}{a}} e^{\\frac{b^2}{4a} + c}.\n\\]\n\n\nCode\nm_post <- qs_k10[[51]][[1]]$m_post\n\npoints <- seq(-3, 3, by=0.1)\nplot(points, polyval2(m_post, points), type = 'line')\n\n\nWarning in plot.xy(xy, type, ...): plot type 'line' will be truncated to first\ncharacter\n\n\n\n\n\nCode\nexp_p_laplace <- function(p, k=0){\n  f <- function(x){polyval2(m_post, x)}\n  x0 <- optimize(f, interval = c(-R, R), maximum = T)$maximum\n  \n  d <- rev(pracma::polyder(rev(m_post)))\n  d2 <- rev(pracma::polyder(rev(d)))\n  \n  f_x0 <- f(x0)\n  d2_x0 <- polyval2(d2, x0)\n  \n  # coefficients\n  a <- 0.5 * abs(d2_x0)\n  b <- - d2_x0 * x0\n  c <- 0.5 * d2_x0 * x0**2\n  \n  # gaussian integral\n  exp(b^2/(4*a) + c + 0.5 * log(pi / a))\n}"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research Notes",
    "section": "",
    "text": "Implementing polynomial approximation VB\n\n\n\n\n\nRough implimentation of of approximate VB regression, where we approximate the data likelihood with a polynomial. We implement a mean field gaussian variational approximation. We demonstrate the method on Gaussian linear model (no approximation), and Bernoulli-logit (logistic regression) and Poisson-log (poisson regression) models with varying approximation degree.\n\n\n\n\n\n\nMar 15, 2023\n\n\nKarl Tayeb\n\n\n\n\n\n\n\n\nPolynomial approximation for variational Bayes\n\n\n\n\n\nUsing polynomial approximations to perform Bayesian regression\n\n\n\n\n\n\nMar 15, 2023\n\n\nKarl Tayeb\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Karl Tayeb’s Website",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  }
]