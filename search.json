[
  {
    "objectID": "research/polynomial_approximation/index.html",
    "href": "research/polynomial_approximation/index.html",
    "title": "Polynomial approximation for variational Bayes",
    "section": "",
    "text": "$$"
  },
  {
    "objectID": "research/polynomial_approximation/index.html#overview",
    "href": "research/polynomial_approximation/index.html#overview",
    "title": "Polynomial approximation for variational Bayes",
    "section": "Overview",
    "text": "Overview\nWe propose approximating Bayesian linear regression problems by replacing the conditional likelihood of each observation with a polynomial approximation designed to be close to the true likelihood on an interval, and a lower bound for values far outside the interval. If the log density of our prior distribution can also be expressed (or approximated) as a polynomial the exact posterior of this polynomial approximation is also a polynomial– i.e. conjugate.\nThis is the main benefit of our approach: inference boils down to simple manipulations of the polynomial coefficients, which can be performed in parallel and with a set of simple linear algebra operations (matrix multiplication, solving a triangular system, etc). There may exist more computationally efficient algorithms for carrying out these operations, which we should explore further.\nThis approach can be generalized to many likelihood (e.g. Binomial, Poisson) with various choices of link function (which relate the regression predictions to the distribution parameters)– at least to the extent that we can develop a good polynomial approximation for each combination of likelihood and link.\nTake a look at this rough implementation."
  },
  {
    "objectID": "research/polynomial_approximation/index.html#global-polynomial-approximations",
    "href": "research/polynomial_approximation/index.html#global-polynomial-approximations",
    "title": "Polynomial approximation for variational Bayes",
    "section": "Global polynomial approximations",
    "text": "Global polynomial approximations\n\nLocal approximations, and trouble with SuSiE\nThis approach was motivated by our work to develop a good variational approximation for logistic SuSiE, that is a logistic regression with the sum of single effects (SuSiE) [(susie?)]. For Gaussian linear models, using a variational approximation that factorizes over single effects produces a lightning fast Bayesian variable selection method. Rather than exploring each configuration of non-zero effect variables, this approximation allows us to update our approximating distribution for one effect while marginalizing over all other effects. Inference in the (Gaussian, linear) SuSiE model reduces to a sequence of embarrassingly parallel univariate linear regression problems.\nIn the case of Gaussian model with identify link (linear regression), the variational approximation is sufficient for computation to simplify, because the log likelihood is quadratic in the regression coefficients \\(\\beta\\). However, the Bernoulli likelihood with a logistic link function (logistic regression), and indeed for many other likelihood-link combinations of interest, do not enjoy the algebraic simplicity of a Gaussian model with identity link (linear regression). The main issue is that it is difficult to marginalize over the other effects. Even if the approximate posterior factorizes, the likelihood computation can combine single effects in a way that complicates the marginalization step.\nWe’ve attempted using local quadratic approximations to the likelihood. The idea is to construct a quadratic lower bound to the likelihood that is “good” in a neighborhood that we care about (contains most of the posterior mass). This is a popular technique for applying variational Bayes to logistic regression (see Jaakkola and Jordan, Polya-Gamma augmentation), and indeed these sorts of “local” approximation schemes can be generalized to super-Gaussian likelihoods [(Galy-Fajou, Wenzel, and Opper 2020)].\nHowever, we find that the local approximation techniques are not well suited to variable selection techniques like SuSiE. In order to use the local approximation techniques, we must optimize a set of variational parameters that essentially dictate where the approximation is good. You only get to select one local approximation per observation. Holding these variational parameters fixed, our inference will systematically favor posterior distributions that place more mass in a neighborhood of these variational parameters. The strong coupling between the local variational parameters and optimal variational approximation make it difficult to navigate the optimization surface via coordinate ascent.\nAdditionally, uncertainty quantification is an important output to SuSiE and other Bayesian variable selection methods. Due to the nature of the local variational approximations we may tend to see a “winner take all” scenario. Assuming we find a good local optimum, the local variational parameters will be tuned to be consistent with the most likely variable configurations. In turn, the objective function will be a tighter lower bound in these regions, and the approximating distribution will place more probability mass on these configurations.\nNote: the Laplace approximation is also a “local” approximation, since we construct a quadratic approximation of the joint log likelihood around the MAP estimate. It is also closely related Gaussian variational approximations.\n\n\nGlobal approximations\nRather than find a “local” approximation which has variational parameters that must be tuned at each iteration, we propose finding a global approximation to the likelihood which is easier to work with.\n“Easier to work with”, in the context of SuSiE means that we can write \\(f(\\psi_l) =\\mathbb E_{q_{-l}}[\\log p(y | \\psi_l + \\psi_{-l})]\\), where \\(f(\\psi_l; q_{-l})\\) is easy to evaluate/optimize over. Notice that if we approximate \\(\\log p(y | \\psi) \\approx f(\\psi) = \\sum_k c_k \\psi^k\\) then taking the expectation of \\(f\\) over \\(q_{-l}\\) results in a polynomial in \\(\\psi_l\\).\n\\[\nE_{q_{-l}}[f(\\psi_l + \\psi_{-l})] = \\hat f(\\psi_l; q_{-l}) = \\sum \\hat c_k(q_{-l}) \\psi_l^k.\n\\]\nHere \\(\\hat c_k(q_{-l})\\) are a transformation of the original coefficients \\(c_k\\) that are achieved by (1) expanding the original polynomial in terms of \\(\\psi_l\\) and \\(\\psi_{-l}\\), (2) marginalizing over \\(\\psi_{-l}\\) w.r.t \\(q_{-l}\\) and then (3) regrouping terms to get coefficients of a polynomial in \\(\\psi_l\\).\nWhile the local variational approximations are only good around a point specified by the variational parameter, we can construct a “global” polynomial approximation that is good on any interval if we allow the degree of the polynomial to be sufficiently high. While we sidestep the issue of needing to tune the local variational parameters, we replace it with the need to select an interval that we care to approximate. There is a trade off here– for a fixed error tolerance, wider intervals will require higher degree approximations to bound the error."
  },
  {
    "objectID": "research/polynomial_approximation/index.html#polynomial-representation",
    "href": "research/polynomial_approximation/index.html#polynomial-representation",
    "title": "Polynomial approximation for variational Bayes",
    "section": "Polynomial representation",
    "text": "Polynomial representation\n\nRepresenting functions with polynomial series\nLet \\(\\set{P_k}_{k=1}^{\\infty}\\) be a sequence of polynomials that form a basis for continuous functions on an interval, e.g. \\([-1, 1]\\) (note: that through a change of variable we can stretch this interval to any interval we want). So that for any \\(f: [-1,1] \\\\rightarrow \\mathbb R\\) there are \\(\\set{c_k}\\) such that\nWe’ll assume that \\(P_k\\) are ordered by increasing degree, and that \\(deg(P_k) \\leq k\\).\n\n\nChebyshev polynomials\nThese are a family of polynomials on \\([-1, 1]\\) defined by\n\\[T_n(\\cos(\\theta)) = \\cos(n \\theta)\\]\nThey can also be obtained by the reccurrence\n\\[\n\\begin{aligned}\n  T_0(x) &= 1 \\\\\n  T_1(x) &= x \\\\\n  T_{n+1}(x) &= 2x T_n(x) - T_{n-1}(x)\n\\end{aligned}\n\\] The Chebyshev polynomials are orthogonal to eachother, and form a basis for ~a certain family of function~ on the interval \\([-1, 1]\\), so that \\(f(x) = \\sum_{k=0}^\\infty c_k T_k(x)\\). This means that we can get the coefficients by evaluating an inner product\n\\[\n\\langle f, T_k \\rangle = c_k ||T_k||^2\n\\] Through a change of variable, we can approximate functions on any interval. We can obtain a \\(K+1\\) degree polynomial approximation by computing the first \\(K\\) coefficients \\((c_k)_{k=0}^K\\). Each coefficient \\(c_k\\) is effectively computed via a \\(k+1\\) point quadrature (Note: it looks like there is a simple rule for computing the coefficients as linear combinations of \\(f\\) evauated at a set of “nodes”, this looks very closely related to the quadrature rule, but I want to figure this out in more detail)\n\n\nConversion between polynomial basis\nSuppose we have a polynomial basis \\(\\{A_k\\}\\) and \\(\\{B_k\\}\\). Let \\(M_k(x) = x^k\\), \\(\\{M_k\\}\\) is the “monomial basis”, so that we can write \\(A_k(x) = \\sum_{j=0}^k \\alpha_{kj} M_j(x)\\). We can arrange these coefficients for the first \\(K\\) polynomials into the upper triangular matrix:\n\\[\nA^{(k)} =\n\\begin{bmatrix}\n\\alpha_{00} & 0 & 0 &\\dots & 0 \\\\\\\\\n\\alpha_{10} & \\alpha_{11} & 0 & \\dots & 0 \\\\\\\\\n\\alpha_{20} & \\alpha_{21} & \\alpha_{22} &\\dots & 0 \\\\\\\\\n\\dots\\\\\\\\\n\\alpha_{K0} & \\alpha_{K1} & \\alpha_{K2} &\\dots & \\alpha_{KK}\n\\end{bmatrix}\n^T\n\\]\nNow we can see that for \\(f(x) = \\sum_{k=0}^K a_k A_k(x)\\). We can take the vector of coefficients \\(\\vec a = (a_0, \\dots, a_K)\\) and convert back to coefficients in the monomial basis with a simple matrix multiplication\nTo convert from the monomial basis to the basis \\(A\\) invovles solving the triangular system\nTo convert from the monomial basis to the basis \\(A\\) to the basis \\(B\\) involves (1) expanding to the monomial basis and (2) solving for the coefficients in basis \\(B\\).\nApparently there are \\(O(K\\log(K))\\) algorithms for changing basis, it may be worth understanding these. But it’s very easy to see how we can move between different bases for polynomials of degree \\(K\\) by matrix multiplication.\n\n\nShifting and scaling\nWe can “shift” the polynomial basis too, that is we rewrite \\(f(x + y) = g(x; y)\\) there \\(g\\) is a polynomial in \\(x\\). This has the effect of moving fixed information \\(y\\) out of the functions argument and into the polynomial coefficients.\nHere \\(\\tilde M_k(x_1; x_2)\\) is a \\(k\\) degree polynomial in \\(x_1\\). \\(x_2\\) is treated as a parameter that is absorbed into the coefficients. Next we’d like to do the same for general polynomials. So we’d like to find \\(\\tilde a\\) such that \\(f(x_1 + x_2) = \\sum a_k A_k(x_1 + x_2) = \\sum_k \\tilde a(x_2) A_k(x_1)\\).\nLet \\(m_{kj} = 0 \\; \\forall k < j\\), and \\(M(x_2)\\) be the upper triangular matrix \\(M(x_2) = \\begin{bmatrix} m_{kj}(x_2)\\end{bmatrix}_{k=0, \\dots K,\\\\; j=0,\\dots,K}\\)\nThen we can find the coordinates\n\\[\n\\tilde a(x_2) = (A^{(k)})^{-1}M(x_2)A^{(k)} a\n\\]\nand\n\\[\n\\mathbb E_{ q(x_2) } \\left[  \\tilde a(x_2)  \\right] =  (A^{(k)})^{-1}\\mathbb E_{ q(x_2) } \\left[  M(x_2)  \\right]A^{(k)} a\n\\]\n\n\nThings to look into\nClenshaw algorithm and Horner’s method. these are recursive methods for evaluating polynomials in the Chebyshev and monomial basis respectively.\nHamming and Salzer develop algorithms for converting polynomials between different basis representations.\nWe may not be able to use these techniques, unless we can get an expression for each coefficient, because we need to evaluate the expected value of the terms."
  },
  {
    "objectID": "research/polynomial_approximation/index.html#variational-approximation",
    "href": "research/polynomial_approximation/index.html#variational-approximation",
    "title": "Polynomial approximation for variational Bayes",
    "section": "Variational approximation",
    "text": "Variational approximation\nHere we present the variational approximation for SuSiE and explore how to perform coordinate ascent variational inference with this approximation and our polynomial likelihoods.\n\nEvidence lower bound, and a polynomial approximation\nWe can write the log likelihood for a single observation as a function \\(y_i\\) of a function of the linear predictor \\(\\psi_i = \\sum_{l} \\psi_{li}\\), then we will approximate that likelihood with a polynomial in \\(\\psi_i\\)\n\\[\n\\log p (y_i | \\psi_i) = l_i(\\psi_i) \\approx \\sum_{k=0}^K m_{ik} \\psi_i^k =: \\hat l_i^{(K)}(\\psi_i)\n\\]\nThen we can approximate the ELBO\n\\[\n\\begin{aligned}\nF_1(q) &= \\mathbb E_{ q } \\left[  \\sum l_i(\\psi_i)  \\right]- KL(q || p) \\\\\n       &\\approx \\mathbb E_{ q } \\left[  \\sum \\hat l_i(\\psi_i)  \\right] - KL(q || p) =: F_2(q)\n\\end{aligned}\n\\]\nWe perform inference by selecting \\(q \\in \\mathcal Q\\) to optimize \\(F_2\\), where \\(\\mathcal Q\\) is a family of distributions.\n\\[\nq^* = \\arg\\max_{q \\in \\mathcal Q} F_2(q)\n\\]\nLet \\({\\bf m}_i = (m_{i0}, \\dots, m_{iK})\\) denote the coefficients for \\(\\hat l_i\\).\nTo perform coordinate ascent variational inference we will need to compute \\(\\mathbb E_{ q_{-l} } \\left[  \\hat l(\\psi_l + \\psi_{-l})  \\right]\\). We will write \\(\\tilde l(\\psi_l; \\psi_{-l}) = \\hat l(\\psi_l + \\psi_{-l})\\) to emphasize that we are treating the likelihood as a function of \\(\\psi_l\\), with \\(psi_{-l}\\) fixed. By expanding the \\((\\psi_l + \\psi_{-l})^k\\) terms and collecting coefficients we can write \\(f(\\psi_l; \\psi_{-l}) = \\sum_k \\tilde m_k(\\psi_{-l}) \\psi_l^k\\). Now \\(\\tilde {\\bf m}(\\psi_{-l}) = (m_k(\\psi_{-l}))_{k=0}^K\\) give the coefficients for a polynomial in \\(\\psi_l\\). Now, taking expectations\n\\[\n\\tilde f(\\psi_l; q_{-l}) := \\mathbb E_{ q_{-l} } \\left[  \\hat l(\\psi_l + \\psi_{-l})  \\right]\n= \\mathbb E_{ q_{-l} } \\left[  \\tilde l(\\psi_l; \\psi_{-l})  \\right]\n= \\sum_k \\mathbb E_{ q_{-l} } \\left[  \\tilde m_k(\\psi_{-l})  \\right] \\psi_l^k\n\\] Noting that we can write \\(\\hat l(\\psi) = \\hat l(\\psi_l + \\psi_{-l})\\)\n\n\nSuSiE variational approximation\nFor SuSiE our latent variables consist of the effect sizes \\(\\set{b_l}\\_{l=1}^L\\) and a set of indicators that select \\(L\\) non-zero effect variables \\(\\set{\\gamma_l}\\_{l=1}^L\\). We select \\(\\mathcal Q\\) to be the family of distributions that factorize over the \\(L\\) single effects, that is\n\\[\nq(\\set{b_l}, \\set{\\gamma_l}) = \\prod_l q(b_l | \\gamma_l)q(\\gamma_l).\n\\]\n\n\nCoordinate ascent\nTo update \\(q_l\\) we need to maximize\nDropping the subscript \\(i\\), for each term in the sum we need to compute\nWe can do this by applying the “expected” shift operator. We can do this by computing the moments of \\(\\psi_{-l}\\) and applying the shift operation once, or by computing the moments of \\(\\psi_m, \\;\\; m\\neq l\\) and performing the shift operation sequentially.\n(TODO: update notation here)\n\\[\n\\tilde a(q_{-l})\n= (A^{(k)})^{-1} \\left(\\prod_{m \\neq l} \\mathbb E_{ q_m } \\left[  M(\\psi_m)  \\right] \\right) A^{(k)} a\n\\]\nA nice feature of the sequential approach is it gives us an easy way of converting between polynomial representations. Let \\(\\Gamma_l = \\mathbb E_{ q_l } \\left[  M(\\psi_l)  \\right]\\) be the matrix for applying the “shifted expectation” operation to the polynomial coefficients for \\(f(\\psi_l)\\), \\({\\bf m}\\). That is \\(\\Gamma_l {\\bf m}\\) gives the coefficients of \\(f(\\psi_{-l}; q_l)\\), which is a polynomial in \\(\\psi_{-l}\\).\nLet \\(\\Gamma = \\prod_l \\Gamma_l\\). Notice that the polynomial with coefficients \\(\\Gamma {\\bf m}\\) evalutated at \\(0\\) give \\(\\mathbb E_{ q } \\left[  f(x)  \\right]\\). Furthermore we can quickly move from \\(f(\\psi_{l}; q_{-l})\\) to \\(f(\\psi_{l+1}; q_{-(l+1)})\\).\nStarting with \\(f(\\psi_{1}; q_{-1})\\) we want the coefficients \\(\\Gamma_{-1} {\\bf m}\\), where \\(\\Gamma_{-1} = \\Gamma_1^{-1} \\Gamma\\). Then, to get \\(f(\\psi_{1}; q_{-1})\\) we need \\(\\Gamma_{-2} {\\bf m}\\), where we can compute \\(\\Gamma_{-2}\\) by\n\\[\n\\Gamma_{-2} = \\Gamma_2^{-1} \\Gamma_{-1} \\Gamma_1.\n\\] We can continue iterating over all \\(L\\) single effects. We note that it is easier to compute the moments of the single effects \\(\\psi_m\\) rather than the moments of all “other” single effects \\(\\psi_{-l}\\). Carrying out the iterated expectations as matrix-vector products in the polynomial coefficients seems like an appealing approach to implementation.\nThis is useful in a coordinate ascent update scheme where we can remove one of the single effect from \\(\\Gamma\\) by a triangular system. Update \\(q_l\\), and then add back the update \\(\\Gamma_l\\) to \\(\\Gamma\\) by a right matrix multiplication.\n\n\nUnconstrained variational posterior\nThe optimal variational approximation looks like\n\\[\nq^*\\_l(b_l | \\gamma_l = j) \\propto e^{f(b_l; q_{-l}, {\\bf x}\\_j)}.\n\\] Where \\(f(b_l; q_{-l}, {\\bf x}\\_j) = \\sum_k \\eta_k b_l^k\\) is a polynomial of degree \\(K\\). Notice that this is an exponential family with sufficient statistics \\(T(b_l) = (b_l^k)\\_{k=0}^K\\) and natural parameters \\({\\bf \\eta}\\). It has a normalizing constant \\(A(\\eta) = \\log \\int \\exp\\{\\langle T(x), \\eta \\rangle\\} dx\\), and \\(\\nabla_{\\eta} A(\\eta) = \\mathbb E_{ q } \\left[  T(x)  \\right]\\). Thus if we can compute (or approximate to satisfactory precision) \\(\\nabla_{\\eta} A(\\eta)\\) we could compute the moments we need for CAVI.\nTo date, I am not really sure how to handle this integral of an exponentiation polynomial. By designing our polynomial approximation correctly, we can ensure that the the exponentiation function will decay and the \\(A\\) will be finite (recall also that \\(\\set{\\eta: A(\\eta) < \\infty}\\) is the natural parameter space).\nOne option is to approximate \\(A(\\eta)\\) by a quadrature rule. We can use automatic differentiation to compute it’s gradient.\n\n\nBest gaussian approximation\nMaybe we don’t know how to compute \\(A(\\eta) = \\log \\int \\exp\\{\\langle T(x), \\eta \\rangle\\} dx\\) Which involves evaluate the integral of an exponentiated polynomial. But perhaps we want to use a Gaussian variational approximation.\n\\[\nq_{l}^*(x) \\propto e^{f(x)}\n\\approx e^{f(\\mu) + f'(\\mu)(x-\\mu) + \\frac{1}{2}f''(\\mu)(x - \\mu)^2}\n\\]\nFor \\(\\mu\\) such that \\(f'(\\mu) = 0\\)\n\\[\ne^{f(\\mu) + f'(\\mu)(x-\\mu) + \\frac{1}{2}f''(\\mu)(x - \\mu)^2} \\propto e^{\\frac{1}{2}f''(\\mu)(x - \\mu)^2} \\propto \\mathcal N (x; \\mu, -\\frac{1}{f''(\\mu)})\n\\]\nIn our case \\(f\\) is a polynomial. Finding \\(\\mu\\) can be achieved by searching over the roots of \\(f'\\) and then \\(f''(\\mu)\\) is computed easily. This is a Laplace approximation to the optimal posterior \\(q_l\\)"
  },
  {
    "objectID": "research/polynomial_approximation/index.html#more-scattered-notes-on-polynomial-approximation-for-susie",
    "href": "research/polynomial_approximation/index.html#more-scattered-notes-on-polynomial-approximation-for-susie",
    "title": "Polynomial approximation for variational Bayes",
    "section": "More scattered notes on polynomial approximation for SuSiE",
    "text": "More scattered notes on polynomial approximation for SuSiE\nAbusing notation a bit, \\(\\phi_l = x_{\\gamma_l} b_l\\).\n\\[\n\\begin{aligned}\nf(\\psi_l)\n&= \\sum \\tilde m(q_{-l}) M_k(\\psi_l) \\\\\\\\\n&= \\sum \\tilde m(q_{-l}) A_k(x_jb_l) \\\\\\\\\n&= \\sum \\hat m_k(q_{-l}, x_j) M_k(b_l) \\\\quad \\hat m_k(q_{-l}, x_j) := \\tilde m_k(q_{-l})x_j^k\n\\end{aligned}\n\\]\n\\[\n\\mathbb E_{ q_{-l} } \\left[  f(\\psi)  \\right]\n= \\mathbb E_{ q_{-l} } \\left[  f(\\psi_l, \\psi_{-l}))  \\right]\n= \\sum \\mathbb E_{ q_{-l} } \\left[  \\tilde {\\bf m}(\\psi_{-l})  \\right] M_k(\\psi_l)\n= \\sum \\left(\\prod_{m \\neq l}\\mathbb E_{ q_m } \\left[  M(\\psi_m))  \\right]\\right){\\bf m}\n\\]\nWe can write\n\\[\n\\Gamma_l = \\mathbb E_{ q_l } \\left[  M(\\psi_l)  \\right]\n\\] \\[\n\\Psi^{(l)} = \\Gamma_{l+1} \\dots \\Gamma_L \\Gamma_1 \\dots \\Gamma_{l-1}\n\\] Then we can compute the coefficients of \\(f(\\psi_l)\\) by a triangular matrix multiplication\n\\[\n\\tilde{\\bf m}\\_l = \\Psi^{(l)}{\\bf m} = \\mathbb E_{ q_{-l} } \\left[  M(\\psi_{-l})  \\right]{\\bf m}\n\\]\nAnd we can compute the next \\(\\Psi\\) by a triangular matrix inversion and two matrix multiplications.\n\\[\n\\Psi^{(l+1)} = \\Gamma_{l+1}^{-1} \\Psi^{(l)} \\Gamma_l\n\\] ### Rescal polynomial\n\\[\nf(bx) = \\sum m_k (bx)^k = \\sum m_k b^kx^k = \\sum (m_k b^k) x^k\n\\]\n\nShift\n\\[\nf(x - c)\n= \\sum m_k (x -c)^k\n= \\sum_k m_k \\sum_{j \\leq k} {k \\choose j} x^j c^{k-j}\n= \\sum_j \\left(\\sum_{k \\geq j} {k \\choose j} c^{k-j}\\right) x^j\n\\]\n\n\nUpdating \\(q_l\\)\nWe’ve written the natural parameters\nFor each observation we can compute\n\\[\n\\hat{{\\bf m}}\\_{li} = \\tilde{\\bf m}(q_{-l}) \\circ (x_i^0, \\dots, x_i^K)\n\\]\nThese are the coefficients in the monomial basis for each observation conditional on effect \\(b_l\\) for covariate \\(x_i\\). \\(f_i(b_l) = \\sum \\tilde {\\bf m}\\_k b_l^k\\). That is, this is the data likelihood as a function of \\(b_l\\), conditional on data \\(x\\), and marginalizing over \\(\\psi_{-l}\\).\nWe can express or approximate our prior with the same polynomial expansion. Suppose we can write our prior\n\\[\n\\log p(b_l) = \\sum \\rho_{kl} b_l^k\n\\]\nThen the posterior distribution is trivially computed with a conjugate computation\n\\[\n{\\bf \\eta}\\_l =\\sum_i \\hat {\\bf m}_{li} + {\\bf \\rho}_l\n\\]\nThe posterior distribution is an exponential family with sufficient statistics \\(T(b_l) = (b_l^0, \\dots, b_l^K)\\) and natural parameters \\(\\eta_l\\).\nIf our original polynomial approximation “goes down” outside the range we care to ensure it is a good approximation, then we should always get a finite log-normalizer/cumulant \\(A(\\eta) = \\log \\int \\exp\\{\\eta^T T(\\psi)\\} d\\psi < \\infty\\). It may be important to ensure that our approximation is good over the range of values of \\(\\psi\\) with high posterior probability. Supposing we have an even degree polynomial assumption, make sure the last coefficient is \\(< 0\\) so that the function is very negative for arguments that are large in absolute value, but the approximation is good for values of small absolute value. Intuitivley, taking expectations over \\(\\psi_{-l}\\) won’t change t\nAdditionally, we ideally want to make sure that our likelihood approximation does not have bad behavior. If our polynomial approximation wildly overestimates the likelihood in some regions that could seriously mess up our inference. There is probably a tradeoff. We can approximate \\(l_i\\) on the interval \\([a, b]\\) with lower error with a polynomial of degree \\(K\\). To approximate \\(l_i\\) on the wider interval \\([a, b] \\subset [A, B]\\) with the same error we need a higher degree \\(K\\).\n\n\nComputing moments of \\(q_l\\)\nAn algorithm would look like this\n\nCompute \\(\\Psi_1\\)\nUpdate update \\(q_1\\)\nCompute \\(\\mathbb E_{ q_1 } \\left[  M(\\psi_1)  \\right]\\)\nCompute \\(\\Psi_2\\) …\n\nNote that \\(\\Psi_l\\) is constructed by taking expectations is a particular order, or multiplying matrices in a particular order. But I think order should not matter. Is it the case that triangular matrix multiplication commutes?"
  },
  {
    "objectID": "research/polynomial_approximation/index.html#discussion",
    "href": "research/polynomial_approximation/index.html#discussion",
    "title": "Polynomial approximation for variational Bayes",
    "section": "Discussion",
    "text": "Discussion\n\nQuality of the global approximation\nTheorem C.3 in [(Huggins, Adams, and Broderick 2017)] provides a bound on the Wasserstein distance between the exact posterior and the polynomial approximation to the posterior, “the result depends primarily on the peakedness of the approximate posterior, and the error of the approximate gradients”. Informally, I suspect that the more data we accrue, and the more peaked our approximate posterior becomes, the greater demand we must put on the quality of our approximation to the log density. Imagine a situation where the true likelihood surface is flat, but looks a little bumpy due to error in the polynomial approximation. These small bumps will accumulate over a large sample size leading to a spiky posterior where it should have been flat.\nIn contrast, the local approximations should shine as we accumulate more evidence and the likelihood becomes more peaked. This is because we can tune the local approximation to be tight where the likelihood peaks. The errors in the local approximation matter less because we don’t need to stray far from where the bound is tight. Note: when the likelihood is log-concave, the local variational approximations are also concave [(Seeger 2009; Challis and Barber 2011)]. I’ll need to do some more work to understand this completely for generalized local approximations, but it is certainly the case for the logistic case. In fact, running EM with the JJ bound is a good alternative to using Newton’s method to get point estimates of the MAP. Whereas Newton’s method may diverge for some initialization, JJ should converge for any initialization of the variational parameters (note: check this claim).\nQ: Is logistic regression with a Gaussian prior/L2 penalty on the effects convex? If so, we’re replacing a convex problem with a potentially multimodal one."
  },
  {
    "objectID": "research/polynomial_approximation/index.html#glossary",
    "href": "research/polynomial_approximation/index.html#glossary",
    "title": "Polynomial approximation for variational Bayes",
    "section": "Glossary",
    "text": "Glossary\n\n\n\n\n\n\n\nSymbol\nDescription\n\n\n\n\n\\({\\bf a}_i\\)\ncoefficients for \\(f_i\\) in basis \\(\\mathcal A\\)\n\n\n\\(\\tilde {\\bf a}_i(\\psi_2)\\)\ncoefficients for \\(f_i(\\psi_1; \\psi_2)\\) in the basis \\(\\mathcal A\\)\n\n\n\\(\\hat {\\bf a}_i(\\psi_2, x_j)\\)\ncoefficients for \\(f_i(b_1; x_j, \\psi_2)\\) in the basis \\(\\mathcal A\\)\n\n\n\\(M(\\psi_2)\\)\ntriangular matrix shifts monomial basis, \\(\\tilde {\\bf m} (\\psi_2) = M(\\psi_2) {\\bf m}\\). Gives coefficients of \\(f_i(\\psi_1; \\psi_2)\\) in \\(\\mathcal M\\)\n\n\n\\(A\\)\ntriangular matrix maps to coordinates in monomial basis, \\({\\bf m} = A {\\bf a}\\). Gives coefficients of \\(f_i(\\psi_1; \\psi_2)\\) in \\(\\mathcal M\\)\n\n\n\\(f_i(\\psi_1; \\psi_2)\\)\nA polynomial is \\(\\psi_1\\) such that \\(f_i(\\psi_1; \\psi_2) = f_i(\\psi_1 + \\psi_2); \\\\;\\\\; \\tilde {\\bf m}(\\psi_2) = M(\\psi_2){\\bf m}\\) gives the coordinates in the monomial basis"
  },
  {
    "objectID": "research/polynomial_approximation/index.html#related-work",
    "href": "research/polynomial_approximation/index.html#related-work",
    "title": "Polynomial approximation for variational Bayes",
    "section": "Related work",
    "text": "Related work\n(Huggins, Adams, and Broderick 2017) (Wong, n.d.)"
  },
  {
    "objectID": "research/polynomial_approximation/index.html#references",
    "href": "research/polynomial_approximation/index.html#references",
    "title": "Polynomial approximation for variational Bayes",
    "section": "References",
    "text": "References\n\n\nChallis, Edward, and David Barber. 2011. “Concave Gaussian Variational Approximations for Inference in Large-Scale Bayesian Linear Models.” In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, 199–207. JMLR Workshop and Conference Proceedings. https://proceedings.mlr.press/v15/challis11a.html.\n\n\nGaly-Fajou, Theo, Florian Wenzel, and Manfred Opper. 2020. “Automated Augmented Conjugate Inference for Non-conjugate Gaussian Process Models.” In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, 3025–35. PMLR. https://proceedings.mlr.press/v108/galy-fajou20a.html.\n\n\nHuggins, Jonathan, Ryan P Adams, and Tamara Broderick. 2017. “PASS-GLM: Polynomial Approximate Sufficient Statistics for Scalable Bayesian GLM Inference.” In Advances in Neural Information Processing Systems. Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2017/hash/07811dc6c422334ce36a09ff5cd6fe71-Abstract.html.\n\n\nSeeger, Matthias W. 2009. “Sparse Linear Models: Variational Approximate Inference and Bayesian Experimental Design.” Journal of Physics: Conference Series 197 (December): 012001. https://doi.org/10.1088/1742-6596/197/1/012001.\n\n\nWong, Lilian. n.d. “Orthogonal Polynomials–Quadrature Algorithm (OPQA): A Functional Analytical Approach to Bayesian Inference.” Orthogonal Polynomials."
  },
  {
    "objectID": "research/polynomial_regression_vb/index.html",
    "href": "research/polynomial_regression_vb/index.html",
    "title": "Implimenting polynomial approximation VB",
    "section": "",
    "text": "Code\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(pracma)\nlibrary(tictoc)\n\n\n\nAttaching package: 'tictoc'\n\n\nThe following objects are masked from 'package:pracma':\n\n    clear, size, tic, toc\n\n\nCode\nset.seed(1)"
  },
  {
    "objectID": "research/polynomial_regression_vb/index.html#computation",
    "href": "research/polynomial_regression_vb/index.html#computation",
    "title": "Implimenting polynomial approximation VB",
    "section": "Computation",
    "text": "Computation\n\nChebyshev approximations\nChebyshev polynomials can be used to approximate functions on the interval \\([-1, 1]\\) (and then also, on any finite interval). \\(f(x) \\approx \\sum c_k T_k(x)\\). We compute the coefficients of \\(c_k\\) for \\(k = 0, \\dots K\\).\nIn the code below we use pracma::polyApprox which implement a scheme of evaluate the coefficients for \\(f\\). This is essentially done via quadrature.\n\n\nCode\nmake_approximation <- function(f, R, n, plot=F){\n  p <- rev(pracma::polyApprox(f, -R, R, n =n)$p)\n  if(plot){\n    S <- R + 2\n    x <- seq(-S, S, by=0.1)\n    plot(f, -S, S)\n    lines(x, polyval2(p, x), col='red', lty='dotted')\n    abline(v=-R); abline(v=R)\n  }\n  return(p)\n}\n\n\n\n\nScaling and shifting polynomials\nCoefficients after a change of variable\n\\[\nf(bx) = \\sum m_k (bx)^k = \\sum m_k b^kx^k = \\sum (m_k b^k) x^k = f_b(x)\n\\]\n\\[\nf(x + c)\n= \\sum m_k (x + c)^k\n= \\sum_k m_k \\sum_{j \\leq k} {k \\choose j} x^j c^{k-j}\n= \\sum_j \\left(\\sum_{k \\geq j} {k \\choose j} c^{k-j}\\right) x^j\n\\]\n\n\nCode\n#' p coefficients of a polynomial in increasing order\n#' @param p polynomial coefficients in INCREASING deree\npolyval2 <- function(p, x){pracma::polyval(rev(p), x)}\n\n#' f(x + c) = f2(x)\nshift_polynomial <- function(p, c){\n  # construct map\n  K <- length(p) - 1\n  M <- matrix(nrow= K+1, ncol=K+1)\n  for(j in 0:K){\n    for(k in 0:K){\n      M[j+1, k+1] <- choose(k, j) * c**(k - j)\n    }\n  }\n  \n  coef_new <- (M %*% p)[, 1]\n  return(coef_new)\n}\n\n# change back to original scale\n# f(bx) = f2(x)\nscale_polynomial <- function(p, b){\n  K <- length(p) - 1\n  coef_new <- p * sapply(0:K, function(k) b**k)\n  return(coef_new)\n}\n\n\n\n\nLaplace approximation to polynomial\n\n\nCode\n#' convert (unnormalized) polynomial density to gaussian approximation\n#' p the coefficients of a polynomial in increasing order p = c(p0, p1, ..., pK)\npoly_to_gaussian <- function(p){\n  p <- rev(p)\n  d <- pracma::polyder(p)\n  d2 <- pracma::polyder(d)\n  \n  #f <- function(x){polyval2(p, x)}\n  #mu <- optimize(f, interval = c(-100, 100), maximum = T)$maximum\n  roots <- Re(pracma::polyroots(d)$root)\n  mu <- roots[which.max(pracma::polyval(p, roots))]\n  \n  var <- - 1 / pracma::polyval(d2, mu)\n  return(list(mu=mu, var=var))\n}\n\n\nTo test it out:\n\n\nCode\ncoef<- c(1, 2, 3)\ncoef2 <- shift_polynomial(coef, -1)\n#f(x-1) = f2(x)\n(polyval2(coef, 3) == polyval2(coef2, 4))\n\n\n[1] TRUE\n\n\nCode\ncoef<- c(1, 2, 3)\ncoef2 <- scale_polynomial(coef, 2)\n# f(2x) = f2(x)\n(polyval2(coef, 6) == polyval2(coef2, 3))\n\n\n[1] TRUE\n\n\n\n\nPlot functions\n\n\nCode\nplot_effect_posterior <- function(q, b, ...){\n  mu_post <- purrr::map_dbl(q, ~purrr::pluck(.x, 'mu'))\n  var_post <- purrr::map_dbl(q, ~purrr::pluck(.x, 'var'))\n  \n  plotrix::plotCI(\n    x = b,\n    y = mu_post,\n    li =  mu_post - 2 * sqrt(var_post),\n    ui =  mu_post + 2 * sqrt(var_post),\n    ...\n  )\n  abline(0, 1, col='red')\n}"
  },
  {
    "objectID": "research/polynomial_regression_vb/index.html#gaussian-linear-regression-mean-field-approximation",
    "href": "research/polynomial_regression_vb/index.html#gaussian-linear-regression-mean-field-approximation",
    "title": "Implimenting polynomial approximation VB",
    "section": "Gaussian linear regression, mean field approximation",
    "text": "Gaussian linear regression, mean field approximation\nHere we illustrate a simple example of our inference technique. We fit a mean field approximation to a bivariate regression problem\n\\[\n\\begin{aligned}\ny &\\sim N(\\sum_j x_j b_j, 1)\\\\\nb_j &\\sim N(0, 1)\\;\\; j = 1, \\dots, p\n\\end{aligned}\n\\]\nWhere \\(x_1\\) and \\(x_2\\) are correlated (inducing dependence in the posterior \\(p(b_1, b_2 | \\mathcal D)\\).\n\nPolynomial representation of Gaussian likelihood\nFor the observations\n\\[\nl(\\psi)\n= C -\\frac{1}{2\\sigma^2}(y - \\psi)^2\n= C -\\frac{1}{2\\sigma^2} y^2 + \\frac{y}{\\sigma^2}\\psi - \\frac{1}{2\\sigma^2}\\psi^2 \\implies {\\bf m}\n= (C -\\frac{y^2}{2 \\sigma^2}, \\frac{y}{\\sigma^2}, -\\frac{1}{2\\sigma^2})\n\\] For the prior\n\\[\n\\log p(b) = C -\\frac{1}{2\\sigma^2} \\left(b - \\mu \\right)^2\n\\implies {\\bf m} = \\left(C - \\frac{\\mu^2}{2\\sigma^2}, \\frac{\\mu}{\\sigma^2}, -\\frac{1}{2 \\sigma^2}\\right)\n\\]\n\n\nMean field variational approximation\n\\[\n\\begin{aligned}\nq({\\bf b}) = \\prod_j q(b_j) \\\\\nq(b_j) = N(\\mu_j, \\sigma^2_j)\n\\end{aligned}\n\\]\n\n\nComputing moment under \\(q\\)\n\\[\n\\mathbb E \\psi_j = \\mu_j x_j\n\\]\n\\[\n\\mathbb E \\psi_j^2 = (\\sigma^2_j + \\mu^2_j) x^2_j\n\\]\n\\[\n\\bar \\psi := \\mathbb E_q[\\psi] = \\sum \\mathbb E_q[\\psi_j]\n\\]\n\\[\n\\bar{\\psi^2} = \\mathbb E_q[\\psi^2] =  \\mathbb E_q[ \\left(\\sum \\psi_j \\right)^2] = Var(\\psi) + \\bar \\psi\n\\]\n\n\nCode\n#' write gaussian log density as polynomial \n#' return a vector c representing polynomial c[3]x^2 + c[2] x + c[1]\nmake_gaussian_coef <- function(y, var=1){\n  c(- 0.5 * y**2/var, y/var, -0.5 / var)\n}\n\n\n\n\nCode\n#' q a p-list of distributions for effect size of each column\n#' X a n x p matrix of covariate\n#' returns a list(mu, mu2) with the means and second moments\ncompute_moments <- function(X, q){\n  p <- ncol(X)\n  mu <- 0\n  var <- 0\n  for (j in 1:p){\n    mu <- mu + (X[,j] * q[[j]]$mu)\n    var <- var + (X[,j]**2 * q[[j]]$var)\n  }\n  mu2 <- var + mu**2\n  return(list(mu=mu, mu2=mu2))\n}\n\n#' compute coeeficient for EXPECTED shift\n#' f2(x) = E(f(x + c))\nshift_polynomial2 <- function(coef, mu, mu2){\n  c <- list(1, mu, mu2)\n  # construct map\n  K <- length(coef) - 1\n  M <- matrix(nrow= K+1, ncol=K+1)\n  for(j in 0:K){\n    for(k in 0:K){\n      M[j+1, k+1] <- choose(k, j) * c[[max(k-j+1, 1)]]\n    }\n  }\n  coef_new <- (M %*% coef)[, 1]\n  return(coef_new)\n}\n\n\n\n\nCode\nsublist <- function(list, j){\n  list[[j]] <- NULL\n  return(list)\n}\n\npolynomial_update2 <- function(m, X, prior_p, q){\n  p <- length(q)\n  n <- nrow(X)\n  for(j in 1:p){\n    # compute moments\n    # right now we just compute two moments, but need higher moments for \n    # higher degree polynomials\n    moments <- compute_moments(X[, -j, drop=F], sublist(q, j))\n    \n    # shift-- new polynomial in terms of \\psi_j\n    m2_tilde <- do.call(rbind, lapply(1:n, function(i) shift_polynomial2(\n      m[i,], moments$mu[i], moments$mu2[i])))\n    \n    # scale-- new polynomial in terms of b_j\n    m2_hat <- do.call(rbind, lapply(1:n, function(i) scale_polynomial(m2_tilde[i,], X[i, j])))\n    \n    # compute posterior polynomial\n    m2_post <- colSums(m2_hat) + prior_p[[j]]\n    \n    # find gaussian approximation\n    q[[j]] <- poly_to_gaussian(m2_post)\n  }\n  \n  return(q)\n}\n\n\n\n\nExample\n\n\nCode\nsimulate <- function(n=500, p=2, lenghtscale = 0.8, prior_variance=5){\n  Z <- matrix(rnorm(n*p), nrow=n)\n  K <- exp(-(outer(1:p, 1:p, '-')/lenghtscale)**2)\n  X <- Z %*% K\n  \n  b <- rnorm(p) * sqrt(prior_variance)\n  y <- (X %*% b)[, 1] + rnorm(n)\n  \n  return(list(y=y, X=X, b=b))\n}\n\nsim <- simulate(p=3)\ny <- sim$y\nX <- sim$X\nb <- sim$b\nprint(b)\n\n\n[1]  1.900755 -2.069063  1.998108\n\n\n\n\nCode\n# observations in polynomial coeeficients\nm <- do.call(rbind, lapply(y, make_gaussian_coef))\np <- ncol(X)\n\n# initialize prior and q\nq <- list()\nprior_p <- list()\nfor(j in 1:p){\n  prior_p[[j]] <- c(0, 0, -0.5)\n  q[[j]] <- list(mu = 0, var=1)\n}\n\n# iteratively update\nparam_history <- list()\nparam_history[[1]] <- q\nfor(i in 1:50){\n  q <- polynomial_update2(m, X, prior_p, q)\n  param_history[[i+1]] <- q\n}\n\nmu_post <- purrr::map_dbl(q, ~purrr::pluck(.x, 'mu'))\nvar_post <- purrr::map_dbl(q, ~purrr::pluck(.x, 'var'))\n\n\nHere we simulate a gaussian linear mode with three covariates. We plot the simulated effects against their posteror mean. It looks like we are able to recover the effects. Nice!\n\n\nCode\nplot_effect_posterior(q, sim$b)\n\n\n\n\n\n\n\nComparison to usual Bayesian computation\nTODO"
  },
  {
    "objectID": "research/polynomial_regression_vb/index.html#logistic-regression",
    "href": "research/polynomial_regression_vb/index.html#logistic-regression",
    "title": "Implimenting polynomial approximation VB",
    "section": "Logistic regression",
    "text": "Logistic regression\nNext we will take a quadratic approximation for logistic regression. We will also test higher degree polynomial approximations. In all cases we continue to use a Gaussian mean-field variational approximation. Computing the moments of the higher degree polynomials may be tricky, so we defer for now. However, if it can be done easily we may benefit from the richer varitional approximation (note: the Gaussian distribution is a degree 2 polynomial exponential family since it’s sufficient statistics are \\(T(x) = [x, x^2]\\))\n\nPolynomial approximation to log likelihood\nFor each data point we want to approximate the log likelihood as a function of the linear predictor\n\\[\n\\log p(y | \\psi) = y\\psi + \\log \\sigma(-\\psi)\n\\]\nHere we plot the \\(\\log p(y=0 | \\psi)\\) and it’s polynomial approximations of degree \\(k=2,4,6,8\\).\nThese polynomial approximations are generated using pracma::polyApprox which use the Chebyshev coefficients of an appropriately rescaled version of the function, to generate a polynomial approximation on the interval \\([a, b]\\). We generate approximations on the interval \\([-R, R]\\) where \\(R = 3\\).\n\n\nCode\n# loglike functions for y=1 and y=0\nloglik1 <- function(psi){\n  psi + log(sigmoid(-psi))\n}\nloglik0 <- function(psi){\n  log(sigmoid(-psi))\n}\n\n# polynomial approximation via pracma\nR <- 3\nll0_p2 <- rev(pracma::polyApprox(loglik0, -R, R, n = 2)$p)\nll0_p4 <- rev(pracma::polyApprox(loglik0, -R, R, n = 4)$p)\nll0_p6 <- rev(pracma::polyApprox(loglik0, -R, R, n = 6)$p)\nll0_p8 <- rev(pracma::polyApprox(loglik0, -R, R, n = 8)$p)\n\n# note: ll0 and ll1 are just reflections over the x axis\n# so we can get ll1 by taking ll0_p2 and flipping the sign of the linear term\nll1_p2 <- rev(pracma::polyApprox(loglik1, -R, R, n = 2)$p)\nll1_p4 <- rev(pracma::polyApprox(loglik1, -R, R, n = 4)$p)\nll1_p6 <- rev(pracma::polyApprox(loglik1, -R, R, n = 6)$p)\nll1_p8 <- rev(pracma::polyApprox(loglik1, -R, R, n = 8)$p)\n\nS <- R + 2\nx <- seq(-S, S, by=0.1)\nplot(loglik0, -S, S)\nlines(x, polyval2(ll0_p2, x), col='red', lty='dotted')\nlines(x, polyval2(ll0_p4, x), col='blue', lty='dotted')\nlines(x, polyval2(ll0_p6, x), col='green', lty='dotted')\nlines(x, polyval2(ll0_p8, x), col='orange', lty='dotted')\nabline(v=R); abline(v=-R)\n\n\n\n\n\n\n\nApproximate date likelihood\nThis just takes a whoe list of vector \\(y\\) and returns a matrix of coefficients\n\n\nCode\n#' get approximate polynomial representation of the data y\nbernoulli_poly_approx <- function(y, R, k){\n  n <- length(y)\n  p0 <- make_approximation(loglik0, R, k)\n  \n  # for y=1 flip the sign of odd coefficients (note: 0 indexing)\n  p1 <- p0\n  p1[seq(2, length(p0), by=2)] <- p1[seq(2, length(p0), by=2)] * -1\n  \n  m <- matrix(nrow = n, ncol = k + 1)\n  for(i in 1:length(y)){\n    if(y[i] == 0){\n      m[i,] <- p0\n    } else{\n      m[i,] <- p1\n    }\n  }\n  return(m)\n}\n\n\n\n\nSimulate logistic regression\n\n\nCode\nsimulate_lr <- function(n=500, p=2, lenghtscale = 0.8, prior_variance=5){\n  Z <- matrix(rnorm(n*p), nrow=n)\n  K <- exp(-(outer(1:p, 1:p, '-')/lenghtscale)**2)\n  X <- Z %*% K\n  \n  b <- rnorm(p) * sqrt(prior_variance)\n  logits <- (X %*% b)[, 1]\n  y <- rbinom(length(logits), 1, sigmoid(logits))\n  return(list(y=y, X=X, b=b, logits=logits))\n}\n\n\n\n\nApproximation with \\(k=2\\)\nWe can reuse our code above, substituting in new “data”, the coefficients for the polynomial approximation to the conditional likelihood.\n\n\nCode\nlogistic_polynomial_approximation_k2 <- function(y, X, R){\n  # observations in polynomial coeeficients\n  m <- bernoulli_poly_approx(y, R, k=2)\n  p <- ncol(X)\n  \n  # initialize prior and q\n  q <- list()\n  prior_p <- list()\n  for(j in 1:p){\n    prior_p[[j]] <- c(0, 0, -0.5)\n    q[[j]] <- list(mu = 0, var=1)\n  }\n  \n  # iteratively update\n  param_history <- list()\n  param_history[[1]] <- q\n  for(i in 1:50){\n    q <- polynomial_update2(m, X, prior_p, q)\n    param_history[[i+1]] <- q\n  }\n  return(param_history)\n}\n\n\nA 2d approximation does not seem to perform very well here.\n\n\nCode\nset.seed(5)\nn <- 500\np <- 3\nlenghtscale <- 0.8\nprior_variance <- 5\n\nZ <- matrix(rnorm(n*p), nrow=n)\nK <- exp(-(outer(1:p, 1:p, '-')/lenghtscale)**2)\nX <- Z %*% K\n\nb <- rnorm(p) * sqrt(prior_variance)\nlogits <- (X %*% b)[, 1]\ny <- rbinom(length(logits), 1, sigmoid(logits))\nprint(b)\n\n\n[1] 3.3319418 0.3807557 5.4429687\n\n\nCode\nqs <- logistic_polynomial_approximation_k2(y, X, R=5)\nq <- tail(qs, 1)[[1]]\nmu_post <- purrr::map_dbl(q, ~purrr::pluck(.x, 'mu'))\nvar_post <- purrr::map_dbl(q, ~purrr::pluck(.x, 'var'))\n\nplotrix::plotCI(\n  x = b,\n  y = mu_post,\n  li =  mu_post - 2 * sqrt(var_post),\n  ui =  mu_post + 2 * sqrt(var_post)\n)\nabline(0, 1, col='red')\n\n\n\n\n\n\nTesting a range of interval widths\nWe can check a few values of \\(R\\). There is a tradeoff here of course– the wider the interval we try to approximate, the worse the approximation will be. But if the interval is too narrow, the polynomial approximate likelihood essentially does not support data that fall far outside the interval. This is because we require the highest odd degree coefficient of our polynomial to be \\(<0\\) otherwise the likelihood grows unbounded outside the interval, and the approximation is not integrable.\n\n\n\n\n\nCode\npar(mfrow = c(1, 3))\nplot_effect_posterior(q_R3, b, main='R=3')\nplot_effect_posterior(q_R5, b, main='R=5')\nplot_effect_posterior(q_R7, b, main='R=7')\n\n\n\n\n\n\n\n\nHigher degree approximations\nNow we will extend our implementation above to handle higher degree approximations. This involves computing higher moments of the effect predictions, which isn’t too hard for Gaussian distributions.\n\n\nCode\n#' Make shift matrix\n#' \n#' Generate matrix that maps coefficients of a polynomial\n#' f(x + y) (represented by coefficients p) to coefficients of\n#' f2(x) = E_{p(y)}[f(x+y)]\n#' @param moments moments of y\nmake_shift_matrix <- function(moments){\n  # construct map\n  K <- length(moments) - 1\n  M <- matrix(nrow= K+1, ncol=K+1)\n  for(j in 0:K){\n    for(k in 0:K){\n      M[j+1, k+1] <- choose(k, j) * moments[[max(k-j+1, 1)]]\n    }\n  }\n  return(M)\n}\n\n#' Transform coefficients of a polynomial f(x + y) (represented by coefficients p)\n#' to coefficients of f2(x) = E_{p(y)}[f(x+y)]\n#' @param p K+1 coefficients of a degree-k polynomial\n#' @param moments moments of y (including E[y^0] = 1)\nshift_polynomial3 <- function(p, moments){\n  M <- make_shift_matrix(moments)\n  p_new <- (M %*% p)[, 1]\n  return(p_new)\n}\n\ncompute_normal_moments <- function(mu, var, k){\n  return(purrr::map_dbl(0:k, ~actuar::mnorm(.x, mu, sqrt(var))))\n}\n\n#' compute k moments for psi = xb, b ~ N(mu, var)\ncompute_psi_moments <- function(x, mu, var, k){\n  normal_moments <- compute_normal_moments(mu, var, k)\n  psi_moments <- do.call(cbind, purrr::map(0:k, ~ (x**.x) * normal_moments[.x + 1]))\n}\n\n#' update q with polynomial approximation of arbitrary degree\npolynomial_update3 <- function(m, X, prior_p, q){\n  K <- ncol(m) - 1\n  p <- ncol(X)\n  n <- nrow(X)\n  for(j in 1:p){\n    m_tilde <- m\n    for(k in (1:p)[-j]){\n      moments <- compute_psi_moments(X[, k], q[[k]]$mu, q[[k]]$var, K)\n      m_tilde <- do.call(rbind, lapply(1:n, function(i) shift_polynomial3(\n        m_tilde[i,], moments[i,])))\n    }\n    \n    # scale-- new polynomial in terms of b_j\n    m_hat <- do.call(rbind, lapply(1:n, function(i) scale_polynomial(\n      m_tilde[i,], X[i, j])))\n    \n    # compute posterior polynomial\n    m_post <- colSums(m_hat) + prior_p[[j]]\n    \n    # find gaussian approximation\n    q[[j]] <- poly_to_gaussian(m_post)\n    q[[j]]$m_post <- m_post\n  }\n  \n  return(q)\n}\n\nlogistic_polynomial_approximation <- function(y, X, R, K=2){\n  # observations in polynomial coeeficients\n  m <- bernoulli_poly_approx(y, R, K)\n  q <- list()\n  prior_p <- list()\n  for(j in 1:p){\n    prior_p[[j]] <- c(c(0, 0, -0.5), rep(0, K-2)) # extend polynomial to agree with m\n    q[[j]] <- list(mu = 0, var=1) # initialize normal posterior\n  }\n  \n  # iteratively update\n  param_history <- list()\n  param_history[[1]] <- q\n  for(i in 1:50){\n    q <- polynomial_update3(m, X, prior_p, q)\n    param_history[[i+1]] <- q\n  }\n  return(param_history)\n}\n\n\n\n\nCompare implimentations\nThe two implimentations agree!\n\n\nCode\nset.seed(12)\nsim <- simulate_lr(p=3)\ny <- sim$y\nX <- sim$X\nb <- sim$b\n\nqs_2 <- logistic_polynomial_approximation_k2(y, X, R=10)\nqs_k2 <- logistic_polynomial_approximation(y, X, R=10, K=2)\n\npar(mfrow=c(1,2))\nplot_effect_posterior(qs_2[[51]], b, main='A')\nplot_effect_posterior(qs_k2[[51]], b, main='B')\n\n\n\n\n\n\n\nHow does the posterior approximation change as we increase degree?\nWe approximate the likelihood on \\([-10, 10]\\) with \\(K=2,6,10,14\\). Note the polynomial approximation cannot be e.g. degree \\(4\\) because these polynomials are unbounded above (\\(c_4 >0\\)), so \\(e^ \\hat f(x)\\) is not integrable over the real line. But \\(c_K < 0\\) for approximations of degree \\(K = 2z + 2\\) for \\(z \\in \\mathbb N\\).\nIn this example it seems that with \\(K=2\\) we tend to over-estimate the effect size, but this is resolved in the higher degree approximations.\n\n\nCode\nset.seed(12)\nsim <- simulate_lr(p=3)\ny <- sim$y\nX <- sim$X\nb <- sim$b\n\nqs_k2 <- logistic_polynomial_approximation(y, X, R=10, K=2)\nqs_k6 <- logistic_polynomial_approximation(y, X, R=10, K=6)\nqs_k10 <- logistic_polynomial_approximation(y, X, R=10, K=10)\nqs_k14 <- logistic_polynomial_approximation(y, X, R=10, K=14)\n\npar(mfrow=c(2,2))\nplot_effect_posterior(qs_k2[[51]], b, main='K=2')\nplot_effect_posterior(qs_k6[[51]], b, main='K=6')\nplot_effect_posterior(qs_k10[[51]], b, main='K=10')\nplot_effect_posterior(qs_k14[[51]], b, main='K=14')"
  },
  {
    "objectID": "research/polynomial_regression_vb/index.html#poisson-regression-example",
    "href": "research/polynomial_regression_vb/index.html#poisson-regression-example",
    "title": "Implimenting polynomial approximation VB",
    "section": "Poisson regression example",
    "text": "Poisson regression example\n\nImpliment\n\n\nCode\npoisson_ll <- function(y){\n  f <- function(psi) dpois(y, exp(psi), log = T)\n  return(f)\n}\n\npoisson_approx <- function(y, R, k, as_function=F){\n  f <- poisson_ll(y)\n  p <- make_approximation(f, R, k, plot = F)\n\n  if(as_function){\n    p2 <- function(x) polyval2(p, x)\n    return(p2)\n  }\n  return(p)\n}\n\npoisson_poly_approx <- function(y, R, k){\n  unique_counts <- unique(y)\n  \n  polynomials <- list()\n  for(yy in unique_counts){\n    polynomials[[yy + 1]] <- poisson_approx(yy, R, k)\n  }\n  \n  m <- do.call(rbind, purrr::map(y, ~polynomials[[.x + 1]]))\n  return(m)\n}\n\npoisson_regression_polynomial_approximation <- function(y, X, R, K=2){\n  # observations in polynomial coeeficients\n  tic()\n  m <- poisson_poly_approx(y, R, K)\n  p <- ncol(X)\n  q <- list()\n  prior_p <- list()\n  for(j in 1:p){\n    prior_p[[j]] <- c(c(0, 0, -0.5), rep(0, K-2)) # extend polynomial to agree with m\n    q[[j]] <- list(mu = 0, var=1) # initialize normal posterior\n  }\n  \n  # iteratively update\n  param_history <- list()\n  param_history[[1]] <- q\n  for(i in 1:50){\n    q <- polynomial_update3(m, X, prior_p, q)\n    param_history[[i+1]] <- q\n  }\n  toc()\n  return(param_history)\n}\n\n\n\n\nVisualize approximation\nWe approximation \\(f(\\psi) = Pois(y=3; \\lambda = e^\\psi)\\) using polynomials of increasing degree on the interval \\([-5, 5]\\)\n\n\nCode\nf <- poisson_ll(3)\n\n# polynomial approximation via pracma\nR <- 5\nf_k2 <- rev(pracma::polyApprox(f, -R, R, n = 2)$p)\nf_k4 <- rev(pracma::polyApprox(f, -R, R, n = 4)$p)\nf_k8 <- rev(pracma::polyApprox(f, -R, R, n = 8)$p)\nf_k16 <- rev(pracma::polyApprox(f, -R, R, n = 16)$p)\n\nS <- R + 2\nx <- seq(-S, S, by=0.1)\nplot(f, -S, S)\nlines(x, polyval2(f_k2, x), col='red', lty='dotted')\nlines(x, polyval2(f_k4, x), col='blue', lty='dotted')\nlines(x, polyval2(f_k8, x), col='green', lty='dotted')\nlines(x, polyval2(f_k16, x), col='orange', lty='dotted')\nabline(v=R); abline(v=-R)\n\n\n\n\n\n\n\nSimulate poisson regression\n\n\nCode\nsimulate_poisson_regression <- function(n=500, p=2, lenghtscale = 0.8, prior_variance=1){\n  Z <- matrix(rnorm(n*p), nrow=n)\n  K <- exp(-(outer(1:p, 1:p, '-')/lenghtscale)**2)\n  X <- Z %*% K\n  \n  b <- rnorm(p) * sqrt(prior_variance)\n  logrates <- (X %*% b)[, 1]\n  y <- rpois(length(logrates), exp(logrates))\n  return(list(y=y, X=X, b=b, logrates=logrates))\n}\n\nsim <- simulate_poisson_regression(p=4, prior_variance = 1)\ny <- sim$y\nX <- sim$X\nb <- sim$b\nprint(b)\n\n\n[1] -1.3296046  0.6149598  0.7740477  0.2643521\n\n\n\n\nExample\n\n\nCode\npois_R5_K2 <- poisson_regression_polynomial_approximation(y, X, R=5, K=2)\n\n\n6.152 sec elapsed\n\n\nCode\npois_R5_K4 <- poisson_regression_polynomial_approximation(y, X, R=5, K=4)\n\n\n9.306 sec elapsed\n\n\nCode\npois_R5_K8 <- poisson_regression_polynomial_approximation(y, X, R=5, K=8)\n\n\n20.818 sec elapsed\n\n\nCode\npois_R5_K16 <- poisson_regression_polynomial_approximation(y, X, R=5, K=16)\n\n\n61.81 sec elapsed\n\n\nCode\npar(mfrow=c(2,2))\nplot_effect_posterior(pois_R5_K2[[51]], b, main='K=2')\nplot_effect_posterior(pois_R5_K4[[51]], b, main='K=4')\nplot_effect_posterior(pois_R5_K8[[51]], b, main='K=8')\nplot_effect_posterior(pois_R5_K16[[51]], b, main='K=16')\n\n\n\n\n\n\n\nCode\nplot_effect_posterior(pois_R5_K2[[51]], sim$b)\n\n\n\n\n\nCode\npurrr::map_dbl(pois_R5_K2[[51]], ~purrr::pluck(.x, 'mu'))\n\n\n[1] -1.51427519  0.50936054  0.92844494  0.08298042"
  },
  {
    "objectID": "research/polynomial_regression_vb/index.html#higher-degree-polynomial-posterior-q",
    "href": "research/polynomial_regression_vb/index.html#higher-degree-polynomial-posterior-q",
    "title": "Implimenting polynomial approximation VB",
    "section": "Higher degree polynomial posterior \\(q\\)",
    "text": "Higher degree polynomial posterior \\(q\\)\nIn the above implementation we compute a polynomial proportional to the posterior density, but reduce this to a Gaussian approximation by taking a Laplace approximation.\nThat is we want to compute\n\\[\n\\mathbb E [b^j] = \\int_{\\mathbb R} b^j q(b)\n\\]\nWhich we approximate by\n\\[\n\\mathbb E [b^j] \\approx \\int_{\\mathbb R} b^j q_{\\text{gauss}}(b)\n\\]\nWhere \\(q_{\\text{gauss}}\\) is the Gaussian distribution that minimizes the divergence to \\(q_{\\text{gauss}} = \\arg \\min_{q_g} KL(q_g ||q)\\).\nIt would be better if we could compute the moments of \\(q(b_l) \\propto \\exp\\{\\sum_k \\eta_k b_l^k\\}\\). We define the log normalizing constant \\(A({\\bf \\eta}) = \\log \\int \\exp\\{\\sum_{k=0}^K \\eta_k b_l^k\\} db\\). Let \\(f(b) = \\sum_{k=0}^K \\eta_k b^k - A(\\eta)\\) so that \\(q(b) = \\exp\\{f(b)}\\).\nBecause \\(q\\) is in an exponential family, we know that \\(\\nabla_{\\eta} A(\\eta) = \\mathbb E[T(x)] = [1, \\mathbb E[b], \\mathbb E[b^2], \\dots, \\mathbb E[b^K]]\\). Is there an easy way to compute the gradient of \\(A\\)?"
  },
  {
    "objectID": "research/mode_matching_rss_vb/index.html",
    "href": "research/mode_matching_rss_vb/index.html",
    "title": "Mode seeking in mean field VB for RSS + sparse prior",
    "section": "",
    "text": "The RSS likelihood relates observed marginal effects to the unobserved effects of a joint model\n\\[\\begin{align}\n\\hat \\beta \\sim \\mathcal N(SRS^{-1} \\beta, SRS) \\\\\n\\beta \\sim g(\\cdot)\n\\end{align}\\]\nWhere we consider the problem of putting an i.i.d. prior on the entries of \\(\\beta\\) and using a mean field approximation for variational inference.\nSpecifically, we put a spike and slab prior on \\(\\beta_j = b_j\\gamma_j\\) for \\(j \\in [p]\\). Where \\(b_j \\sim N(0, \\sigma^2)\\) gives the distribution of non-zero effects, and and \\(\\gamma_j \\sim Bernoulli(\\pi)\\). That is, the effect is non-zero with probability \\(\\pi\\).\nThe problem we demonstrate, is that due to the mode matching behavior of the “reverse” KL divergence, which is minimized in variational inference, the posterior on \\(q(\\gamma_1, \\dots, \\gamma_p)\\) will tend to concentrate instead of accurately representing uncertainty. Furthermore, due to strong dependence among the posterior means.\nWe work with a simplified version of RSS assuming we observe \\(z\\)-scores \\(\\hat z\\).\n\\[\n\\begin{aligned}\n\\hat z &\\sim \\mathcal N(Rz, R) \\\\\nz_i &\\sim \\pi_0 \\delta_0 + \\pi_1 \\mathcal N(0, \\sigma^2)\n\\end{aligned}\n\\]\n\\[\nq(z, \\gamma) = \\prod_j q(z_j, \\gamma_j)\n\\]\n\\[\n\\begin{aligned}\nELBO(q_j) &= \\mathbb E_{q_{-j}} \\squarb{\\log p(\\hat z| z, R) + \\log p(z_j) - \\log q(b_l, \\gamma_l)} + H(q_l) \\\\\n&= \\hat z_j (b_j \\gamma_j) - \\frac{1}{2} \\left[ (b_j \\gamma_j)^2 + 2 (b_j \\gamma_j) \\sum_{i \\neq j} R_{ij} \\mathbb E_{q_{-j}} \\squarb{z_j} \\right] + \\log p(b_l | \\gamma_l) + \\log p(\\gamma_l) + H(q_l) + C\n\\end{aligned}\n\\]\nThen \\(q(b_l | \\gamma_l = 1) = N(\\frac{\\nu_j}{\\tau_j}, \\tau^{-1}_j)\\) Where \\(\\nu_j = \\hat z_j - \\sum_{i\\neq j} R_{ij} \\alpha_i \\mu_i\\), and \\(\\tau_j = 1 + \\sigma^{-2}_0\\).\nIt’s easy to see that the best choice for \\(q(b_l | \\gamma_l = 0)\\) is the prior, since all fo the data terms disappear, also noted here [(Titsias and Lázaro-Gredilla, n.d.)]\nAnd \\(q(\\gamma_j) = Bernoulli(\\alpha_j)\\), where \\(\\log \\left(\\frac{\\alpha_j}{1 - \\alpha_j}\\right) = \\hat z \\mu_j - \\frac{1}{2} \\left[\\mu^2_j + \\sigma^2_j + 2 \\mu_j \\sum_{i\\neq j} R_{ij} \\mu_i \\alpha_i \\right] + \\log\\left(\\frac{\\pi}{1 - \\pi}\\right)\\).\n\nSimulation\n\n\nCode\n#' @param q q(mu, var, alpha)\n#' @param R LD matrix-- assumes diag(R) = rep(1, p)\n#' @param tau0 prior effect variance\n#' @param prior_logit p-vector with prior log odds for gamma = 1\nrssvb <- function(zhat, q, R, tau0, prior_logit){\n  # unpack\n  mu <- q$mu\n  var <- q$var\n  alpha <- q$alpha\n\n  p <- length(zhat)\n  psi <- (R %*% (mu * alpha))[,1] # prediction\n  for(i in 1:p){\n    # remove effect of this variable\n    psi <- psi - R[i,] * (mu[i]*alpha[i])\n\n    # compute q(beta | gamma = 1)\n    nu <- zhat[i] - psi[i]\n    tau <- 1 + tau0\n    mu[i] <- nu/tau\n    var[i] <- 1/tau\n\n    # logit <- zhat[i] * mu[i]\n    #   - 0.5 * (psi[i] * mu[i] +  mu[i]^2 + var[i])\n    #   -0.5 * tau0 * (mu[i]^2 + var[i]) + prior_logit[i]\n    logit <- 0.5 * (mu[i]^2/var[i] + log(var[i]) + log(tau0)) + prior_logit[i]\n    alpha[i] <- 1/(1 + exp(-logit))\n\n    alpha[i]\n    psi <- psi + R[i,] * (mu[i]*alpha[i])\n  }\n  return(list(mu=mu, var=var, alpha=alpha))\n}\n\n\n\n\nCode\nsim_zscores <- function(n, p){\n  X <- logisticsusie:::sim_X(n=n, p = p, length_scale = 5)\n  R <- cor(X)\n  z <- rep(0, p)\n  z[10] <- 5\n  zhat <- (R %*% z)[,1] + mvtnorm::rmvnorm(1, sigma=R)[1,]\n  return(list(zhat = zhat, z=z, R=R))\n}\n\ninit_q <- function(p){\n  q = list(\n    mu = rep(0, p),\n    var = rep(1, p),\n    alpha = rep(1/p, p)\n  )\n  return(q)\n}\n\nrun_sim <- function(n = 100, p = 50, tau0=1, prior_logit = -3){\n  sim <- sim_zscores(n = n, p = p)\n  q <- init_q(p)\n  prior_logit <- rep(prior_logit, p)\n  for(i in 1:100){\n    q <- with(sim, rssvb(zhat, q, R, tau0, prior_logit))\n  }\n  \n  sim$q <- q\n  return(sim)\n}\n\n\nFor 100 independent simulations, we simulate \\(50\\) dependent \\(z\\)-scores. The true non-zero \\(z\\)-score is at index \\(10\\) with \\(\\mathbb E[\\hat z_{10}] = 5\\). However, over half the time, the VB approximation confidently selects another nearby feature.\n\n\nCode\nset.seed(10)\nsims <- purrr::map(1:100, ~run_sim(tau0=0.01))\nmax_idx <- purrr::map_int(1:100, ~which.max(sims[[.x]]$q$alpha))\n\nalpha10 <- purrr::map_dbl(1:100, ~sims[[.x]]$q$alpha[10])\nhist(alpha10)\n\n\n\n\n\nCode\ntable(max_idx)\n\n\nmax_idx\n 8  9 10 11 12 \n 4 47 36 11  2 \n\n\n\n\nMany small effects vs a few large effects\nThe interpretation of \\(\\sigma_0^2\\) depends a lot on how polygenic the trait is. Even though we only simulate one non-zero effect, if we use a prior \\(\\pi_1 >> 0\\) the model approaches a mean field approximation of ridge regression. Since ridge can estimate many small effects we get less shrinkage than if we enforce sparse architecture with \\(\\pi_1 \\approx 0\\).\n\n\nCode\nposterior_mean <- function(sim){\n  return((sim$R %*% (sim$q$mu * sim$q$alpha))[, 1])\n}\n\nshrinkage_plot <- function(sims, ...){\n  lims <- range(purrr::map(1:length(sims), ~sims[[.x]]$zhat))\n  plot(\n    sims[[1]]$zhat,\n    posterior_mean(sims[[1]]),\n    xlim = c(-4, 7),\n    ylim = c(-4, 7),\n    xlab = 'zhat',\n    ylab = 'posterior mean z',\n    ...\n  )\n  for(i in 1:100){\n    points(sims[[i]]$zhat, posterior_mean(sims[[i]]))\n  }\n  abline(0, 1, col='red')\n}\n\nset.seed(10)\n\nsim_sparse <- purrr::map(1:100, ~run_sim(tau0=0.1, prior_logit = -3))\nsim_poly <- purrr::map(1:100, ~run_sim(tau0=0.1, prior_logit = 3))\n\npar(mfrow=c(1,2))\nshrinkage_plot(sim_sparse, main='Sparse')\nshrinkage_plot(sim_poly, main='Polygenic')\n\n\n\n\n\n\n\n\n\n\nReferences\n\nTitsias, Michalis K, and Miguel Lázaro-Gredilla. n.d. “Doubly Stochastic Variational Bayes for Non-Conjugate Inference.”"
  },
  {
    "objectID": "research/polynomial_susie/index.html",
    "href": "research/polynomial_susie/index.html",
    "title": "Polynomial approximation SuSiE",
    "section": "",
    "text": "The sum of single effects (SuSiE) regression, is a regression with a SuSiE prior on the effects. For a Gaussian model with identity link, using a variational approximation that factorizes over single effects yields a fast coordinate ascent variational inference scheme which can be optimized by fitting a sequence of single effect regression (SERs). The computational simplicity of this result relies on (1) the variational approximation and (2) the fact the the log likelihood is quadratic in the regression coefficients.\nHere, we consider the more general case where the log-likelihood is a polynomial in the regression coefficients. We can approximate the log-likelihood of models with many choices of likelihood and link function with polynomials, so having a fast inference scheme for this case provides a uniform treatment of extensions of SuSiE to different observation models. We use the same variational approximation as in linear SuSiE. Approximating the log-likelihood as as polynomial is sufficient to"
  },
  {
    "objectID": "research/polynomial_susie/index.html#preliminaries",
    "href": "research/polynomial_susie/index.html#preliminaries",
    "title": "Polynomial approximation SuSiE",
    "section": "Preliminaries",
    "text": "Preliminaries\n\nPolynomial shift\nGiven the coefficients \\({\\bf a}\\) we would like to find the coefficients for the change of variable\n\\[\\phi_{\\bf a}(x + y) = \\phi_{{\\bf b}({\\bf a}, y)}(x)\\]\n\\[\n\\begin{aligned}\n\\phi_{\\bf a}(x + y)\n&= \\sum_{m=0}^M a_m(x + y)^m \\\\\n&= \\sum_{m=0}^M a_m \\sum_{k=0}^m {m \\choose k} x^k y^{m-k} \\\\\n&= \\sum_{k=0}^M \\left(\\sum_{m=k}^M a_m {m \\choose k}y^{m-k} \\right) x^k \\\\\n&= \\sum_{k=0}^M {\\bf b}({\\bf a}, y)_k x^k, \\quad  {\\bf b}({\\bf a}, y)_k := \\left(\\sum_{m=k}^M a_m {m \\choose k}y^{m-k} \\right)\n\\end{aligned}\n\\] Inspecting the expression for \\({\\bf b}({\\bf a}, y)_k\\) we can see that the new coefficient vector \\({\\bf b}({\\bf a}, y)\\) can be written compactly in matrix form\n\\[\n{\\bf b}({\\bf a}, y) = M(y) {\\bf a}, \\quad M_{ij} = {j \\choose i} y^{j -i}\\;  \\forall j \\geq i, 0 \\text{ otherwise}\n\\] Supposing \\(y\\) is random, we will also have need to compute the expected value of \\({\\bf b}({\\bf a}, y)\\). For \\(y \\sim p\\), \\(\\mathcal M(p) = \\mathbb E_p[M(y)]\\), we can compute these expected coefficients\n\\[\n{\\bf c}({\\bf a}, p) = \\mathcal M(p) {\\bf a}\n\\]\n\n\nPolynomial rescaling\n\\[\\phi_{\\bf a}(cx) = \\phi_{{\\bf d}({\\bf a}, c)}(x)\\]\n\\[\n\\sum_m a_m(c x)^m = \\sum_m (a_m c^m) x^m = \\sum_m {\\bf d}({\\bf a}, c)_m x^m\n\\]"
  },
  {
    "objectID": "research/polynomial_susie/index.html#model",
    "href": "research/polynomial_susie/index.html#model",
    "title": "Polynomial approximation SuSiE",
    "section": "Model",
    "text": "Model\n\\[\n\\begin{aligned}\ny_i | \\mu_i &\\sim f(\\mu_i, \\theta) \\\\\n\\mu_i &= g({\\bf x}^T_i \\beta ) \\\\\n\\beta &\\sim \\text{SuSiE}(L, \\{\\sigma_{0l}\\})\n\\end{aligned}\n\\] \\(f\\) is the observation model, parameterized by \\(\\mu\\) and \\(\\theta\\). \\(g\\) is a link function which maps the linear predictions \\(\\psi_i := {\\bf x}_i^T \\beta\\) to the parameter \\(\\mu_i\\).\n\nPolynomial approximation to the log-likelihood\nFor each observation \\(y\\), we can approximate the log-likelihood as a polynomial in the linear prediction. \\(f\\) is a polynomial of degree \\(M\\)\n\\[\n\\log p(y | \\psi) = \\log f(y | g(\\psi)) \\approx f(\\psi)\n\\]\n\\[\nf(\\psi) = \\sum_{m=0}^M a_m \\psi^m\n\\]\nThere are many ways to construct this approximation. At a high level, we want the approximation to be “good” (some measure of error between the approximate log-likelihood and the exact log-likelihood is small) at plausible values of \\(\\psi\\). So far, if have used a truncated Chebyshev series.\nWe write \\(f_i(\\psi)\\) as the polynomial approximation for \\(\\log p(y_i | \\psi)\\). We write it’s coefficients \\(\\left( a_m^{(i)} \\right)_{m \\in [M]}\\). To denote a polynomial with coefficients \\({\\bf a}\\) we will write \\(\\phi_{\\bf a}\\), e.g. \\(f(\\psi) = \\phi_{\\bf a}(\\psi)\\).\nWe have a polynomial approximation for each observations. Let \\({\\bf a}_i\\) denote the coefficients for observations \\(i\\). and \\(A\\) be the \\(n \\times m\\) matrix where each row corresponds to an observation."
  },
  {
    "objectID": "research/polynomial_susie/index.html#single-effect-regression-with-polynomial-approximation",
    "href": "research/polynomial_susie/index.html#single-effect-regression-with-polynomial-approximation",
    "title": "Polynomial approximation SuSiE",
    "section": "Single effect regression with polynomial approximation",
    "text": "Single effect regression with polynomial approximation\n\nSER prior\nThe SER prior \\(\\beta \\sim SER(\\sigma_0^2, \\pi)\\)\n\\[\n\\begin{aligned}\n\\beta = b\\gamma\\\\\nb \\sim N(0, \\sigma_0^2) \\\\\n\\gamma \\sim \\text{Mult}(1, \\pi)\n\\end{aligned}\n\\]\n\n\nPolynomial approximation for SER posterior\nThe SER follows from \\(p\\) univariate regressions\n\\[\n\\begin{aligned}\np(b | {\\bf y}, X, \\gamma=j, \\sigma^2_0)\n  &= p(b | {\\bf x}_j, {\\bf y}, \\sigma^2_0) \\\\\n  &\\propto p({\\bf y}, b| {\\bf x_j}, \\sigma^2_0) \\\\\n  &\\approx \\exp\\{ \\sum_i \\phi_{\\bf a_i}(x_{ij} b) + \\log p(b)\\}\n\\end{aligned}\n\\]\nSupposing \\(\\log p(b) \\approxeq \\phi_{\\bf \\rho}(b)\\) for some coefficients \\(\\rho\\), the unnormalized log posterior density can be written as a degree \\(M\\) polynomial with coefficients \\({\\bf f}(A, {\\bf x}_j, \\rho)\\) defined below:\n\\[\n\\begin{aligned}\n\\sum_i \\phi_{\\bf a_i}(x_{ij} b) + \\phi_{\\bf \\rho}(b) &= \\sum_i \\phi_{{\\bf d}({\\bf a}_i, x_{ij})}(b) + \\phi_{\\bf \\rho}(b), \\\\\n&= \\phi_{{\\bf f}(A, {\\bf x}_j, \\rho)}(b), \\quad {\\bf f}(A, {\\bf x}_j, \\rho) := \\sum_i {\\bf d}({\\bf a}_i, x_{ij}) + \\rho. \\\\\n\\end{aligned}\n\\]\nFor clarity we write \\({\\bf f}_j\\) for \\({\\bf f}(A, {\\bf x}_j, \\rho)\\). The posterior density is \\(q(b) = \\frac{1}{Z_j}\\exp\\{\\phi_{{\\bf f}_j}(b)\\}\\) where \\(Z_j = \\int_{\\mathbb R} \\exp\\{\\phi_{{\\bf f}_j}(b)\\} db\\). The normalizing constant may be computed numerically. Or, we can take a Gaussian approximation for \\(q(b)\\).\n\\[\n\\begin{aligned}\np(\\gamma = j | {\\bf y}, X, \\sigma^2_0)\n&\\propto p({\\bf y} | {\\bf x}_j, \\gamma = j) \\pi_j \\\\\n&= \\left(\\int p({\\bf y}, b | {\\bf x}_j, \\gamma = j) db\\right) \\pi_j \\\\\n&= \\left(\\int \\exp\\{\\sum_i \\phi_{{\\bf a}_i} (x_{ij} b) + \\phi_{\\rho}(b) \\}\\right) \\pi_j \\\\\n&= \\left(\\int \\exp\\{\\phi_{{\\bf f}_j}(b) \\} db\\right) \\pi_j = Z_j \\pi_j \\\\\n\\end{aligned}\n\\]\n\\[\\alpha_j := q(\\gamma = j) = \\frac{Z_j \\pi_j}{\\sum_k Z_k \\pi_k}\\]\n\n\nVariational objective\nIt will be useful to write out the variational objective for the SER:\n\\[\nF_{SER}(q| {\\bf y}, X, \\sigma^2_0) = \\mathbb E_q[\\log p({\\bf y} | X, b, \\gamma, \\sigma^2_0)] - KL[q || p].\n\\] The exact posterior maximizes \\(F_{SER}\\), that is\n\\[\np_{post} = \\arg\\max_q F_{SER}(q| {\\bf y}, X, \\sigma^2_0).\n\\]\nWe can approximate the variational objective by substituting our polynomial approximation\n\\[\n\\hat F_{SER}(q| A, X, \\sigma^2_0) = \\mathbb E_q \\left[\n\\sum_i \\phi_{\\exp\\{{\\bf a}_i}(X (b\\gamma)) + \\log p(b | \\sigma^2_0) + \\log p(\\gamma | \\pi)\\}\n\\right]  - KL[q || p].\n\\]\nAs discussed above, \\(q(b | \\gamma) \\propto \\exp\\{\\phi_{{\\bf f}_j}(b)\\}\\) and \\(q(\\gamma = j) = \\frac{Z_j}{\\sum_k Z_k}\\).\nLet \\(SER(A, X, \\sigma^2_0, \\pi)\\) be a function that returns \\(q\\).\n\n\nPolynomial to Gaussian\nFor the extention to SuSiE, we are going to need to compute \\(\\mu_j^k = \\mathbb E[b^k | \\gamma = j] = \\int_{\\mathbb R} b^k \\exp\\{\\phi_{{\\bf f}(A, {\\bf x}_j, \\rho)}(b)\\} db\\). We can ensure that this function is integrable by making selecting polynomial approximations \\({\\bf a}_i\\) which are of even degree and \\(a_{iM} < 0\\). This ensures that the leading coefficient of \\({\\bf f}\\) is negative and that \\(e^{\\bf f}\\) decays. However, this integral may not be available analytically in general.\nHowever, if \\(\\phi_{\\bf f}\\) is degree \\(2\\), this is a special case where the moments of a Gaussian distribution can be computed analytically. This motivates us to “approximate the approximation”.\nWe take a Laplace approximation to \\(\\phi_{\\bf f}\\). Currently I approximate the degree \\(M\\) polynomial \\(\\phi_{\\bf f}\\) with the degree \\(2\\) polynomial by the following: first we search for the mode of \\(\\phi_{\\bf f}\\) e.g. by finding the roots of \\(\\phi_{\\bf f}^\\prime\\) and selecting the one that maximizes \\(\\phi_{\\bf f}\\) (although there may be cheaper ways to do this?). Then, we take a second order Taylor series expansion around the mode.\nIt’s worth noting that these two levels of approximation result in a different strategy than taking a “global” second order approximation directly, or a Laplace approximation \\(\\log p(y, \\beta) \\rightarrow {\\bf f} \\rightarrow {\\bf g}\\). “Laplace approximation of a degree-\\(M\\) polynomial approximation” is a local approximation around the approximate mode. Directly applying Laplace approximation would result in a local approximation around the exact mode. We would prefer the latter except that (1) the polynomial approximation provides the computational simplifications we need to extend to SuSiE and (2) finding the posterior mode and it’s second derivative may be more expensive in the general case."
  },
  {
    "objectID": "research/polynomial_susie/index.html#polynomial-susie",
    "href": "research/polynomial_susie/index.html#polynomial-susie",
    "title": "Polynomial approximation SuSiE",
    "section": "Polynomial SuSiE",
    "text": "Polynomial SuSiE\n\nVariational approximation\nWe will use the variational approximation that factorizes over single effects. This is the same variational approximation used in linear SuSiE\n\\(\\beta_l = \\gamma_l b_l\\) \\[q(\\beta_1, \\dots, \\beta_L) = \\prod_l q(\\beta_l)\\]\n\n\nVariational objective\nWe want to find \\(q\\) that optimizes the evidence lower bound (ELBO) \\(F\\)\n\\[\nF_{\\text{SuSiE}}(q| {\\bf y}, X, \\{\\sigma^2_{0l}\\}, \\{\\pi_l\\}) = \\mathbb E_q[\\sum \\log p(y_i | {\\bf x}_i, \\beta)] - \\sum KL[q(\\beta_l) || p(\\beta_l | \\sigma^2_{0l}, \\pi_l)]\n\\]\nEvaluating the expectations \\(\\mathbb E_q[\\log p(y_i | {\\bf x}_i, \\beta)]\\) is generally hard, we make progress by substituting the polynomial approximation, we denote the approximate ELBO \\(\\hat F\\).\n\\[\n\\begin{aligned}\n\\hat F_{\\text{SuSiE}}(q| A, X, \\{\\sigma^2_{0l}\\}, \\{\\pi_l\\})\n&= \\mathbb E_q[\\sum_i \\phi_{{\\bf a}_i}({\\bf x}_i^T \\beta)]\n- \\sum KL[q(\\beta_l) || p(\\beta_l | \\sigma^2_{0l}, \\pi_l)]\n\\end{aligned}\n\\]\n\n\nCoordinate ascent in the approximate ELBO\nWith \\(\\psi_{li} := {\\bf x}_i^T \\beta_l\\) and \\(\\psi_i = {\\bf x}_i^T \\beta = \\sum {\\bf x}_i^T \\beta_l = \\sum_l \\psi_{li}\\) and \\(\\psi_{-li} = \\psi_i - \\psi_{li}\\).\nConsider the approximate ELBO just as a function of \\(q_l\\)\n\\[\n\\begin{aligned}\n\\hat F_{\\text{SuSiE},l}(q_l| A, X,  q_{-l}, \\sigma^2_{0l}, \\pi_l)\n= \\mathbb E_{q_l} \\left[\n    \\mathbb E_{q_{-l}} \\left[\n      \\sum_i \\phi_{{\\bf a}_i}(\\psi_{li} + \\psi_{-li})\n    \\right]\n\\right] - KL \\left[p(\\beta_l) || q(\\beta_{-l})\\right] + \\kappa\n\\end{aligned}\n\\]\nWhere \\(\\kappa\\) is a constant that does not depend on \\(q_l\\) (note: it does depend on \\(\\{\\sigma^2_{0l}\\}, \\{\\pi_l\\}\\), but they are not written in the arguments to \\(\\hat F_{\\text{SuSiE},l}\\). We will need to evaluate \\(\\mathbb E_{q_{-l}}[\\phi_{\\bf a_i}(\\psi_{li} + \\psi_{-li})]\\) for each \\(i\\). Focusing on a single observation\n\\[\n\\mathbb E_{q{-l}} \\left[\\phi_{\\bf a_i}(\\psi_{li} + \\psi_{-li})\\right]\n= \\mathbb E_{q_{-l}} \\left[\\phi_{{\\bf b}({\\bf a}, \\psi_{-li})}(\\psi_{li})\\right] = \\phi_{{\\bf c}({\\bf a_i}, q_{-l})}(\\psi_{li})\n\\]\nWe will write \\({\\bf b}_{li} = {\\bf b}({\\bf a}, \\psi_{-li})\\) and \\({\\bf c}_{li} = {\\bf c}({\\bf a_i}, q_{-l})\\), Indicating that they are the coefficient for an \\(M\\) degree polynomial in \\(\\psi_{li}\\).\nRevisiting the approximate ELBO we see\n\\[\n\\begin{aligned}\n\\hat F_{\\text{SuSiE},l}(q_l| A, X,  q_{-l})\n& = \\mathbb E_{q_l} \\left[\n    \\mathbb E_{q_{-l}} \\left[\n      \\sum_i \\phi_{{\\bf a}_i}(\\psi_{li} + \\psi_{-li})\n    \\right]\n\\right] - KL \\left[q(\\beta_l) || p(\\beta_{-l} | \\sigma^2_{0l}, \\pi_l)\\right] + \\kappa \\\\\n&= \\left(\\mathbb E_{q_l} \\left[\n  \\sum_i \\phi_{{\\bf c}_{li}}(\\psi_{li})\n\\right] - KL \\left[q(\\beta_l) || p(\\beta_{-l} | \\sigma^2_{0l}, \\pi_l)\\right] \\right)  + \\kappa \\\\\n&= \\hat F_{\\text{SER}}(q_l | C_{l}, X, \\sigma^2_{0l}, \\pi_l) + \\kappa\n\\end{aligned}\n\\]\nWe can see that with respect to the \\(q_l\\) the SuSiE approximate ELBO is equal, up to a constant, to the SER approximate ELBO, for a new set of coefficients \\(C_l\\), which depend on \\(q_{-l}\\). It follows that the coordinate ascent update for \\(q_l\\) is achieved by fitting the polynomial approximate SER with coefficients \\(C_l\\).\nThis is analagous to how we fit linear SuSiE by a sequence of SERs: we can fit the polynomial approximate SuSiE with a sequence of polynomial approximate SERs. Rather than “residualizing”, we fit the polynomial approximate SER with coefficients \\(C_l\\).\nThe crux of this approach is having a fast way to compute \\(C_l\\).\n\n\nUpdate Rule\nAt iteration \\(t\\) we have the current variational approximation\n\\[\nq^{(t)} = \\prod q_l^{(t)}\n\\]\nDefine \\[\n{\\bf c}_i^{(0)} := \\mathcal M(\\psi_i, q^{(0)}) {\\bf a}_i\n\\]\nNote that \\(\\mathcal M(\\psi_i, q^{(0)})\\) gives the expected shift matrix that removes the entire linear prediction, so that \\({\\bf c}_{i0}^{(0)} = \\phi_{\\bf c_i^{(0)}}(0) = \\mathbb E_{q^{(0)}}[\\log p(y_i | \\psi_i)]\\).\nAt iteration \\(t\\), our coordinate ascent updates require \\({\\bf c}_{li}^{(t)} = \\mathcal M(\\psi_{li}, q_{-l}^{(t)})\\). However, we know that:\n\\[\n{\\bf c}_{i}^{(t)} = \\mathcal M(\\psi_{li}, q_l^{(t)}){\\bf c}_{li}^{(t)} \\implies {\\bf c}_{li}^{(t)} = \\mathcal M(\\psi_{li}, q_l^{(t)})^{-1}{\\bf c}_{i}^{(t)}\n\\]\nWe can use \\(C_l^{(t)}\\) to compute \\(q_l^{(t+1)}\\) and then\n\\[\n{\\bf c}_{i}^{(t)} \\leftarrow \\mathcal M(\\psi_{li}, q_l^{(t + 1)}){\\bf c}_{li}^{(t)}\n\\]\nso, by solving a triangular systems, and multiplying by upper triangular matrices \\(\\mathcal M\\) we can “efficiently” move between the coefficient representations needed for each SER update. I worry that \\(\\mathcal M_l\\) may be poorly conditioned resulting in numerical instability, but I have not seen it in toy examples yet.\n\n\nAlgorithm\nInitialize \\(C^{(0)} = A\\) and \\(q^{(0)} = \\prod q_l^{(0)}\\) such that \\(\\mathbb E[\\psi_l^k] = 0\\;\\; \\forall l, k\\).\nFor \\(t = 1, 2, \\dots\\):\n\n\\(C^{(t)} = C^{(t-1)}\\)\nFor each \\(l \\in [L]\\):\n\nCompute \\(C_l^{(t)}\\) by \\({\\bf c}_{li}^{(t)} \\leftarrow \\mathcal M(\\psi_{li}, q_l^{(t-1)})^{-1} {\\bf c}_i^{(t)}\\) for \\(i \\in [n]\\)\n\\(q_l^{(t)} \\leftarrow \\text{SER}(C_l^{(t)}, X, \\sigma_{0l}^2, \\pi_l)\\)\nUpdate \\(C^{(t)}\\) by \\({\\bf c}_i^{(t)} \\leftarrow \\mathcal M(\\psi_{li}, q^{(t)}_l) {\\bf c}^{(l)}_i\\)\n\n\n\n\nComplexity\nLet’s break down the complexity of the inner loop (that is, updating \\(q_l\\) and the requisite computations for the next run of the loop).\nComputing coefficients \\(C_l\\): Computing \\({\\bf c}_{li}\\) is \\(O(M^2)\\) to construct \\(\\mathcal M_l\\) and to solve the triangular system \\({\\bf c}_i = \\mathcal M_l(\\psi_{li}, q_l) {\\bf c}_{li}\\). This is per observations, so computing \\(C_{l}\\) is \\(O(M^2m)\\)\nFitting SER Once we computed the coefficients we update the SER in \\(O(Mnp)\\). We just sum the coefficients to construct the polynomial posterior for each variable). Then \\(O(pM^3)\\) to perform root finding and make the Gaussian approximation for \\(q_l\\).\nComputing moments We need to evaluate the moments \\(\\mathbb E[\\psi_{li}^k]\\) for \\(k=1, \\dots, M-1\\). \\(O(Mnp)\\)? This is fast if \\(q_l\\) is Gaussian but the scaling factor may be large if we need to compute numerically. If we are making the Gaussian approximation, that costs \\(O(M^3p)\\).\nUpdating \\(C\\) Again \\(O(nM^2)\\). We need to construct \\(\\mathcal M (\\psi_{li}, q_l)\\) and compute the matrix-vector product \\(\\mathcal M_l {\\bf c}\\).\nTotal \\(O(M^2n + Mnp + M^3p)\\)"
  },
  {
    "objectID": "research/constrained_multinomial_stickbreaking/index.html",
    "href": "research/constrained_multinomial_stickbreaking/index.html",
    "title": "Constrained multinomial stick-breaking",
    "section": "",
    "text": "Code\nsigmoid <- function(x){1/(1 + exp(-x))}\n\ntilde_pi2pi <- function(tpi){\n  tmp <- c(1, head(cumprod(1 - tpi), -1))\n  pi <- tmp * tpi\n  pi <- c(pi, (1 - sum(tmp * pi)))\n  return(pi)\n}\n\nmake_pi <- function(K, b0, b, x){\n  psi <-  do.call(cbind, purrr::map(b0, ~b + x + .x))\n  tilde_pi <- sigmoid(psi)\n  tpi <- tilde_pi[12, ]\n  pi <- do.call(rbind, purrr::map(1:nrow(tilde_pi), ~tilde_pi2pi(tilde_pi[.x,])))\n  return(pi)\n}\n\nplot_pi <- function(pi, idx, x){\n  par(mfrow = c(1, length(idx)))\n  K <- ncol(pi)\n  for(i in idx){\n    plot(1:K, pi[i,], type = 'b', xlab = 'K', ylab = 'prob', main=paste0('x = ', x[i]))\n  }\n}\n\n\n\nShared\n\\(\\psi_k \\equiv \\psi\\; \\forall\\  k \\in[0, K-1]\\)\n\n\nCode\nK <- 20\nb0 <- rep(0, K)\nb <- 1\nx <- seq(-3, 3.2, by=0.2)\npi <- make_pi(K, b0, b, x)\nplot_pi(pi, c(6, 11, 21, 26), x)\n\n\n\n\n\n\n\nFixed prediction, seperate intercept\n\n\nCode\nK <- 10\nb0 <- rep(0, K)\nb <- 1\nx <- seq(-3, 3, by=0.2)\npi <- make_pi(K, b0, b, x)\nplot_pi(pi, c(6, 11, 21, 26), x)\n\n\n\n\n\n\n\nCode\nK <- 10\nb0 <- rnorm(10)\nb <- 1\nx <- seq(-3, 3, by=0.2)\npi <- make_pi(K, b0, b, x)\nplot_pi(pi, c(6, 11, 21, 26), x)\n\n\n\n\n\n\n\nCode\nK <- 10\nb0 <- 1:K\nb <- 1\nx <- seq(-5, 5, by=0.1)\npi <- make_pi(K, b0, b, x)\nplot(pi[40,])\n\n\n\n\n\n\n\nCode\nK <- 10\nb0 <- rnorm(K)\nb <- 1\nx <- seq(-5, 5, by=0.1)\npi <- make_pi(K, b0, b, x)\n\npar(mfrow = c(2, 3))\nplot(pi[10,])\nplot(pi[20,])\nplot(pi[30,])\nplot(pi[40,])\nplot(pi[50,])\nplot(pi[60,])"
  },
  {
    "objectID": "research/multinomial_stickbreaking/index.html",
    "href": "research/multinomial_stickbreaking/index.html",
    "title": "Multinomial stick-breaking",
    "section": "",
    "text": "We’re interested in modelling multinomial or categorical data in the case where the probability of each category depends on side information. For \\(\\pi: \\mathcal X \\rightarrow \\Delta^{K-1}\\)\n\\[\n{\\bf y} \\sim \\text{Multinomial}(n, {\\bf \\pi}({\\bf x})) \\\\\n\\]\nCommonly \\(\\pi({\\bf x})\\) is written as a composition \\(\\pi = \\sigma \\circ \\eta\\), where \\(\\sigma: \\mathbb R^K \\rightarrow \\Delta^{K-1}\\) is the softmax functions defined element wise as \\(\\sigma({\\bf \\eta})_i = \\left(\\frac{e^{\\eta_i}}{\\sum_{j=1}^K e^{\\eta_j}}\\right)\\), and \\(\\eta:\\mathcal X \\rightarrow \\mathbb R^K\\) is some other function mapping the covariates \\({\\bf x}\\) to a set of unormalized log probabilities.\nThe trouble with this formulation is it is not easy to express uncertainty in the map \\(\\eta\\). As a simple example consider multinomial linear regression where \\(\\eta(z)_k = \\beta_k^T z\\) for some \\(\\beta_k \\in \\mathbb R^d\\). \\(\\pi = \\sigma \\circ \\eta\\) is differential, and point estimates of \\(B =\\{\\beta_k\\}_{k=1, \\dots, K}\\) could be obtained through gradient based optimization. In contrast if we take a Bayesian approach and specify a prior on \\(\\beta_k \\sim g_k\\; k \\in [K]\\) obtaining the posterior distribution over \\(B\\) involves evaluating a nasty integral of the soft max.\n\\[\n\\int_{B} \\sigma(\\eta(z ; B)) dB\n\\]\nThere is plenty of work on bounding softmax with functions that are easier to integrate (Bouchard, n.d.; Titsias RC AUEB 2016), but it is hard problem to get anlytic bounds that are easy to work.\nThere is also quite a bit of work developing bounds for the sigmoid function (softmax with \\(K=2\\), usually people describe softmax as a generalization of sigmoid to \\(K > 2\\)). In particular, techniques for constructing local approximations are popular in variational inference (Jaakkola and Jordan, n.d.; Saul and Jordan 1998). These local approximations are tight at a point, but the quality of the bound decays as you get far from that point. Thus, these approximation techniques require selecting/optimizing at what point the bound is tight.\nWe’re operating under the assumption that it is easier to construct good bounds for the sigmoid function compared to the softmax function. We are going to explore a construction of the Categorical/Multinomial distribution that let us utilize these bounds."
  },
  {
    "objectID": "research/multinomial_stickbreaking/index.html#multinomial-stick-breaking",
    "href": "research/multinomial_stickbreaking/index.html#multinomial-stick-breaking",
    "title": "Multinomial stick-breaking",
    "section": "Multinomial stick breaking",
    "text": "Multinomial stick breaking\nThe the multinomial logit construction \\(\\eta\\) is a set of unnormalized log probabilities This is not the only way to construct a multinomial distribution. We can also use a stick breaking construction. In stick breaking we start with a “stick” of length \\(1\\). At the first step we break off a fraction of the stick \\(p_1\\). The remainder of the stick is now length \\(1 - p_1\\). At each successive step we break off a fraction of the remaining stick. After \\(K-1\\) breaks we have broken the stick into \\(K\\) pieces, giving a discrete probability distribution over \\(K\\) categories. Clearly, we can use this process to construct and distribution \\(\\pi\\) over \\(K\\) categories where\n\\[\n\\begin{aligned}\n\\pi_1 &= p_1 \\\\\n\\pi_k &= p_k \\prod_{j < k}(1 - p_k)\n\\end{aligned}\n\\]\nNoting that \\(\\left(1 - \\sum_{j < k} \\pi_j \\right)\\) is the length of the remaining stick after \\(k-1\\) breaks, we can also write\n\\[\n\\begin{aligned}\n\\pi_k &= p_k \\left(1 - \\sum_{j < k} \\pi_j \\right)\n\\end{aligned}\n\\]\nIn the stick breaking construction, \\(\\nu_k,\\; k \\in[K-1]\\) will be a set of log odds such that \\(p_k = \\sigma(\\nu_k)\\) gives the proportion of the stick broken off at step \\(k\\). Using the stick breaking constructiong we can write the multinational pmf as a product of binomial pmfs.\n\\[\n\\text{Multinomial}({\\bf y}; n, \\pi) = \\prod_{k=1}^{K-1} \\text{Binomial}(y_k; n_k, p_k)\n\\]\nWhere \\(n_k = n - \\sum_{j < k} y_j\\) counts the number of remaining trials, conditional on the first \\(k-1\\) draws. This constructing is not new, it has been proposed by several authors (Khan et al. 2012; Linderman, Johnson, and Adams, n.d.).\nTo do multinomial regression we will write \\(\\nu_k = \\beta_k^T {\\bf z}\\). \\(nu_k\\) gives the log odds of selecting category \\(k\\) given that we did not select category \\(1, \\dots, k-1\\)."
  },
  {
    "objectID": "research/multinomial_stickbreaking/index.html#stick-breaking-for-variational-inference",
    "href": "research/multinomial_stickbreaking/index.html#stick-breaking-for-variational-inference",
    "title": "Multinomial stick-breaking",
    "section": "Stick breaking for variational inference",
    "text": "Stick breaking for variational inference\nThe stick breaking construction is particular useful for variational inference. The multinomial log likelihood can be written as a sum of \\(K-1\\) terms, each a binomial log-likelihood. By selecting a variational approximation where the \\(\\nu_k\\) factorize, the variational objective can be optimized in an embarrassingly parallel fashion– the multinomial regression reduces to a set of \\(K-1\\) independent binomial regression problems. Each of these problems still requires additional approximation of the sigmoid function for tractable inference, but these can be dealt with more easily."
  },
  {
    "objectID": "research/multinomial_stickbreaking/index.html#a-distribution-of-pi",
    "href": "research/multinomial_stickbreaking/index.html#a-distribution-of-pi",
    "title": "Multinomial stick-breaking",
    "section": "A distribution of \\(\\pi\\)",
    "text": "A distribution of \\(\\pi\\)\nWhile stick breaking can be used to construct any discrete distribution, we should take note that the distribution on \\(\\pi\\) is dependent on the distribution we specify for the breakpoints and\nThe Dirichlet \\(Dir((\\alpha_1, \\dots, \\alpha_K))\\) can be constructed through stick breaking, where the break points are\n\\[p_k \\sim Beta(\\alpha_k, \\sum_{j > k } \\alpha_j)\\]\nAgain \\(\\pi_1 = p_1\\), and \\(p_k = (1 - \\sum_{j < k} \\pi_j) p_k\\). If \\(\\alpha_i = \\alpha\\; \\forall i \\in [K]\\) then then the Dirichlet is said to be symmetric– permuting category labels won’t change the likelihood of the sample. Notice that in this case \\(p_k \\sim Beta (\\alpha, (K- k) \\alpha)\\). We should expect to break off smaller fractions of the stick for small \\(k\\) than for large \\(k\\). This makes sense. A necessary condition for the Dirichlet to be exchangeable is that the stick lengths have the same marginal distribution. In order for the stick lengths to have the same marginal distribution, at each successive step we need to balance out the fact that the stick is getting shorter by taking larger fraction of the stick at each step (ultimately \\(\\mathbb E[p_{K-1}] = \\frac{1}{2}\\)).\nIn the code below we simulate the Dirichlet distribution using stick breaking with a Beta distribution. We see that across 10000 simulations each category is equally likely to show up on top.\n\n\nCode\n#' Sample from a Dirichlet distribution using the stick breaking construction\ndirichlet_from_beta_stick_breaking <- function(alpha, K){\n  if(length(alpha) == 1){ \n    alpha <- rep(alpha, K)\n  }\n  beta <- rev(cumsum(rev(alpha))) - alpha # sum {j < k} \\alpha_j\n  p <- rbeta(K, alpha, beta)\n  tmp <- c(1, head(cumprod(1 - p), -1))\n  pi <- c(p * tmp)\n  return(pi)\n}\n\n# each component equally likely to have the most probability mass\ntable(purrr::map_int(1:10000, ~which.max(\n  dirichlet_from_beta_stick_breaking(1, 4))))\n\n\n\n   1    2    3    4 \n2458 2539 2492 2511 \n\n\nTODO: sample for \\(K=3\\)\nQ: What distribution of \\(\\nu_k\\) would give an exchangeable distribution for \\(\\pi\\) (basically, what is the stick-breaking construction for a symmetric Dirichlet?)"
  },
  {
    "objectID": "research/multinomial_stickbreaking/index.html#ordering-of-the-categories",
    "href": "research/multinomial_stickbreaking/index.html#ordering-of-the-categories",
    "title": "Multinomial stick-breaking",
    "section": "Ordering of the categories",
    "text": "Ordering of the categories\nSuccessive categories seem to have less and less information, as \\(n_k \\leq n_j\\) for \\(k > j\\). It seems odd that permuting the category labels would change how certain we are about each \\(\\nu_k\\). Can we make sense of this?"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research Notes",
    "section": "",
    "text": "Constrained multinomial stick-breaking\n\n\n\n\n\nMultinomial stickbreaking is an easy\n\n\n\n\n\n\nApr 1, 2023\n\n\nKarl Tayeb\n\n\n\n\n\n\n\n\nImplementation polynomial approximation VB\n\n\n\n\n\nRough implementation of of approximate VB regression, where we approximate the data likelihood with a polynomial. We implement a mean field gaussian variational approximation. We demonstrate the method on Gaussian linear model (no approximation), and Bernoulli-logit (logistic regression) and Poisson-log (poisson regression) models with varying approximation degree.\n\n\n\n\n\n\nMar 15, 2023\n\n\nKarl Tayeb\n\n\n\n\n\n\n\n\nMode seeking in mean field VB for RSS + sparse prior\n\n\n\n\n\nUsing polynomial approximations to perform Bayesian regression\n\n\n\n\n\n\nApr 1, 2023\n\n\nKarl Tayeb\n\n\n\n\n\n\n\n\nMultinomial stick-breaking\n\n\n\n\n\nExploration of multinomial stickbreaking\n\n\n\n\n\n\nMar 19, 2023\n\n\nKarl Tayeb\n\n\n\n\n\n\n\n\nPolynomial approximation SuSiE\n\n\n\n\n\nWe extend the sum of single effects regression to support arbitrary likelihood and link function by representing (approximately) the log likelihood of each observations as a polynomial in the linear prediction. Once this approximation is made we can treat inference in multiple likelihoods with a uniform inference procedure. Furthermore, we can make the polynomial approximation arbitrarily precise by increasing the degree of the approximation.\n\n\n\n\n\n\nMar 15, 2023\n\n\nKarl Tayeb\n\n\n\n\n\n\n\n\nPolynomial approximation for variational Bayes\n\n\n\n\n\nUsing polynomial approximations to perform Bayesian regression\n\n\n\n\n\n\nMar 15, 2023\n\n\nKarl Tayeb\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Karl Tayeb’s Website",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  }
]