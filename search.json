[
  {
    "objectID": "research/polynomial_approximation/index.html",
    "href": "research/polynomial_approximation/index.html",
    "title": "Polynomial approximation for variational Bayes",
    "section": "",
    "text": "$$"
  },
  {
    "objectID": "research/polynomial_approximation/index.html#overview",
    "href": "research/polynomial_approximation/index.html#overview",
    "title": "Polynomial approximation for variational Bayes",
    "section": "Overview",
    "text": "Overview\nWe propose approximating Bayesian linear regression problems by replacing the conditional likelihood of each observation with a polynomial approximation designed to be close to the true likelihood on an interval, and a lower bound for values far outside the interval. If the log density of our prior distribution can also be expressed (or approximated) as a polynomial the exact posterior of this polynomial approximation is also a polynomial– i.e. conjugate.\nThis is the main benefit of our approach: inference boils down to simple manipulations of the polynomial coefficients, which can be performed in parallel and with a set of simple linear algebra operations (matrix multiplication, solving a triangular system, etc). There may exist more computationally efficient algorithms for carrying out these operations, which we should explore further.\nThis approach can be generalized to many likelihood (e.g. Binomial, Poisson) with various choices of link function (which relate the regression predictions to the distribution parameters)– at least to the extent that we can develop a good polynomial approximation for each combination of likelihood and link.\nTake a look at this rough implementation."
  },
  {
    "objectID": "research/polynomial_approximation/index.html#global-polynomial-approximations",
    "href": "research/polynomial_approximation/index.html#global-polynomial-approximations",
    "title": "Polynomial approximation for variational Bayes",
    "section": "Global polynomial approximations",
    "text": "Global polynomial approximations\n\nLocal approximations, and trouble with SuSiE\nThis approach was motivated by our work to develop a good variational approximation for logistic SuSiE, that is a logistic regression with the sum of single effects (SuSiE) [(susie?)]. For Gaussian linear models, using a variational approximation that factorizes over single effects produces a lightning fast Bayesian variable selection method. Rather than exploring each configuration of non-zero effect variables, this approximation allows us to update our approximating distribution for one effect while marginalizing over all other effects. Inference in the (Gaussian, linear) SuSiE model reduces to a sequence of embarrassingly parallel univariate linear regression problems.\nIn the case of Gaussian model with identify link (linear regression), the variational approximation is sufficient for computation to simplify, because the log likelihood is quadratic in the regression coefficients \\(\\beta\\). However, the Bernoulli likelihood with a logistic link function (logistic regression), and indeed for many other likelihood-link combinations of interest, do not enjoy the algebraic simplicity of a Gaussian model with identity link (linear regression). The main issue is that it is difficult to marginalize over the other effects. Even if the approximate posterior factorizes, the likelihood computation can combine single effects in a way that complicates the marginalization step.\nWe’ve attempted using local quadratic approximations to the likelihood. The idea is to construct a quadratic lower bound to the likelihood that is “good” in a neighborhood that we care about (contains most of the posterior mass). This is a popular technique for applying variational Bayes to logistic regression (see Jaakkola and Jordan, Polya-Gamma augmentation), and indeed these sorts of “local” approximation schemes can be generalized to super-Gaussian likelihoods [(Galy-Fajou, Wenzel, and Opper 2020)].\nHowever, we find that the local approximation techniques are not well suited to variable selection techniques like SuSiE. In order to use the local approximation techniques, we must optimize a set of variational parameters that essentially dictate where the approximation is good. You only get to select one local approximation per observation. Holding these variational parameters fixed, our inference will systematically favor posterior distributions that place more mass in a neighborhood of these variational parameters. The strong coupling between the local variational parameters and optimal variational approximation make it difficult to navigate the optimization surface via coordinate ascent.\nAdditionally, uncertainty quantification is an important output to SuSiE and other Bayesian variable selection methods. Due to the nature of the local variational approximations we may tend to see a “winner take all” scenario. Assuming we find a good local optimum, the local variational parameters will be tuned to be consistent with the most likely variable configurations. In turn, the objective function will be a tighter lower bound in these regions, and the approximating distribution will place more probability mass on these configurations.\nNote: the Laplace approximation is also a “local” approximation, since we construct a quadratic approximation of the joint log likelihood around the MAP estimate. It is also closely related Gaussian variational approximations.\n\n\nGlobal approximations\nRather than find a “local” approximation which has variational parameters that must be tuned at each iteration, we propose finding a global approximation to the likelihood which is easier to work with.\n“Easier to work with”, in the context of SuSiE means that we can write \\(f(\\psi_l) =\\mathbb E_{q_{-l}}[\\log p(y | \\psi_l + \\psi_{-l})]\\), where \\(f(\\psi_l; q_{-l})\\) is easy to evaluate/optimize over. Notice that if we approximate \\(\\log p(y | \\psi) \\approx f(\\psi) = \\sum_k c_k \\psi^k\\) then taking the expectation of \\(f\\) over \\(q_{-l}\\) results in a polynomial in \\(\\psi_l\\).\n\\[\nE_{q_{-l}}[f(\\psi_l + \\psi_{-l})] = \\hat f(\\psi_l; q_{-l}) = \\sum \\hat c_k(q_{-l}) \\psi_l^k.\n\\]\nHere \\(\\hat c_k(q_{-l})\\) are a transformation of the original coefficients \\(c_k\\) that are achieved by (1) expanding the original polynomial in terms of \\(\\psi_l\\) and \\(\\psi_{-l}\\), (2) marginalizing over \\(\\psi_{-l}\\) w.r.t \\(q_{-l}\\) and then (3) regrouping terms to get coefficients of a polynomial in \\(\\psi_l\\).\nWhile the local variational approximations are only good around a point specified by the variational parameter, we can construct a “global” polynomial approximation that is good on any interval if we allow the degree of the polynomial to be sufficiently high. While we sidestep the issue of needing to tune the local variational parameters, we replace it with the need to select an interval that we care to approximate. There is a trade off here– for a fixed error tolerance, wider intervals will require higher degree approximations to bound the error."
  },
  {
    "objectID": "research/polynomial_approximation/index.html#polynomial-representation",
    "href": "research/polynomial_approximation/index.html#polynomial-representation",
    "title": "Polynomial approximation for variational Bayes",
    "section": "Polynomial representation",
    "text": "Polynomial representation\n\nRepresenting functions with polynomial series\nLet \\(\\set{P_k}_{k=1}^{\\infty}\\) be a sequence of polynomials that form a basis for continuous functions on an interval, e.g. \\([-1, 1]\\) (note: that through a change of variable we can stretch this interval to any interval we want). So that for any \\(f: [-1,1] \\\\rightarrow \\mathbb R\\) there are \\(\\set{c_k}\\) such that\nWe’ll assume that \\(P_k\\) are ordered by increasing degree, and that \\(deg(P_k) \\leq k\\).\n\n\nChebyshev polynomials\nThese are a family of polynomials on \\([-1, 1]\\) defined by\n\\[T_n(\\cos(\\theta)) = \\cos(n \\theta)\\]\nThey can also be obtained by the reccurrence\n\\[\n\\begin{aligned}\n  T_0(x) &= 1 \\\\\n  T_1(x) &= x \\\\\n  T_{n+1}(x) &= 2x T_n(x) - T_{n-1}(x)\n\\end{aligned}\n\\] The Chebyshev polynomials are orthogonal to eachother, and form a basis for ~a certain family of function~ on the interval \\([-1, 1]\\), so that \\(f(x) = \\sum_{k=0}^\\infty c_k T_k(x)\\). This means that we can get the coefficients by evaluating an inner product\n\\[\n\\langle f, T_k \\rangle = c_k ||T_k||^2\n\\] Through a change of variable, we can approximate functions on any interval. We can obtain a \\(K+1\\) degree polynomial approximation by computing the first \\(K\\) coefficients \\((c_k)_{k=0}^K\\). Each coefficient \\(c_k\\) is effectively computed via a \\(k+1\\) point quadrature (Note: it looks like there is a simple rule for computing the coefficients as linear combinations of \\(f\\) evauated at a set of “nodes”, this looks very closely related to the quadrature rule, but I want to figure this out in more detail)\n\n\nConversion between polynomial basis\nSuppose we have a polynomial basis \\(\\{A_k\\}\\) and \\(\\{B_k\\}\\). Let \\(M_k(x) = x^k\\), \\(\\{M_k\\}\\) is the “monomial basis”, so that we can write \\(A_k(x) = \\sum_{j=0}^k \\alpha_{kj} M_j(x)\\). We can arrange these coefficients for the first \\(K\\) polynomials into the upper triangular matrix:\n\\[\nA^{(k)} =\n\\begin{bmatrix}\n\\alpha_{00} & 0 & 0 &\\dots & 0 \\\\\\\\\n\\alpha_{10} & \\alpha_{11} & 0 & \\dots & 0 \\\\\\\\\n\\alpha_{20} & \\alpha_{21} & \\alpha_{22} &\\dots & 0 \\\\\\\\\n\\dots\\\\\\\\\n\\alpha_{K0} & \\alpha_{K1} & \\alpha_{K2} &\\dots & \\alpha_{KK}\n\\end{bmatrix}\n^T\n\\]\nNow we can see that for \\(f(x) = \\sum_{k=0}^K a_k A_k(x)\\). We can take the vector of coefficients \\(\\vec a = (a_0, \\dots, a_K)\\) and convert back to coefficients in the monomial basis with a simple matrix multiplication\nTo convert from the monomial basis to the basis \\(A\\) invovles solving the triangular system\nTo convert from the monomial basis to the basis \\(A\\) to the basis \\(B\\) involves (1) expanding to the monomial basis and (2) solving for the coefficients in basis \\(B\\).\nApparently there are \\(O(K\\log(K))\\) algorithms for changing basis, it may be worth understanding these. But it’s very easy to see how we can move between different bases for polynomials of degree \\(K\\) by matrix multiplication.\n\n\nShifting and scaling\nWe can “shift” the polynomial basis too, that is we rewrite \\(f(x + y) = g(x; y)\\) there \\(g\\) is a polynomial in \\(x\\). This has the effect of moving fixed information \\(y\\) out of the functions argument and into the polynomial coefficients.\nHere \\(\\tilde M_k(x_1; x_2)\\) is a \\(k\\) degree polynomial in \\(x_1\\). \\(x_2\\) is treated as a parameter that is absorbed into the coefficients. Next we’d like to do the same for general polynomials. So we’d like to find \\(\\tilde a\\) such that \\(f(x_1 + x_2) = \\sum a_k A_k(x_1 + x_2) = \\sum_k \\tilde a(x_2) A_k(x_1)\\).\nLet \\(m_{kj} = 0 \\; \\forall k < j\\), and \\(M(x_2)\\) be the upper triangular matrix \\(M(x_2) = \\begin{bmatrix} m_{kj}(x_2)\\end{bmatrix}_{k=0, \\dots K,\\\\; j=0,\\dots,K}\\)\nThen we can find the coordinates\n\\[\n\\tilde a(x_2) = (A^{(k)})^{-1}M(x_2)A^{(k)} a\n\\]\nand\n\\[\n\\mathbb E_{ q(x_2) } \\left[  \\tilde a(x_2)  \\right] =  (A^{(k)})^{-1}\\mathbb E_{ q(x_2) } \\left[  M(x_2)  \\right]A^{(k)} a\n\\]\n\n\nThings to look into\nClenshaw algorithm and Horner’s method. these are recursive methods for evaluating polynomials in the Chebyshev and monomial basis respectively.\nHamming and Salzer develop algorithms for converting polynomials between different basis representations.\nWe may not be able to use these techniques, unless we can get an expression for each coefficient, because we need to evaluate the expected value of the terms."
  },
  {
    "objectID": "research/polynomial_approximation/index.html#variational-approximation",
    "href": "research/polynomial_approximation/index.html#variational-approximation",
    "title": "Polynomial approximation for variational Bayes",
    "section": "Variational approximation",
    "text": "Variational approximation\nHere we present the variational approximation for SuSiE and explore how to perform coordinate ascent variational inference with this approximation and our polynomial likelihoods.\n\nEvidence lower bound, and a polynomial approximation\nWe can write the log likelihood for a single observation as a function \\(y_i\\) of a function of the linear predictor \\(\\psi_i = \\sum_{l} \\psi_{li}\\), then we will approximate that likelihood with a polynomial in \\(\\psi_i\\)\n\\[\n\\log p (y_i | \\psi_i) = l_i(\\psi_i) \\approx \\sum_{k=0}^K m_{ik} \\psi_i^k =: \\hat l_i^{(K)}(\\psi_i)\n\\]\nThen we can approximate the ELBO\n\\[\n\\begin{aligned}\nF_1(q) &= \\mathbb E_{ q } \\left[  \\sum l_i(\\psi_i)  \\right]- KL(q || p) \\\\\n       &\\approx \\mathbb E_{ q } \\left[  \\sum \\hat l_i(\\psi_i)  \\right] - KL(q || p) =: F_2(q)\n\\end{aligned}\n\\]\nWe perform inference by selecting \\(q \\in \\mathcal Q\\) to optimize \\(F_2\\), where \\(\\mathcal Q\\) is a family of distributions.\n\\[\nq^* = \\arg\\max_{q \\in \\mathcal Q} F_2(q)\n\\]\nLet \\({\\bf m}_i = (m_{i0}, \\dots, m_{iK})\\) denote the coefficients for \\(\\hat l_i\\).\nTo perform coordinate ascent variational inference we will need to compute \\(\\mathbb E_{ q_{-l} } \\left[  \\hat l(\\psi_l + \\psi_{-l})  \\right]\\). We will write \\(\\tilde l(\\psi_l; \\psi_{-l}) = \\hat l(\\psi_l + \\psi_{-l})\\) to emphasize that we are treating the likelihood as a function of \\(\\psi_l\\), with \\(psi_{-l}\\) fixed. By expanding the \\((\\psi_l + \\psi_{-l})^k\\) terms and collecting coefficients we can write \\(f(\\psi_l; \\psi_{-l}) = \\sum_k \\tilde m_k(\\psi_{-l}) \\psi_l^k\\). Now \\(\\tilde {\\bf m}(\\psi_{-l}) = (m_k(\\psi_{-l}))_{k=0}^K\\) give the coefficients for a polynomial in \\(\\psi_l\\). Now, taking expectations\n\\[\n\\tilde f(\\psi_l; q_{-l}) := \\mathbb E_{ q_{-l} } \\left[  \\hat l(\\psi_l + \\psi_{-l})  \\right]\n= \\mathbb E_{ q_{-l} } \\left[  \\tilde l(\\psi_l; \\psi_{-l})  \\right]\n= \\sum_k \\mathbb E_{ q_{-l} } \\left[  \\tilde m_k(\\psi_{-l})  \\right] \\psi_l^k\n\\] Noting that we can write \\(\\hat l(\\psi) = \\hat l(\\psi_l + \\psi_{-l})\\)\n\n\nSuSiE variational approximation\nFor SuSiE our latent variables consist of the effect sizes \\(\\set{b_l}\\_{l=1}^L\\) and a set of indicators that select \\(L\\) non-zero effect variables \\(\\set{\\gamma_l}\\_{l=1}^L\\). We select \\(\\mathcal Q\\) to be the family of distributions that factorize over the \\(L\\) single effects, that is\n\\[\nq(\\set{b_l}, \\set{\\gamma_l}) = \\prod_l q(b_l | \\gamma_l)q(\\gamma_l).\n\\]\n\n\nCoordinate ascent\nTo update \\(q_l\\) we need to maximize\nDropping the subscript \\(i\\), for each term in the sum we need to compute\nWe can do this by applying the “expected” shift operator. We can do this by computing the moments of \\(\\psi_{-l}\\) and applying the shift operation once, or by computing the moments of \\(\\psi_m, \\;\\; m\\neq l\\) and performing the shift operation sequentially.\n(TODO: update notation here)\n\\[\n\\tilde a(q_{-l})\n= (A^{(k)})^{-1} \\left(\\prod_{m \\neq l} \\mathbb E_{ q_m } \\left[  M(\\psi_m)  \\right] \\right) A^{(k)} a\n\\]\nA nice feature of the sequential approach is it gives us an easy way of converting between polynomial representations. Let \\(\\Gamma_l = \\mathbb E_{ q_l } \\left[  M(\\psi_l)  \\right]\\) be the matrix for applying the “shifted expectation” operation to the polynomial coefficients for \\(f(\\psi_l)\\), \\({\\bf m}\\). That is \\(\\Gamma_l {\\bf m}\\) gives the coefficients of \\(f(\\psi_{-l}; q_l)\\), which is a polynomial in \\(\\psi_{-l}\\).\nLet \\(\\Gamma = \\prod_l \\Gamma_l\\). Notice that the polynomial with coefficients \\(\\Gamma {\\bf m}\\) evalutated at \\(0\\) give \\(\\mathbb E_{ q } \\left[  f(x)  \\right]\\). Furthermore we can quickly move from \\(f(\\psi_{l}; q_{-l})\\) to \\(f(\\psi_{l+1}; q_{-(l+1)})\\).\nStarting with \\(f(\\psi_{1}; q_{-1})\\) we want the coefficients \\(\\Gamma_{-1} {\\bf m}\\), where \\(\\Gamma_{-1} = \\Gamma_1^{-1} \\Gamma\\). Then, to get \\(f(\\psi_{1}; q_{-1})\\) we need \\(\\Gamma_{-2} {\\bf m}\\), where we can compute \\(\\Gamma_{-2}\\) by\n\\[\n\\Gamma_{-2} = \\Gamma_2^{-1} \\Gamma_{-1} \\Gamma_1.\n\\] We can continue iterating over all \\(L\\) single effects. We note that it is easier to compute the moments of the single effects \\(\\psi_m\\) rather than the moments of all “other” single effects \\(\\psi_{-l}\\). Carrying out the iterated expectations as matrix-vector products in the polynomial coefficients seems like an appealing approach to implementation.\nThis is useful in a coordinate ascent update scheme where we can remove one of the single effect from \\(\\Gamma\\) by a triangular system. Update \\(q_l\\), and then add back the update \\(\\Gamma_l\\) to \\(\\Gamma\\) by a right matrix multiplication.\n\n\nUnconstrained variational posterior\nThe optimal variational approximation looks like\n\\[\nq^*\\_l(b_l | \\gamma_l = j) \\propto e^{f(b_l; q_{-l}, {\\bf x}\\_j)}.\n\\] Where \\(f(b_l; q_{-l}, {\\bf x}\\_j) = \\sum_k \\eta_k b_l^k\\) is a polynomial of degree \\(K\\). Notice that this is an exponential family with sufficient statistics \\(T(b_l) = (b_l^k)\\_{k=0}^K\\) and natural parameters \\({\\bf \\eta}\\). It has a normalizing constant \\(A(\\eta) = \\log \\int \\exp\\{\\langle T(x), \\eta \\rangle\\} dx\\), and \\(\\nabla_{\\eta} A(\\eta) = \\mathbb E_{ q } \\left[  T(x)  \\right]\\). Thus if we can compute (or approximate to satisfactory precision) \\(\\nabla_{\\eta} A(\\eta)\\) we could compute the moments we need for CAVI.\nTo date, I am not really sure how to handle this integral of an exponentiation polynomial. By designing our polynomial approximation correctly, we can ensure that the the exponentiation function will decay and the \\(A\\) will be finite (recall also that \\(\\set{\\eta: A(\\eta) < \\infty}\\) is the natural parameter space).\nOne option is to approximate \\(A(\\eta)\\) by a quadrature rule. We can use automatic differentiation to compute it’s gradient.\n\n\nBest gaussian approximation\nMaybe we don’t know how to compute \\(A(\\eta) = \\log \\int \\exp\\{\\langle T(x), \\eta \\rangle\\} dx\\) Which involves evaluate the integral of an exponentiated polynomial. But perhaps we want to use a Gaussian variational approximation.\n\\[\nq_{l}^*(x) \\propto e^{f(x)}\n\\approx e^{f(\\mu) + f'(\\mu)(x-\\mu) + \\frac{1}{2}f''(\\mu)(x - \\mu)^2}\n\\]\nFor \\(\\mu\\) such that \\(f'(\\mu) = 0\\)\n\\[\ne^{f(\\mu) + f'(\\mu)(x-\\mu) + \\frac{1}{2}f''(\\mu)(x - \\mu)^2} \\propto e^{\\frac{1}{2}f''(\\mu)(x - \\mu)^2} \\propto \\mathcal N (x; \\mu, -\\frac{1}{f''(\\mu)})\n\\]\nIn our case \\(f\\) is a polynomial. Finding \\(\\mu\\) can be achieved by searching over the roots of \\(f'\\) and then \\(f''(\\mu)\\) is computed easily. This is a Laplace approximation to the optimal posterior \\(q_l\\)"
  },
  {
    "objectID": "research/polynomial_approximation/index.html#more-scattered-notes-on-polynomial-approximation-for-susie",
    "href": "research/polynomial_approximation/index.html#more-scattered-notes-on-polynomial-approximation-for-susie",
    "title": "Polynomial approximation for variational Bayes",
    "section": "More scattered notes on polynomial approximation for SuSiE",
    "text": "More scattered notes on polynomial approximation for SuSiE\nAbusing notation a bit, \\(\\phi_l = x_{\\gamma_l} b_l\\).\n\\[\n\\begin{aligned}\nf(\\psi_l)\n&= \\sum \\tilde m(q_{-l}) M_k(\\psi_l) \\\\\\\\\n&= \\sum \\tilde m(q_{-l}) A_k(x_jb_l) \\\\\\\\\n&= \\sum \\hat m_k(q_{-l}, x_j) M_k(b_l) \\\\quad \\hat m_k(q_{-l}, x_j) := \\tilde m_k(q_{-l})x_j^k\n\\end{aligned}\n\\]\n\\[\n\\mathbb E_{ q_{-l} } \\left[  f(\\psi)  \\right]\n= \\mathbb E_{ q_{-l} } \\left[  f(\\psi_l, \\psi_{-l}))  \\right]\n= \\sum \\mathbb E_{ q_{-l} } \\left[  \\tilde {\\bf m}(\\psi_{-l})  \\right] M_k(\\psi_l)\n= \\sum \\left(\\prod_{m \\neq l}\\mathbb E_{ q_m } \\left[  M(\\psi_m))  \\right]\\right){\\bf m}\n\\]\nWe can write\n\\[\n\\Gamma_l = \\mathbb E_{ q_l } \\left[  M(\\psi_l)  \\right]\n\\] \\[\n\\Psi^{(l)} = \\Gamma_{l+1} \\dots \\Gamma_L \\Gamma_1 \\dots \\Gamma_{l-1}\n\\] Then we can compute the coefficients of \\(f(\\psi_l)\\) by a triangular matrix multiplication\n\\[\n\\tilde{\\bf m}\\_l = \\Psi^{(l)}{\\bf m} = \\mathbb E_{ q_{-l} } \\left[  M(\\psi_{-l})  \\right]{\\bf m}\n\\]\nAnd we can compute the next \\(\\Psi\\) by a triangular matrix inversion and two matrix multiplications.\n\\[\n\\Psi^{(l+1)} = \\Gamma_{l+1}^{-1} \\Psi^{(l)} \\Gamma_l\n\\] ### Rescal polynomial\n\\[\nf(bx) = \\sum m_k (bx)^k = \\sum m_k b^kx^k = \\sum (m_k b^k) x^k\n\\]\n\nShift\n\\[\nf(x - c)\n= \\sum m_k (x -c)^k\n= \\sum_k m_k \\sum_{j \\leq k} {k \\choose j} x^j c^{k-j}\n= \\sum_j \\left(\\sum_{k \\geq j} {k \\choose j} c^{k-j}\\right) x^j\n\\]\n\n\nUpdating \\(q_l\\)\nWe’ve written the natural parameters\nFor each observation we can compute\n\\[\n\\hat{{\\bf m}}\\_{li} = \\tilde{\\bf m}(q_{-l}) \\circ (x_i^0, \\dots, x_i^K)\n\\]\nThese are the coefficients in the monomial basis for each observation conditional on effect \\(b_l\\) for covariate \\(x_i\\). \\(f_i(b_l) = \\sum \\tilde {\\bf m}\\_k b_l^k\\). That is, this is the data likelihood as a function of \\(b_l\\), conditional on data \\(x\\), and marginalizing over \\(\\psi_{-l}\\).\nWe can express or approximate our prior with the same polynomial expansion. Suppose we can write our prior\n\\[\n\\log p(b_l) = \\sum \\rho_{kl} b_l^k\n\\]\nThen the posterior distribution is trivially computed with a conjugate computation\n\\[\n{\\bf \\eta}\\_l =\\sum_i \\hat {\\bf m}_{li} + {\\bf \\rho}_l\n\\]\nThe posterior distribution is an exponential family with sufficient statistics \\(T(b_l) = (b_l^0, \\dots, b_l^K)\\) and natural parameters \\(\\eta_l\\).\nIf our original polynomial approximation “goes down” outside the range we care to ensure it is a good approximation, then we should always get a finite log-normalizer/cumulant \\(A(\\eta) = \\log \\int \\exp\\{\\eta^T T(\\psi)\\} d\\psi < \\infty\\). It may be important to ensure that our approximation is good over the range of values of \\(\\psi\\) with high posterior probability. Supposing we have an even degree polynomial assumption, make sure the last coefficient is \\(< 0\\) so that the function is very negative for arguments that are large in absolute value, but the approximation is good for values of small absolute value. Intuitivley, taking expectations over \\(\\psi_{-l}\\) won’t change t\nAdditionally, we ideally want to make sure that our likelihood approximation does not have bad behavior. If our polynomial approximation wildly overestimates the likelihood in some regions that could seriously mess up our inference. There is probably a tradeoff. We can approximate \\(l_i\\) on the interval \\([a, b]\\) with lower error with a polynomial of degree \\(K\\). To approximate \\(l_i\\) on the wider interval \\([a, b] \\subset [A, B]\\) with the same error we need a higher degree \\(K\\).\n\n\nComputing moments of \\(q_l\\)\nAn algorithm would look like this\n\nCompute \\(\\Psi_1\\)\nUpdate update \\(q_1\\)\nCompute \\(\\mathbb E_{ q_1 } \\left[  M(\\psi_1)  \\right]\\)\nCompute \\(\\Psi_2\\) …\n\nNote that \\(\\Psi_l\\) is constructed by taking expectations is a particular order, or multiplying matrices in a particular order. But I think order should not matter. Is it the case that triangular matrix multiplication commutes?"
  },
  {
    "objectID": "research/polynomial_approximation/index.html#discussion",
    "href": "research/polynomial_approximation/index.html#discussion",
    "title": "Polynomial approximation for variational Bayes",
    "section": "Discussion",
    "text": "Discussion\n\nQuality of the global approximation\nTheorem C.3 in [(Huggins, Adams, and Broderick 2017)] provides a bound on the Wasserstein distance between the exact posterior and the polynomial approximation to the posterior, “the result depends primarily on the peakedness of the approximate posterior, and the error of the approximate gradients”. Informally, I suspect that the more data we accrue, and the more peaked our approximate posterior becomes, the greater demand we must put on the quality of our approximation to the log density. Imagine a situation where the true likelihood surface is flat, but looks a little bumpy due to error in the polynomial approximation. These small bumps will accumulate over a large sample size leading to a spiky posterior where it should have been flat.\nIn contrast, the local approximations should shine as we accumulate more evidence and the likelihood becomes more peaked. This is because we can tune the local approximation to be tight where the likelihood peaks. The errors in the local approximation matter less because we don’t need to stray far from where the bound is tight. Note: when the likelihood is log-concave, the local variational approximations are also concave [(Seeger 2009; Challis and Barber 2011)]. I’ll need to do some more work to understand this completely for generalized local approximations, but it is certainly the case for the logistic case. In fact, running EM with the JJ bound is a good alternative to using Newton’s method to get point estimates of the MAP. Whereas Newton’s method may diverge for some initialization, JJ should converge for any initialization of the variational parameters (note: check this claim).\nQ: Is logistic regression with a Gaussian prior/L2 penalty on the effects convex? If so, we’re replacing a convex problem with a potentially multimodal one."
  },
  {
    "objectID": "research/polynomial_approximation/index.html#glossary",
    "href": "research/polynomial_approximation/index.html#glossary",
    "title": "Polynomial approximation for variational Bayes",
    "section": "Glossary",
    "text": "Glossary\n\n\n\n\n\n\n\nSymbol\nDescription\n\n\n\n\n\\({\\bf a}_i\\)\ncoefficients for \\(f_i\\) in basis \\(\\mathcal A\\)\n\n\n\\(\\tilde {\\bf a}_i(\\psi_2)\\)\ncoefficients for \\(f_i(\\psi_1; \\psi_2)\\) in the basis \\(\\mathcal A\\)\n\n\n\\(\\hat {\\bf a}_i(\\psi_2, x_j)\\)\ncoefficients for \\(f_i(b_1; x_j, \\psi_2)\\) in the basis \\(\\mathcal A\\)\n\n\n\\(M(\\psi_2)\\)\ntriangular matrix shifts monomial basis, \\(\\tilde {\\bf m} (\\psi_2) = M(\\psi_2) {\\bf m}\\). Gives coefficients of \\(f_i(\\psi_1; \\psi_2)\\) in \\(\\mathcal M\\)\n\n\n\\(A\\)\ntriangular matrix maps to coordinates in monomial basis, \\({\\bf m} = A {\\bf a}\\). Gives coefficients of \\(f_i(\\psi_1; \\psi_2)\\) in \\(\\mathcal M\\)\n\n\n\\(f_i(\\psi_1; \\psi_2)\\)\nA polynomial is \\(\\psi_1\\) such that \\(f_i(\\psi_1; \\psi_2) = f_i(\\psi_1 + \\psi_2); \\\\;\\\\; \\tilde {\\bf m}(\\psi_2) = M(\\psi_2){\\bf m}\\) gives the coordinates in the monomial basis"
  },
  {
    "objectID": "research/polynomial_approximation/index.html#related-work",
    "href": "research/polynomial_approximation/index.html#related-work",
    "title": "Polynomial approximation for variational Bayes",
    "section": "Related work",
    "text": "Related work\n(Huggins, Adams, and Broderick 2017) (Wong, n.d.)"
  },
  {
    "objectID": "research/polynomial_approximation/index.html#references",
    "href": "research/polynomial_approximation/index.html#references",
    "title": "Polynomial approximation for variational Bayes",
    "section": "References",
    "text": "References\n\n\nChallis, Edward, and David Barber. 2011. “Concave Gaussian Variational Approximations for Inference in Large-Scale Bayesian Linear Models.” In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, 199–207. JMLR Workshop and Conference Proceedings. https://proceedings.mlr.press/v15/challis11a.html.\n\n\nGaly-Fajou, Theo, Florian Wenzel, and Manfred Opper. 2020. “Automated Augmented Conjugate Inference for Non-conjugate Gaussian Process Models.” In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, 3025–35. PMLR. https://proceedings.mlr.press/v108/galy-fajou20a.html.\n\n\nHuggins, Jonathan, Ryan P Adams, and Tamara Broderick. 2017. “PASS-GLM: Polynomial Approximate Sufficient Statistics for Scalable Bayesian GLM Inference.” In Advances in Neural Information Processing Systems. Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2017/hash/07811dc6c422334ce36a09ff5cd6fe71-Abstract.html.\n\n\nSeeger, Matthias W. 2009. “Sparse Linear Models: Variational Approximate Inference and Bayesian Experimental Design.” Journal of Physics: Conference Series 197 (December): 012001. https://doi.org/10.1088/1742-6596/197/1/012001.\n\n\nWong, Lilian. n.d. “Orthogonal Polynomials–Quadrature Algorithm (OPQA): A Functional Analytical Approach to Bayesian Inference.” Orthogonal Polynomials."
  },
  {
    "objectID": "research/polynomial_regression_vb/index.html",
    "href": "research/polynomial_regression_vb/index.html",
    "title": "Implimenting polynomial approximation VB",
    "section": "",
    "text": "Code\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(pracma)\nlibrary(tictoc)\n\n\n\nAttaching package: 'tictoc'\n\n\nThe following objects are masked from 'package:pracma':\n\n    clear, size, tic, toc\n\n\nCode\nset.seed(1)"
  },
  {
    "objectID": "research/polynomial_regression_vb/index.html#computation",
    "href": "research/polynomial_regression_vb/index.html#computation",
    "title": "Implimenting polynomial approximation VB",
    "section": "Computation",
    "text": "Computation\n\nChebyshev approximations\nChebyshev polynomials can be used to approximate functions on the interval \\([-1, 1]\\) (and then also, on any finite interval). \\(f(x) \\approx \\sum c_k T_k(x)\\). We compute the coefficients of \\(c_k\\) for \\(k = 0, \\dots K\\).\nIn the code below we use pracma::polyApprox which implement a scheme of evaluate the coefficients for \\(f\\). This is essentially done via quadrature.\n\n\nCode\nmake_approximation <- function(f, R, n, plot=F){\n  p <- rev(pracma::polyApprox(f, -R, R, n =n)$p)\n  if(plot){\n    S <- R + 2\n    x <- seq(-S, S, by=0.1)\n    plot(f, -S, S)\n    lines(x, polyval2(p, x), col='red', lty='dotted')\n    abline(v=-R); abline(v=R)\n  }\n  return(p)\n}\n\n\n\n\nScaling and shifting polynomials\nCoefficients after a change of variable\n\\[\nf(bx) = \\sum m_k (bx)^k = \\sum m_k b^kx^k = \\sum (m_k b^k) x^k = f_b(x)\n\\]\n\\[\nf(x + c)\n= \\sum m_k (x + c)^k\n= \\sum_k m_k \\sum_{j \\leq k} {k \\choose j} x^j c^{k-j}\n= \\sum_j \\left(\\sum_{k \\geq j} {k \\choose j} c^{k-j}\\right) x^j\n\\]\n\n\nCode\n#' p coefficients of a polynomial in increasing order\n#' @param p polynomial coefficients in INCREASING deree\npolyval2 <- function(p, x){pracma::polyval(rev(p), x)}\n\n#' f(x + c) = f2(x)\nshift_polynomial <- function(p, c){\n  # construct map\n  K <- length(p) - 1\n  M <- matrix(nrow= K+1, ncol=K+1)\n  for(j in 0:K){\n    for(k in 0:K){\n      M[j+1, k+1] <- choose(k, j) * c**(k - j)\n    }\n  }\n  \n  coef_new <- (M %*% p)[, 1]\n  return(coef_new)\n}\n\n# change back to original scale\n# f(bx) = f2(x)\nscale_polynomial <- function(p, b){\n  K <- length(p) - 1\n  coef_new <- p * sapply(0:K, function(k) b**k)\n  return(coef_new)\n}\n\n\n\n\nLaplace approximation to polynomial\n\n\nCode\n#' convert (unnormalized) polynomial density to gaussian approximation\n#' p the coefficients of a polynomial in increasing order p = c(p0, p1, ..., pK)\npoly_to_gaussian <- function(p){\n  p <- rev(p)\n  d <- pracma::polyder(p)\n  d2 <- pracma::polyder(d)\n  \n  #f <- function(x){polyval2(p, x)}\n  #mu <- optimize(f, interval = c(-100, 100), maximum = T)$maximum\n  roots <- Re(pracma::polyroots(d)$root)\n  mu <- roots[which.max(pracma::polyval(p, roots))]\n  \n  var <- - 1 / pracma::polyval(d2, mu)\n  return(list(mu=mu, var=var))\n}\n\n\nTo test it out:\n\n\nCode\ncoef<- c(1, 2, 3)\ncoef2 <- shift_polynomial(coef, -1)\n#f(x-1) = f2(x)\n(polyval2(coef, 3) == polyval2(coef2, 4))\n\n\n[1] TRUE\n\n\nCode\ncoef<- c(1, 2, 3)\ncoef2 <- scale_polynomial(coef, 2)\n# f(2x) = f2(x)\n(polyval2(coef, 6) == polyval2(coef2, 3))\n\n\n[1] TRUE\n\n\n\n\nPlot functions\n\n\nCode\nplot_effect_posterior <- function(q, b, ...){\n  mu_post <- purrr::map_dbl(q, ~purrr::pluck(.x, 'mu'))\n  var_post <- purrr::map_dbl(q, ~purrr::pluck(.x, 'var'))\n  \n  plotrix::plotCI(\n    x = b,\n    y = mu_post,\n    li =  mu_post - 2 * sqrt(var_post),\n    ui =  mu_post + 2 * sqrt(var_post),\n    ...\n  )\n  abline(0, 1, col='red')\n}"
  },
  {
    "objectID": "research/polynomial_regression_vb/index.html#gaussian-linear-regression-mean-field-approximation",
    "href": "research/polynomial_regression_vb/index.html#gaussian-linear-regression-mean-field-approximation",
    "title": "Implimenting polynomial approximation VB",
    "section": "Gaussian linear regression, mean field approximation",
    "text": "Gaussian linear regression, mean field approximation\nHere we illustrate a simple example of our inference technique. We fit a mean field approximation to a bivariate regression problem\n\\[\n\\begin{aligned}\ny &\\sim N(\\sum_j x_j b_j, 1)\\\\\nb_j &\\sim N(0, 1)\\;\\; j = 1, \\dots, p\n\\end{aligned}\n\\]\nWhere \\(x_1\\) and \\(x_2\\) are correlated (inducing dependence in the posterior \\(p(b_1, b_2 | \\mathcal D)\\).\n\nPolynomial representation of Gaussian likelihood\nFor the observations\n\\[\nl(\\psi)\n= C -\\frac{1}{2\\sigma^2}(y - \\psi)^2\n= C -\\frac{1}{2\\sigma^2} y^2 + \\frac{y}{\\sigma^2}\\psi - \\frac{1}{2\\sigma^2}\\psi^2 \\implies {\\bf m}\n= (C -\\frac{y^2}{2 \\sigma^2}, \\frac{y}{\\sigma^2}, -\\frac{1}{2\\sigma^2})\n\\] For the prior\n\\[\n\\log p(b) = C -\\frac{1}{2\\sigma^2} \\left(b - \\mu \\right)^2\n\\implies {\\bf m} = \\left(C - \\frac{\\mu^2}{2\\sigma^2}, \\frac{\\mu}{\\sigma^2}, -\\frac{1}{2 \\sigma^2}\\right)\n\\]\n\n\nMean field variational approximation\n\\[\n\\begin{aligned}\nq({\\bf b}) = \\prod_j q(b_j) \\\\\nq(b_j) = N(\\mu_j, \\sigma^2_j)\n\\end{aligned}\n\\]\n\n\nComputing moment under \\(q\\)\n\\[\n\\mathbb E \\psi_j = \\mu_j x_j\n\\]\n\\[\n\\mathbb E \\psi_j^2 = (\\sigma^2_j + \\mu^2_j) x^2_j\n\\]\n\\[\n\\bar \\psi := \\mathbb E_q[\\psi] = \\sum \\mathbb E_q[\\psi_j]\n\\]\n\\[\n\\bar{\\psi^2} = \\mathbb E_q[\\psi^2] =  \\mathbb E_q[ \\left(\\sum \\psi_j \\right)^2] = Var(\\psi) + \\bar \\psi\n\\]\n\n\nCode\n#' write gaussian log density as polynomial \n#' return a vector c representing polynomial c[3]x^2 + c[2] x + c[1]\nmake_gaussian_coef <- function(y, var=1){\n  c(- 0.5 * y**2/var, y/var, -0.5 / var)\n}\n\n\n\n\nCode\n#' q a p-list of distributions for effect size of each column\n#' X a n x p matrix of covariate\n#' returns a list(mu, mu2) with the means and second moments\ncompute_moments <- function(X, q){\n  p <- ncol(X)\n  mu <- 0\n  var <- 0\n  for (j in 1:p){\n    mu <- mu + (X[,j] * q[[j]]$mu)\n    var <- var + (X[,j]**2 * q[[j]]$var)\n  }\n  mu2 <- var + mu**2\n  return(list(mu=mu, mu2=mu2))\n}\n\n#' compute coeeficient for EXPECTED shift\n#' f2(x) = E(f(x + c))\nshift_polynomial2 <- function(coef, mu, mu2){\n  c <- list(1, mu, mu2)\n  # construct map\n  K <- length(coef) - 1\n  M <- matrix(nrow= K+1, ncol=K+1)\n  for(j in 0:K){\n    for(k in 0:K){\n      M[j+1, k+1] <- choose(k, j) * c[[max(k-j+1, 1)]]\n    }\n  }\n  coef_new <- (M %*% coef)[, 1]\n  return(coef_new)\n}\n\n\n\n\nCode\nsublist <- function(list, j){\n  list[[j]] <- NULL\n  return(list)\n}\n\npolynomial_update2 <- function(m, X, prior_p, q){\n  p <- length(q)\n  n <- nrow(X)\n  for(j in 1:p){\n    # compute moments\n    # right now we just compute two moments, but need higher moments for \n    # higher degree polynomials\n    moments <- compute_moments(X[, -j, drop=F], sublist(q, j))\n    \n    # shift-- new polynomial in terms of \\psi_j\n    m2_tilde <- do.call(rbind, lapply(1:n, function(i) shift_polynomial2(\n      m[i,], moments$mu[i], moments$mu2[i])))\n    \n    # scale-- new polynomial in terms of b_j\n    m2_hat <- do.call(rbind, lapply(1:n, function(i) scale_polynomial(m2_tilde[i,], X[i, j])))\n    \n    # compute posterior polynomial\n    m2_post <- colSums(m2_hat) + prior_p[[j]]\n    \n    # find gaussian approximation\n    q[[j]] <- poly_to_gaussian(m2_post)\n  }\n  \n  return(q)\n}\n\n\n\n\nExample\n\n\nCode\nsimulate <- function(n=500, p=2, lenghtscale = 0.8, prior_variance=5){\n  Z <- matrix(rnorm(n*p), nrow=n)\n  K <- exp(-(outer(1:p, 1:p, '-')/lenghtscale)**2)\n  X <- Z %*% K\n  \n  b <- rnorm(p) * sqrt(prior_variance)\n  y <- (X %*% b)[, 1] + rnorm(n)\n  \n  return(list(y=y, X=X, b=b))\n}\n\nsim <- simulate(p=3)\ny <- sim$y\nX <- sim$X\nb <- sim$b\nprint(b)\n\n\n[1]  1.900755 -2.069063  1.998108\n\n\n\n\nCode\n# observations in polynomial coeeficients\nm <- do.call(rbind, lapply(y, make_gaussian_coef))\np <- ncol(X)\n\n# initialize prior and q\nq <- list()\nprior_p <- list()\nfor(j in 1:p){\n  prior_p[[j]] <- c(0, 0, -0.5)\n  q[[j]] <- list(mu = 0, var=1)\n}\n\n# iteratively update\nparam_history <- list()\nparam_history[[1]] <- q\nfor(i in 1:50){\n  q <- polynomial_update2(m, X, prior_p, q)\n  param_history[[i+1]] <- q\n}\n\nmu_post <- purrr::map_dbl(q, ~purrr::pluck(.x, 'mu'))\nvar_post <- purrr::map_dbl(q, ~purrr::pluck(.x, 'var'))\n\n\nHere we simulate a gaussian linear mode with three covariates. We plot the simulated effects against their posteror mean. It looks like we are able to recover the effects. Nice!\n\n\nCode\nplot_effect_posterior(q, sim$b)\n\n\n\n\n\n\n\nComparison to usual Bayesian computation\nTODO"
  },
  {
    "objectID": "research/polynomial_regression_vb/index.html#logistic-regression",
    "href": "research/polynomial_regression_vb/index.html#logistic-regression",
    "title": "Implimenting polynomial approximation VB",
    "section": "Logistic regression",
    "text": "Logistic regression\nNext we will take a quadratic approximation for logistic regression. We will also test higher degree polynomial approximations. In all cases we continue to use a Gaussian mean-field variational approximation. Computing the moments of the higher degree polynomials may be tricky, so we defer for now. However, if it can be done easily we may benefit from the richer varitional approximation (note: the Gaussian distribution is a degree 2 polynomial exponential family since it’s sufficient statistics are \\(T(x) = [x, x^2]\\))\n\nPolynomial approximation to log likelihood\nFor each data point we want to approximate the log likelihood as a function of the linear predictor\n\\[\n\\log p(y | \\psi) = y\\psi + \\log \\sigma(-\\psi)\n\\]\nHere we plot the \\(\\log p(y=0 | \\psi)\\) and it’s polynomial approximations of degree \\(k=2,4,6,8\\).\nThese polynomial approximations are generated using pracma::polyApprox which use the Chebyshev coefficients of an appropriately rescaled version of the function, to generate a polynomial approximation on the interval \\([a, b]\\). We generate approximations on the interval \\([-R, R]\\) where \\(R = 3\\).\n\n\nCode\n# loglike functions for y=1 and y=0\nloglik1 <- function(psi){\n  psi + log(sigmoid(-psi))\n}\nloglik0 <- function(psi){\n  log(sigmoid(-psi))\n}\n\n# polynomial approximation via pracma\nR <- 3\nll0_p2 <- rev(pracma::polyApprox(loglik0, -R, R, n = 2)$p)\nll0_p4 <- rev(pracma::polyApprox(loglik0, -R, R, n = 4)$p)\nll0_p6 <- rev(pracma::polyApprox(loglik0, -R, R, n = 6)$p)\nll0_p8 <- rev(pracma::polyApprox(loglik0, -R, R, n = 8)$p)\n\n# note: ll0 and ll1 are just reflections over the x axis\n# so we can get ll1 by taking ll0_p2 and flipping the sign of the linear term\nll1_p2 <- rev(pracma::polyApprox(loglik1, -R, R, n = 2)$p)\nll1_p4 <- rev(pracma::polyApprox(loglik1, -R, R, n = 4)$p)\nll1_p6 <- rev(pracma::polyApprox(loglik1, -R, R, n = 6)$p)\nll1_p8 <- rev(pracma::polyApprox(loglik1, -R, R, n = 8)$p)\n\nS <- R + 2\nx <- seq(-S, S, by=0.1)\nplot(loglik0, -S, S)\nlines(x, polyval2(ll0_p2, x), col='red', lty='dotted')\nlines(x, polyval2(ll0_p4, x), col='blue', lty='dotted')\nlines(x, polyval2(ll0_p6, x), col='green', lty='dotted')\nlines(x, polyval2(ll0_p8, x), col='orange', lty='dotted')\nabline(v=R); abline(v=-R)\n\n\n\n\n\n\n\nApproximate date likelihood\nThis just takes a whoe list of vector \\(y\\) and returns a matrix of coefficients\n\n\nCode\n#' get approximate polynomial representation of the data y\nbernoulli_poly_approx <- function(y, R, k){\n  n <- length(y)\n  p0 <- make_approximation(loglik0, R, k)\n  \n  # for y=1 flip the sign of odd coefficients (note: 0 indexing)\n  p1 <- p0\n  p1[seq(2, length(p0), by=2)] <- p1[seq(2, length(p0), by=2)] * -1\n  \n  m <- matrix(nrow = n, ncol = k + 1)\n  for(i in 1:length(y)){\n    if(y[i] == 0){\n      m[i,] <- p0\n    } else{\n      m[i,] <- p1\n    }\n  }\n  return(m)\n}\n\n\n\n\nSimulate logistic regression\n\n\nCode\nsimulate_lr <- function(n=500, p=2, lenghtscale = 0.8, prior_variance=5){\n  Z <- matrix(rnorm(n*p), nrow=n)\n  K <- exp(-(outer(1:p, 1:p, '-')/lenghtscale)**2)\n  X <- Z %*% K\n  \n  b <- rnorm(p) * sqrt(prior_variance)\n  logits <- (X %*% b)[, 1]\n  y <- rbinom(length(logits), 1, sigmoid(logits))\n  return(list(y=y, X=X, b=b, logits=logits))\n}\n\n\n\n\nApproximation with \\(k=2\\)\nWe can reuse our code above, substituting in new “data”, the coefficients for the polynomial approximation to the conditional likelihood.\n\n\nCode\nlogistic_polynomial_approximation_k2 <- function(y, X, R){\n  # observations in polynomial coeeficients\n  m <- bernoulli_poly_approx(y, R, k=2)\n  p <- ncol(X)\n  \n  # initialize prior and q\n  q <- list()\n  prior_p <- list()\n  for(j in 1:p){\n    prior_p[[j]] <- c(0, 0, -0.5)\n    q[[j]] <- list(mu = 0, var=1)\n  }\n  \n  # iteratively update\n  param_history <- list()\n  param_history[[1]] <- q\n  for(i in 1:50){\n    q <- polynomial_update2(m, X, prior_p, q)\n    param_history[[i+1]] <- q\n  }\n  return(param_history)\n}\n\n\nA 2d approximation does not seem to perform very well here.\n\n\nCode\nset.seed(5)\nn <- 500\np <- 3\nlenghtscale <- 0.8\nprior_variance <- 5\n\nZ <- matrix(rnorm(n*p), nrow=n)\nK <- exp(-(outer(1:p, 1:p, '-')/lenghtscale)**2)\nX <- Z %*% K\n\nb <- rnorm(p) * sqrt(prior_variance)\nlogits <- (X %*% b)[, 1]\ny <- rbinom(length(logits), 1, sigmoid(logits))\nprint(b)\n\n\n[1] 3.3319418 0.3807557 5.4429687\n\n\nCode\nqs <- logistic_polynomial_approximation_k2(y, X, R=5)\nq <- tail(qs, 1)[[1]]\nmu_post <- purrr::map_dbl(q, ~purrr::pluck(.x, 'mu'))\nvar_post <- purrr::map_dbl(q, ~purrr::pluck(.x, 'var'))\n\nplotrix::plotCI(\n  x = b,\n  y = mu_post,\n  li =  mu_post - 2 * sqrt(var_post),\n  ui =  mu_post + 2 * sqrt(var_post)\n)\nabline(0, 1, col='red')\n\n\n\n\n\n\nTesting a range of interval widths\nWe can check a few values of \\(R\\). There is a tradeoff here of course– the wider the interval we try to approximate, the worse the approximation will be. But if the interval is too narrow, the polynomial approximate likelihood essentially does not support data that fall far outside the interval. This is because we require the highest odd degree coefficient of our polynomial to be \\(<0\\) otherwise the likelihood grows unbounded outside the interval, and the approximation is not integrable.\n\n\n\n\n\nCode\npar(mfrow = c(1, 3))\nplot_effect_posterior(q_R3, b, main='R=3')\nplot_effect_posterior(q_R5, b, main='R=5')\nplot_effect_posterior(q_R7, b, main='R=7')\n\n\n\n\n\n\n\n\nHigher degree approximations\nNow we will extend our implementation above to handle higher degree approximations. This involves computing higher moments of the effect predictions, which isn’t too hard for Gaussian distributions.\n\n\nCode\n#' Make shift matrix\n#' \n#' Generate matrix that maps coefficients of a polynomial\n#' f(x + y) (represented by coefficients p) to coefficients of\n#' f2(x) = E_{p(y)}[f(x+y)]\n#' @param moments moments of y\nmake_shift_matrix <- function(moments){\n  # construct map\n  K <- length(moments) - 1\n  M <- matrix(nrow= K+1, ncol=K+1)\n  for(j in 0:K){\n    for(k in 0:K){\n      M[j+1, k+1] <- choose(k, j) * moments[[max(k-j+1, 1)]]\n    }\n  }\n  return(M)\n}\n\n#' Transform coefficients of a polynomial f(x + y) (represented by coefficients p)\n#' to coefficients of f2(x) = E_{p(y)}[f(x+y)]\n#' @param p K+1 coefficients of a degree-k polynomial\n#' @param moments moments of y (including E[y^0] = 1)\nshift_polynomial3 <- function(p, moments){\n  M <- make_shift_matrix(moments)\n  p_new <- (M %*% p)[, 1]\n  return(p_new)\n}\n\ncompute_normal_moments <- function(mu, var, k){\n  return(purrr::map_dbl(0:k, ~actuar::mnorm(.x, mu, sqrt(var))))\n}\n\n#' compute k moments for psi = xb, b ~ N(mu, var)\ncompute_psi_moments <- function(x, mu, var, k){\n  normal_moments <- compute_normal_moments(mu, var, k)\n  psi_moments <- do.call(cbind, purrr::map(0:k, ~ (x**.x) * normal_moments[.x + 1]))\n}\n\n#' update q with polynomial approximation of arbitrary degree\npolynomial_update3 <- function(m, X, prior_p, q){\n  K <- ncol(m) - 1\n  p <- ncol(X)\n  n <- nrow(X)\n  for(j in 1:p){\n    m_tilde <- m\n    for(k in (1:p)[-j]){\n      moments <- compute_psi_moments(X[, k], q[[k]]$mu, q[[k]]$var, K)\n      m_tilde <- do.call(rbind, lapply(1:n, function(i) shift_polynomial3(\n        m_tilde[i,], moments[i,])))\n    }\n    \n    # scale-- new polynomial in terms of b_j\n    m_hat <- do.call(rbind, lapply(1:n, function(i) scale_polynomial(\n      m_tilde[i,], X[i, j])))\n    \n    # compute posterior polynomial\n    m_post <- colSums(m_hat) + prior_p[[j]]\n    \n    # find gaussian approximation\n    q[[j]] <- poly_to_gaussian(m_post)\n    q[[j]]$m_post <- m_post\n  }\n  \n  return(q)\n}\n\nlogistic_polynomial_approximation <- function(y, X, R, K=2){\n  # observations in polynomial coeeficients\n  m <- bernoulli_poly_approx(y, R, K)\n  q <- list()\n  prior_p <- list()\n  for(j in 1:p){\n    prior_p[[j]] <- c(c(0, 0, -0.5), rep(0, K-2)) # extend polynomial to agree with m\n    q[[j]] <- list(mu = 0, var=1) # initialize normal posterior\n  }\n  \n  # iteratively update\n  param_history <- list()\n  param_history[[1]] <- q\n  for(i in 1:50){\n    q <- polynomial_update3(m, X, prior_p, q)\n    param_history[[i+1]] <- q\n  }\n  return(param_history)\n}\n\n\n\n\nCompare implimentations\nThe two implimentations agree!\n\n\nCode\nset.seed(12)\nsim <- simulate_lr(p=3)\ny <- sim$y\nX <- sim$X\nb <- sim$b\n\nqs_2 <- logistic_polynomial_approximation_k2(y, X, R=10)\nqs_k2 <- logistic_polynomial_approximation(y, X, R=10, K=2)\n\npar(mfrow=c(1,2))\nplot_effect_posterior(qs_2[[51]], b, main='A')\nplot_effect_posterior(qs_k2[[51]], b, main='B')\n\n\n\n\n\n\n\nHow does the posterior approximation change as we increase degree?\nWe approximate the likelihood on \\([-10, 10]\\) with \\(K=2,6,10,14\\). Note the polynomial approximation cannot be e.g. degree \\(4\\) because these polynomials are unbounded above (\\(c_4 >0\\)), so \\(e^ \\hat f(x)\\) is not integrable over the real line. But \\(c_K < 0\\) for approximations of degree \\(K = 2z + 2\\) for \\(z \\in \\mathbb N\\).\nIn this example it seems that with \\(K=2\\) we tend to over-estimate the effect size, but this is resolved in the higher degree approximations.\n\n\nCode\nset.seed(12)\nsim <- simulate_lr(p=3)\ny <- sim$y\nX <- sim$X\nb <- sim$b\n\nqs_k2 <- logistic_polynomial_approximation(y, X, R=10, K=2)\nqs_k6 <- logistic_polynomial_approximation(y, X, R=10, K=6)\nqs_k10 <- logistic_polynomial_approximation(y, X, R=10, K=10)\nqs_k14 <- logistic_polynomial_approximation(y, X, R=10, K=14)\n\npar(mfrow=c(2,2))\nplot_effect_posterior(qs_k2[[51]], b, main='K=2')\nplot_effect_posterior(qs_k6[[51]], b, main='K=6')\nplot_effect_posterior(qs_k10[[51]], b, main='K=10')\nplot_effect_posterior(qs_k14[[51]], b, main='K=14')"
  },
  {
    "objectID": "research/polynomial_regression_vb/index.html#poisson-regression-example",
    "href": "research/polynomial_regression_vb/index.html#poisson-regression-example",
    "title": "Implimenting polynomial approximation VB",
    "section": "Poisson regression example",
    "text": "Poisson regression example\n\nImpliment\n\n\nCode\npoisson_ll <- function(y){\n  f <- function(psi) dpois(y, exp(psi), log = T)\n  return(f)\n}\n\npoisson_approx <- function(y, R, k, as_function=F){\n  f <- poisson_ll(y)\n  p <- make_approximation(f, R, k, plot = F)\n\n  if(as_function){\n    p2 <- function(x) polyval2(p, x)\n    return(p2)\n  }\n  return(p)\n}\n\npoisson_poly_approx <- function(y, R, k){\n  unique_counts <- unique(y)\n  \n  polynomials <- list()\n  for(yy in unique_counts){\n    polynomials[[yy + 1]] <- poisson_approx(yy, R, k)\n  }\n  \n  m <- do.call(rbind, purrr::map(y, ~polynomials[[.x + 1]]))\n  return(m)\n}\n\npoisson_regression_polynomial_approximation <- function(y, X, R, K=2){\n  # observations in polynomial coeeficients\n  tic()\n  m <- poisson_poly_approx(y, R, K)\n  p <- ncol(X)\n  q <- list()\n  prior_p <- list()\n  for(j in 1:p){\n    prior_p[[j]] <- c(c(0, 0, -0.5), rep(0, K-2)) # extend polynomial to agree with m\n    q[[j]] <- list(mu = 0, var=1) # initialize normal posterior\n  }\n  \n  # iteratively update\n  param_history <- list()\n  param_history[[1]] <- q\n  for(i in 1:50){\n    q <- polynomial_update3(m, X, prior_p, q)\n    param_history[[i+1]] <- q\n  }\n  toc()\n  return(param_history)\n}\n\n\n\n\nVisualize approximation\nWe approximation \\(f(\\psi) = Pois(y=3; \\lambda = e^\\psi)\\) using polynomials of increasing degree on the interval \\([-5, 5]\\)\n\n\nCode\nf <- poisson_ll(3)\n\n# polynomial approximation via pracma\nR <- 5\nf_k2 <- rev(pracma::polyApprox(f, -R, R, n = 2)$p)\nf_k4 <- rev(pracma::polyApprox(f, -R, R, n = 4)$p)\nf_k8 <- rev(pracma::polyApprox(f, -R, R, n = 8)$p)\nf_k16 <- rev(pracma::polyApprox(f, -R, R, n = 16)$p)\n\nS <- R + 2\nx <- seq(-S, S, by=0.1)\nplot(f, -S, S)\nlines(x, polyval2(f_k2, x), col='red', lty='dotted')\nlines(x, polyval2(f_k4, x), col='blue', lty='dotted')\nlines(x, polyval2(f_k8, x), col='green', lty='dotted')\nlines(x, polyval2(f_k16, x), col='orange', lty='dotted')\nabline(v=R); abline(v=-R)\n\n\n\n\n\n\n\nSimulate poisson regression\n\n\nCode\nsimulate_poisson_regression <- function(n=500, p=2, lenghtscale = 0.8, prior_variance=1){\n  Z <- matrix(rnorm(n*p), nrow=n)\n  K <- exp(-(outer(1:p, 1:p, '-')/lenghtscale)**2)\n  X <- Z %*% K\n  \n  b <- rnorm(p) * sqrt(prior_variance)\n  logrates <- (X %*% b)[, 1]\n  y <- rpois(length(logrates), exp(logrates))\n  return(list(y=y, X=X, b=b, logrates=logrates))\n}\n\nsim <- simulate_poisson_regression(p=4, prior_variance = 1)\ny <- sim$y\nX <- sim$X\nb <- sim$b\nprint(b)\n\n\n[1] -1.3296046  0.6149598  0.7740477  0.2643521\n\n\n\n\nExample\n\n\nCode\npois_R5_K2 <- poisson_regression_polynomial_approximation(y, X, R=5, K=2)\n\n\n6.152 sec elapsed\n\n\nCode\npois_R5_K4 <- poisson_regression_polynomial_approximation(y, X, R=5, K=4)\n\n\n9.306 sec elapsed\n\n\nCode\npois_R5_K8 <- poisson_regression_polynomial_approximation(y, X, R=5, K=8)\n\n\n20.818 sec elapsed\n\n\nCode\npois_R5_K16 <- poisson_regression_polynomial_approximation(y, X, R=5, K=16)\n\n\n61.81 sec elapsed\n\n\nCode\npar(mfrow=c(2,2))\nplot_effect_posterior(pois_R5_K2[[51]], b, main='K=2')\nplot_effect_posterior(pois_R5_K4[[51]], b, main='K=4')\nplot_effect_posterior(pois_R5_K8[[51]], b, main='K=8')\nplot_effect_posterior(pois_R5_K16[[51]], b, main='K=16')\n\n\n\n\n\n\n\nCode\nplot_effect_posterior(pois_R5_K2[[51]], sim$b)\n\n\n\n\n\nCode\npurrr::map_dbl(pois_R5_K2[[51]], ~purrr::pluck(.x, 'mu'))\n\n\n[1] -1.51427519  0.50936054  0.92844494  0.08298042"
  },
  {
    "objectID": "research/polynomial_regression_vb/index.html#higher-degree-polynomial-posterior-q",
    "href": "research/polynomial_regression_vb/index.html#higher-degree-polynomial-posterior-q",
    "title": "Implimenting polynomial approximation VB",
    "section": "Higher degree polynomial posterior \\(q\\)",
    "text": "Higher degree polynomial posterior \\(q\\)\nIn the above implementation we compute a polynomial proportional to the posterior density, but reduce this to a Gaussian approximation by taking a Laplace approximation.\nThat is we want to compute\n\\[\n\\mathbb E [b^j] = \\int_{\\mathbb R} b^j q(b)\n\\]\nWhich we approximate by\n\\[\n\\mathbb E [b^j] \\approx \\int_{\\mathbb R} b^j q_{\\text{gauss}}(b)\n\\]\nWhere \\(q_{\\text{gauss}}\\) is the Gaussian distribution that minimizes the divergence to \\(q_{\\text{gauss}} = \\arg \\min_{q_g} KL(q_g ||q)\\).\nIt would be better if we could compute the moments of \\(q(b_l) \\propto \\exp\\{\\sum_k \\eta_k b_l^k\\}\\). We define the log normalizing constant \\(A({\\bf \\eta}) = \\log \\int \\exp\\{\\sum_{k=0}^K \\eta_k b_l^k\\} db\\). Let \\(f(b) = \\sum_{k=0}^K \\eta_k b^k - A(\\eta)\\) so that \\(q(b) = \\exp\\{f(b)}\\).\nBecause \\(q\\) is in an exponential family, we know that \\(\\nabla_{\\eta} A(\\eta) = \\mathbb E[T(x)] = [1, \\mathbb E[b], \\mathbb E[b^2], \\dots, \\mathbb E[b^K]]\\). Is there an easy way to compute the gradient of \\(A\\)?"
  },
  {
    "objectID": "research/mode_matching_rss_vb/index.html",
    "href": "research/mode_matching_rss_vb/index.html",
    "title": "Mode seeking in mean field VB for RSS + sparse prior",
    "section": "",
    "text": "The RSS likelihood relates observed marginal effects to the unobserved effects of a joint model\n\\[\\begin{align}\n\\hat \\beta \\sim \\mathcal N(SRS^{-1} \\beta, SRS) \\\\\n\\beta \\sim g(\\cdot)\n\\end{align}\\]\nWhere we consider the problem of putting an i.i.d. prior on the entries of \\(\\beta\\) and using a mean field approximation for variational inference.\nSpecifically, we put a spike and slab prior on \\(\\beta_j = b_j\\gamma_j\\) for \\(j \\in [p]\\). Where \\(b_j \\sim N(0, \\sigma^2)\\) gives the distribution of non-zero effects, and and \\(\\gamma_j \\sim Bernoulli(\\pi)\\). That is, the effect is non-zero with probability \\(\\pi\\).\nThe problem we demonstrate, is that due to the mode matching behavior of the “reverse” KL divergence, which is minimized in variational inference, the posterior on \\(q(\\gamma_1, \\dots, \\gamma_p)\\) will tend to concentrate instead of accurately representing uncertainty. Furthermore, due to strong dependence among the posterior means.\nWe work with a simplified version of RSS assuming we observe \\(z\\)-scores \\(\\hat z\\).\n\\[\n\\begin{aligned}\n\\hat z &\\sim \\mathcal N(Rz, R) \\\\\nz_i &\\sim \\pi_0 \\delta_0 + \\pi_1 \\mathcal N(0, \\sigma^2)\n\\end{aligned}\n\\]\n\\[\nq(z, \\gamma) = \\prod_j q(z_j, \\gamma_j)\n\\]\n\\[\n\\begin{aligned}\nELBO(q_j) &= \\mathbb E_{q_{-j}} \\squarb{\\log p(\\hat z| z, R) + \\log p(z_j) - \\log q(b_l, \\gamma_l)} + H(q_l) \\\\\n&= \\hat z_j (b_j \\gamma_j) - \\frac{1}{2} \\left[ (b_j \\gamma_j)^2 + 2 (b_j \\gamma_j) \\sum_{i \\neq j} R_{ij} \\mathbb E_{q_{-j}} \\squarb{z_j} \\right] + \\log p(b_l | \\gamma_l) + \\log p(\\gamma_l) + H(q_l) + C\n\\end{aligned}\n\\]\nThen \\(q(b_l | \\gamma_l = 1) = N(\\frac{\\nu_j}{\\tau_j}, \\tau^{-1}_j)\\) Where \\(\\nu_j = \\hat z_j - \\sum_{i\\neq j} R_{ij} \\alpha_i \\mu_i\\), and \\(\\tau_j = 1 + \\sigma^{-2}_0\\).\nIt’s easy to see that the best choice for \\(q(b_l | \\gamma_l = 0)\\) is the prior, since all fo the data terms disappear, also noted here [(Titsias and Lázaro-Gredilla, n.d.)]\nAnd \\(q(\\gamma_j) = Bernoulli(\\alpha_j)\\), where \\(\\log \\left(\\frac{\\alpha_j}{1 - \\alpha_j}\\right) = \\hat z \\mu_j - \\frac{1}{2} \\left[\\mu^2_j + \\sigma^2_j + 2 \\mu_j \\sum_{i\\neq j} R_{ij} \\mu_i \\alpha_i \\right] + \\log\\left(\\frac{\\pi}{1 - \\pi}\\right)\\).\n\nSimulation\n\n\nCode\n#' @param q q(mu, var, alpha)\n#' @param R LD matrix-- assumes diag(R) = rep(1, p)\n#' @param tau0 prior effect variance\n#' @param prior_logit p-vector with prior log odds for gamma = 1\nrssvb <- function(zhat, q, R, tau0, prior_logit){\n  # unpack\n  mu <- q$mu\n  var <- q$var\n  alpha <- q$alpha\n\n  p <- length(zhat)\n  psi <- (R %*% (mu * alpha))[,1] # prediction\n  for(i in 1:p){\n    # remove effect of this variable\n    psi <- psi - R[i,] * (mu[i]*alpha[i])\n\n    # compute q(beta | gamma = 1)\n    nu <- zhat[i] - psi[i]\n    tau <- 1 + tau0\n    mu[i] <- nu/tau\n    var[i] <- 1/tau\n\n    # logit <- zhat[i] * mu[i]\n    #   - 0.5 * (psi[i] * mu[i] +  mu[i]^2 + var[i])\n    #   -0.5 * tau0 * (mu[i]^2 + var[i]) + prior_logit[i]\n    logit <- 0.5 * (mu[i]^2/var[i] + log(var[i]) + log(tau0)) + prior_logit[i]\n    alpha[i] <- 1/(1 + exp(-logit))\n\n    alpha[i]\n    psi <- psi + R[i,] * (mu[i]*alpha[i])\n  }\n  return(list(mu=mu, var=var, alpha=alpha))\n}\n\n\n\n\nCode\nsim_zscores <- function(n, p){\n  X <- logisticsusie:::sim_X(n=n, p = p, length_scale = 5)\n  R <- cor(X)\n  z <- rep(0, p)\n  z[10] <- 5\n  zhat <- (R %*% z)[,1] + mvtnorm::rmvnorm(1, sigma=R)[1,]\n  return(list(zhat = zhat, z=z, R=R))\n}\n\ninit_q <- function(p){\n  q = list(\n    mu = rep(0, p),\n    var = rep(1, p),\n    alpha = rep(1/p, p)\n  )\n  return(q)\n}\n\nrun_sim <- function(n = 100, p = 50, tau0=1, prior_logit = -3){\n  sim <- sim_zscores(n = n, p = p)\n  q <- init_q(p)\n  prior_logit <- rep(prior_logit, p)\n  for(i in 1:100){\n    q <- with(sim, rssvb(zhat, q, R, tau0, prior_logit))\n  }\n  \n  sim$q <- q\n  return(sim)\n}\n\n\nFor 100 independent simulations, we simulate \\(50\\) dependent \\(z\\)-scores. The true non-zero \\(z\\)-score is at index \\(10\\) with \\(\\mathbb E[\\hat z_{10}] = 5\\). However, over half the time, the VB approximation confidently selects another nearby feature.\n\n\nCode\nset.seed(10)\nsims <- purrr::map(1:100, ~run_sim(tau0=0.01))\nmax_idx <- purrr::map_int(1:100, ~which.max(sims[[.x]]$q$alpha))\n\nalpha10 <- purrr::map_dbl(1:100, ~sims[[.x]]$q$alpha[10])\nhist(alpha10)\n\n\n\n\n\nCode\ntable(max_idx)\n\n\nmax_idx\n 8  9 10 11 12 \n 4 47 36 11  2 \n\n\n\n\nMany small effects vs a few large effects\nThe interpretation of \\(\\sigma_0^2\\) depends a lot on how polygenic the trait is. Even though we only simulate one non-zero effect, if we use a prior \\(\\pi_1 >> 0\\) the model approaches a mean field approximation of ridge regression. Since ridge can estimate many small effects we get less shrinkage than if we enforce sparse architecture with \\(\\pi_1 \\approx 0\\).\n\n\nCode\nposterior_mean <- function(sim){\n  return((sim$R %*% (sim$q$mu * sim$q$alpha))[, 1])\n}\n\nshrinkage_plot <- function(sims, ...){\n  lims <- range(purrr::map(1:length(sims), ~sims[[.x]]$zhat))\n  plot(\n    sims[[1]]$zhat,\n    posterior_mean(sims[[1]]),\n    xlim = c(-4, 7),\n    ylim = c(-4, 7),\n    xlab = 'zhat',\n    ylab = 'posterior mean z',\n    ...\n  )\n  for(i in 1:100){\n    points(sims[[i]]$zhat, posterior_mean(sims[[i]]))\n  }\n  abline(0, 1, col='red')\n}\n\nset.seed(10)\n\nsim_sparse <- purrr::map(1:100, ~run_sim(tau0=0.1, prior_logit = -3))\nsim_poly <- purrr::map(1:100, ~run_sim(tau0=0.1, prior_logit = 3))\n\npar(mfrow=c(1,2))\nshrinkage_plot(sim_sparse, main='Sparse')\nshrinkage_plot(sim_poly, main='Polygenic')\n\n\n\n\n\n\n\n\n\n\nReferences\n\nTitsias, Michalis K, and Miguel Lázaro-Gredilla. n.d. “Doubly Stochastic Variational Bayes for Non-Conjugate Inference.”"
  },
  {
    "objectID": "research/polynomial_susie/index.html",
    "href": "research/polynomial_susie/index.html",
    "title": "Polynomial approximation SuSiE",
    "section": "",
    "text": "The sum of single effects (SuSiE) regression, is a regression with a SuSiE prior on the effects. For a Gaussian model with identity link, using a variational approximation that factorizes over single effects yields a fast coordinate ascent variational inference scheme which can be optimized by fitting a sequence of single effect regression (SERs). The computational simplicity of this result relies on (1) the variational approximation and (2) the fact the the log likelihood is quadratic in the regression coefficients.\nHere, we consider the more general case where the log-likelihood is a polynomial in the regression coefficients. We can approximate the log-likelihood of models with many choices of likelihood and link function with polynomials, so having a fast inference scheme for this case provides a uniform treatment of extensions of SuSiE to different observation models. We use the same variational approximation as in linear SuSiE. Approximating the log-likelihood as as polynomial is sufficient to"
  },
  {
    "objectID": "research/polynomial_susie/index.html#preliminaries",
    "href": "research/polynomial_susie/index.html#preliminaries",
    "title": "Polynomial approximation SuSiE",
    "section": "Preliminaries",
    "text": "Preliminaries\n\nPolynomial shift\nGiven the coefficients \\({\\bf a}\\) we would like to find the coefficients for the change of variable\n\\[\\phi_{\\bf a}(x + y) = \\phi_{{\\bf b}({\\bf a}, y)}(x)\\]\n\\[\n\\begin{aligned}\n\\phi_{\\bf a}(x + y)\n&= \\sum_{m=0}^M a_m(x + y)^m \\\\\n&= \\sum_{m=0}^M a_m \\sum_{k=0}^m {m \\choose k} x^k y^{m-k} \\\\\n&= \\sum_{k=0}^M \\left(\\sum_{m=k}^M a_m {m \\choose k}y^{m-k} \\right) x^k \\\\\n&= \\sum_{k=0}^M {\\bf b}({\\bf a}, y)_k x^k, \\quad  {\\bf b}({\\bf a}, y)_k := \\left(\\sum_{m=k}^M a_m {m \\choose k}y^{m-k} \\right)\n\\end{aligned}\n\\] Inspecting the expression for \\({\\bf b}({\\bf a}, y)_k\\) we can see that the new coefficient vector \\({\\bf b}({\\bf a}, y)\\) can be written compactly in matrix form\n\\[\n{\\bf b}({\\bf a}, y) = M(y) {\\bf a}, \\quad M_{ij} = {j \\choose i} y^{j -i}\\;  \\forall j \\geq i, 0 \\text{ otherwise}\n\\] Supposing \\(y\\) is random, we will also have need to compute the expected value of \\({\\bf b}({\\bf a}, y)\\). For \\(y \\sim p\\), \\(\\mathcal M(p) = \\mathbb E_p[M(y)]\\), we can compute these expected coefficients\n\\[\n{\\bf c}({\\bf a}, p) = \\mathcal M(p) {\\bf a}\n\\]\n\n\nPolynomial rescaling\n\\[\\phi_{\\bf a}(cx) = \\phi_{{\\bf d}({\\bf a}, c)}(x)\\]\n\\[\n\\sum_m a_m(c x)^m = \\sum_m (a_m c^m) x^m = \\sum_m {\\bf d}({\\bf a}, c)_m x^m\n\\]"
  },
  {
    "objectID": "research/polynomial_susie/index.html#model",
    "href": "research/polynomial_susie/index.html#model",
    "title": "Polynomial approximation SuSiE",
    "section": "Model",
    "text": "Model\n\\[\n\\begin{aligned}\ny_i | \\mu_i &\\sim f(\\mu_i, \\theta) \\\\\n\\mu_i &= g({\\bf x}^T_i \\beta ) \\\\\n\\beta &\\sim \\text{SuSiE}(L, \\{\\sigma_{0l}\\})\n\\end{aligned}\n\\] \\(f\\) is the observation model, parameterized by \\(\\mu\\) and \\(\\theta\\). \\(g\\) is a link function which maps the linear predictions \\(\\psi_i := {\\bf x}_i^T \\beta\\) to the parameter \\(\\mu_i\\).\n\nPolynomial approximation to the log-likelihood\nFor each observation \\(y\\), we can approximate the log-likelihood as a polynomial in the linear prediction. \\(f\\) is a polynomial of degree \\(M\\)\n\\[\n\\log p(y | \\psi) = \\log f(y | g(\\psi)) \\approx f(\\psi)\n\\]\n\\[\nf(\\psi) = \\sum_{m=0}^M a_m \\psi^m\n\\]\nThere are many ways to construct this approximation. At a high level, we want the approximation to be “good” (some measure of error between the approximate log-likelihood and the exact log-likelihood is small) at plausible values of \\(\\psi\\). So far, if have used a truncated Chebyshev series.\nWe write \\(f_i(\\psi)\\) as the polynomial approximation for \\(\\log p(y_i | \\psi)\\). We write it’s coefficients \\(\\left( a_m^{(i)} \\right)_{m \\in [M]}\\). To denote a polynomial with coefficients \\({\\bf a}\\) we will write \\(\\phi_{\\bf a}\\), e.g. \\(f(\\psi) = \\phi_{\\bf a}(\\psi)\\).\nWe have a polynomial approximation for each observations. Let \\({\\bf a}_i\\) denote the coefficients for observations \\(i\\). and \\(A\\) be the \\(n \\times m\\) matrix where each row corresponds to an observation."
  },
  {
    "objectID": "research/polynomial_susie/index.html#single-effect-regression-with-polynomial-approximation",
    "href": "research/polynomial_susie/index.html#single-effect-regression-with-polynomial-approximation",
    "title": "Polynomial approximation SuSiE",
    "section": "Single effect regression with polynomial approximation",
    "text": "Single effect regression with polynomial approximation\n\nSER prior\nThe SER prior \\(\\beta \\sim SER(\\sigma_0^2, \\pi)\\)\n\\[\n\\begin{aligned}\n\\beta = b\\gamma\\\\\nb \\sim N(0, \\sigma_0^2) \\\\\n\\gamma \\sim \\text{Mult}(1, \\pi)\n\\end{aligned}\n\\]\n\n\nPolynomial approximation for SER posterior\nThe SER follows from \\(p\\) univariate regressions\n\\[\n\\begin{aligned}\np(b | {\\bf y}, X, \\gamma=j, \\sigma^2_0)\n  &= p(b | {\\bf x}_j, {\\bf y}, \\sigma^2_0) \\\\\n  &\\propto p({\\bf y}, b| {\\bf x_j}, \\sigma^2_0) \\\\\n  &\\approx \\exp\\{ \\sum_i \\phi_{\\bf a_i}(x_{ij} b) + \\log p(b)\\}\n\\end{aligned}\n\\]\nSupposing \\(\\log p(b) \\approxeq \\phi_{\\bf \\rho}(b)\\) for some coefficients \\(\\rho\\), the unnormalized log posterior density can be written as a degree \\(M\\) polynomial with coefficients \\({\\bf f}(A, {\\bf x}_j, \\rho)\\) defined below:\n\\[\n\\begin{aligned}\n\\sum_i \\phi_{\\bf a_i}(x_{ij} b) + \\phi_{\\bf \\rho}(b) &= \\sum_i \\phi_{{\\bf d}({\\bf a}_i, x_{ij})}(b) + \\phi_{\\bf \\rho}(b), \\\\\n&= \\phi_{{\\bf f}(A, {\\bf x}_j, \\rho)}(b), \\quad {\\bf f}(A, {\\bf x}_j, \\rho) := \\sum_i {\\bf d}({\\bf a}_i, x_{ij}) + \\rho. \\\\\n\\end{aligned}\n\\]\nFor clarity we write \\({\\bf f}_j\\) for \\({\\bf f}(A, {\\bf x}_j, \\rho)\\). The posterior density is \\(q(b) = \\frac{1}{Z_j}\\exp\\{\\phi_{{\\bf f}_j}(b)\\}\\) where \\(Z_j = \\int_{\\mathbb R} \\exp\\{\\phi_{{\\bf f}_j}(b)\\} db\\). The normalizing constant may be computed numerically. Or, we can take a Gaussian approximation for \\(q(b)\\).\n\\[\n\\begin{aligned}\np(\\gamma = j | {\\bf y}, X, \\sigma^2_0)\n&\\propto p({\\bf y} | {\\bf x}_j, \\gamma = j) \\pi_j \\\\\n&= \\left(\\int p({\\bf y}, b | {\\bf x}_j, \\gamma = j) db\\right) \\pi_j \\\\\n&= \\left(\\int \\exp\\{\\sum_i \\phi_{{\\bf a}_i} (x_{ij} b) + \\phi_{\\rho}(b) \\}\\right) \\pi_j \\\\\n&= \\left(\\int \\exp\\{\\phi_{{\\bf f}_j}(b) \\} db\\right) \\pi_j = Z_j \\pi_j \\\\\n\\end{aligned}\n\\]\n\\[\\alpha_j := q(\\gamma = j) = \\frac{Z_j \\pi_j}{\\sum_k Z_k \\pi_k}\\]\n\n\nVariational objective\nIt will be useful to write out the variational objective for the SER:\n\\[\nF_{SER}(q| {\\bf y}, X, \\sigma^2_0) = \\mathbb E_q[\\log p({\\bf y} | X, b, \\gamma, \\sigma^2_0)] - KL[q || p].\n\\] The exact posterior maximizes \\(F_{SER}\\), that is\n\\[\np_{post} = \\arg\\max_q F_{SER}(q| {\\bf y}, X, \\sigma^2_0).\n\\]\nWe can approximate the variational objective by substituting our polynomial approximation\n\\[\n\\hat F_{SER}(q| A, X, \\sigma^2_0) = \\mathbb E_q \\left[\n\\sum_i \\phi_{\\exp\\{{\\bf a}_i}(X (b\\gamma)) + \\log p(b | \\sigma^2_0) + \\log p(\\gamma | \\pi)\\}\n\\right]  - KL[q || p].\n\\]\nAs discussed above, \\(q(b | \\gamma) \\propto \\exp\\{\\phi_{{\\bf f}_j}(b)\\}\\) and \\(q(\\gamma = j) = \\frac{Z_j}{\\sum_k Z_k}\\).\nLet \\(SER(A, X, \\sigma^2_0, \\pi)\\) be a function that returns \\(q\\).\n\n\nPolynomial to Gaussian\nFor the extention to SuSiE, we are going to need to compute \\(\\mu_j^k = \\mathbb E[b^k | \\gamma = j] = \\int_{\\mathbb R} b^k \\exp\\{\\phi_{{\\bf f}(A, {\\bf x}_j, \\rho)}(b)\\} db\\). We can ensure that this function is integrable by making selecting polynomial approximations \\({\\bf a}_i\\) which are of even degree and \\(a_{iM} < 0\\). This ensures that the leading coefficient of \\({\\bf f}\\) is negative and that \\(e^{\\bf f}\\) decays. However, this integral may not be available analytically in general.\nHowever, if \\(\\phi_{\\bf f}\\) is degree \\(2\\), this is a special case where the moments of a Gaussian distribution can be computed analytically. This motivates us to “approximate the approximation”.\nWe take a Laplace approximation to \\(\\phi_{\\bf f}\\). Currently I approximate the degree \\(M\\) polynomial \\(\\phi_{\\bf f}\\) with the degree \\(2\\) polynomial by the following: first we search for the mode of \\(\\phi_{\\bf f}\\) e.g. by finding the roots of \\(\\phi_{\\bf f}^\\prime\\) and selecting the one that maximizes \\(\\phi_{\\bf f}\\) (although there may be cheaper ways to do this?). Then, we take a second order Taylor series expansion around the mode.\nIt’s worth noting that these two levels of approximation result in a different strategy than taking a “global” second order approximation directly, or a Laplace approximation \\(\\log p(y, \\beta) \\rightarrow {\\bf f} \\rightarrow {\\bf g}\\). “Laplace approximation of a degree-\\(M\\) polynomial approximation” is a local approximation around the approximate mode. Directly applying Laplace approximation would result in a local approximation around the exact mode. We would prefer the latter except that (1) the polynomial approximation provides the computational simplifications we need to extend to SuSiE and (2) finding the posterior mode and it’s second derivative may be more expensive in the general case."
  },
  {
    "objectID": "research/polynomial_susie/index.html#polynomial-susie",
    "href": "research/polynomial_susie/index.html#polynomial-susie",
    "title": "Polynomial approximation SuSiE",
    "section": "Polynomial SuSiE",
    "text": "Polynomial SuSiE\n\nVariational approximation\nWe will use the variational approximation that factorizes over single effects. This is the same variational approximation used in linear SuSiE\n\\(\\beta_l = \\gamma_l b_l\\) \\[q(\\beta_1, \\dots, \\beta_L) = \\prod_l q(\\beta_l)\\]\n\n\nVariational objective\nWe want to find \\(q\\) that optimizes the evidence lower bound (ELBO) \\(F\\)\n\\[\nF_{\\text{SuSiE}}(q| {\\bf y}, X, \\{\\sigma^2_{0l}\\}, \\{\\pi_l\\}) = \\mathbb E_q[\\sum \\log p(y_i | {\\bf x}_i, \\beta)] - \\sum KL[q(\\beta_l) || p(\\beta_l | \\sigma^2_{0l}, \\pi_l)]\n\\]\nEvaluating the expectations \\(\\mathbb E_q[\\log p(y_i | {\\bf x}_i, \\beta)]\\) is generally hard, we make progress by substituting the polynomial approximation, we denote the approximate ELBO \\(\\hat F\\).\n\\[\n\\begin{aligned}\n\\hat F_{\\text{SuSiE}}(q| A, X, \\{\\sigma^2_{0l}\\}, \\{\\pi_l\\})\n&= \\mathbb E_q[\\sum_i \\phi_{{\\bf a}_i}({\\bf x}_i^T \\beta)]\n- \\sum KL[q(\\beta_l) || p(\\beta_l | \\sigma^2_{0l}, \\pi_l)]\n\\end{aligned}\n\\]\n\n\nCoordinate ascent in the approximate ELBO\nWith \\(\\psi_{li} := {\\bf x}_i^T \\beta_l\\) and \\(\\psi_i = {\\bf x}_i^T \\beta = \\sum {\\bf x}_i^T \\beta_l = \\sum_l \\psi_{li}\\) and \\(\\psi_{-li} = \\psi_i - \\psi_{li}\\).\nConsider the approximate ELBO just as a function of \\(q_l\\)\n\\[\n\\begin{aligned}\n\\hat F_{\\text{SuSiE},l}(q_l| A, X,  q_{-l}, \\sigma^2_{0l}, \\pi_l)\n= \\mathbb E_{q_l} \\left[\n    \\mathbb E_{q_{-l}} \\left[\n      \\sum_i \\phi_{{\\bf a}_i}(\\psi_{li} + \\psi_{-li})\n    \\right]\n\\right] - KL \\left[p(\\beta_l) || q(\\beta_{-l})\\right] + \\kappa\n\\end{aligned}\n\\]\nWhere \\(\\kappa\\) is a constant that does not depend on \\(q_l\\) (note: it does depend on \\(\\{\\sigma^2_{0l}\\}, \\{\\pi_l\\}\\), but they are not written in the arguments to \\(\\hat F_{\\text{SuSiE},l}\\). We will need to evaluate \\(\\mathbb E_{q_{-l}}[\\phi_{\\bf a_i}(\\psi_{li} + \\psi_{-li})]\\) for each \\(i\\). Focusing on a single observation\n\\[\n\\mathbb E_{q{-l}} \\left[\\phi_{\\bf a_i}(\\psi_{li} + \\psi_{-li})\\right]\n= \\mathbb E_{q_{-l}} \\left[\\phi_{{\\bf b}({\\bf a}, \\psi_{-li})}(\\psi_{li})\\right] = \\phi_{{\\bf c}({\\bf a_i}, q_{-l})}(\\psi_{li})\n\\]\nWe will write \\({\\bf b}_{li} = {\\bf b}({\\bf a}, \\psi_{-li})\\) and \\({\\bf c}_{li} = {\\bf c}({\\bf a_i}, q_{-l})\\), Indicating that they are the coefficient for an \\(M\\) degree polynomial in \\(\\psi_{li}\\).\nRevisiting the approximate ELBO we see\n\\[\n\\begin{aligned}\n\\hat F_{\\text{SuSiE},l}(q_l| A, X,  q_{-l})\n& = \\mathbb E_{q_l} \\left[\n    \\mathbb E_{q_{-l}} \\left[\n      \\sum_i \\phi_{{\\bf a}_i}(\\psi_{li} + \\psi_{-li})\n    \\right]\n\\right] - KL \\left[q(\\beta_l) || p(\\beta_{-l} | \\sigma^2_{0l}, \\pi_l)\\right] + \\kappa \\\\\n&= \\left(\\mathbb E_{q_l} \\left[\n  \\sum_i \\phi_{{\\bf c}_{li}}(\\psi_{li})\n\\right] - KL \\left[q(\\beta_l) || p(\\beta_{-l} | \\sigma^2_{0l}, \\pi_l)\\right] \\right)  + \\kappa \\\\\n&= \\hat F_{\\text{SER}}(q_l | C_{l}, X, \\sigma^2_{0l}, \\pi_l) + \\kappa\n\\end{aligned}\n\\]\nWe can see that with respect to the \\(q_l\\) the SuSiE approximate ELBO is equal, up to a constant, to the SER approximate ELBO, for a new set of coefficients \\(C_l\\), which depend on \\(q_{-l}\\). It follows that the coordinate ascent update for \\(q_l\\) is achieved by fitting the polynomial approximate SER with coefficients \\(C_l\\).\nThis is analagous to how we fit linear SuSiE by a sequence of SERs: we can fit the polynomial approximate SuSiE with a sequence of polynomial approximate SERs. Rather than “residualizing”, we fit the polynomial approximate SER with coefficients \\(C_l\\).\nThe crux of this approach is having a fast way to compute \\(C_l\\).\n\n\nUpdate Rule\nAt iteration \\(t\\) we have the current variational approximation\n\\[\nq^{(t)} = \\prod q_l^{(t)}\n\\]\nDefine \\[\n{\\bf c}_i^{(0)} := \\mathcal M(\\psi_i, q^{(0)}) {\\bf a}_i\n\\]\nNote that \\(\\mathcal M(\\psi_i, q^{(0)})\\) gives the expected shift matrix that removes the entire linear prediction, so that \\({\\bf c}_{i0}^{(0)} = \\phi_{\\bf c_i^{(0)}}(0) = \\mathbb E_{q^{(0)}}[\\log p(y_i | \\psi_i)]\\).\nAt iteration \\(t\\), our coordinate ascent updates require \\({\\bf c}_{li}^{(t)} = \\mathcal M(\\psi_{li}, q_{-l}^{(t)})\\). However, we know that:\n\\[\n{\\bf c}_{i}^{(t)} = \\mathcal M(\\psi_{li}, q_l^{(t)}){\\bf c}_{li}^{(t)} \\implies {\\bf c}_{li}^{(t)} = \\mathcal M(\\psi_{li}, q_l^{(t)})^{-1}{\\bf c}_{i}^{(t)}\n\\]\nWe can use \\(C_l^{(t)}\\) to compute \\(q_l^{(t+1)}\\) and then\n\\[\n{\\bf c}_{i}^{(t)} \\leftarrow \\mathcal M(\\psi_{li}, q_l^{(t + 1)}){\\bf c}_{li}^{(t)}\n\\]\nso, by solving a triangular systems, and multiplying by upper triangular matrices \\(\\mathcal M\\) we can “efficiently” move between the coefficient representations needed for each SER update. I worry that \\(\\mathcal M_l\\) may be poorly conditioned resulting in numerical instability, but I have not seen it in toy examples yet.\n\n\nAlgorithm\nInitialize \\(C^{(0)} = A\\) and \\(q^{(0)} = \\prod q_l^{(0)}\\) such that \\(\\mathbb E[\\psi_l^k] = 0\\;\\; \\forall l, k\\).\nFor \\(t = 1, 2, \\dots\\):\n\n\\(C^{(t)} = C^{(t-1)}\\)\nFor each \\(l \\in [L]\\):\n\nCompute \\(C_l^{(t)}\\) by \\({\\bf c}_{li}^{(t)} \\leftarrow \\mathcal M(\\psi_{li}, q_l^{(t-1)})^{-1} {\\bf c}_i^{(t)}\\) for \\(i \\in [n]\\)\n\\(q_l^{(t)} \\leftarrow \\text{SER}(C_l^{(t)}, X, \\sigma_{0l}^2, \\pi_l)\\)\nUpdate \\(C^{(t)}\\) by \\({\\bf c}_i^{(t)} \\leftarrow \\mathcal M(\\psi_{li}, q^{(t)}_l) {\\bf c}^{(l)}_i\\)\n\n\n\n\nComplexity\nLet’s break down the complexity of the inner loop (that is, updating \\(q_l\\) and the requisite computations for the next run of the loop).\nComputing coefficients \\(C_l\\): Computing \\({\\bf c}_{li}\\) is \\(O(M^2)\\) to construct \\(\\mathcal M_l\\) and to solve the triangular system \\({\\bf c}_i = \\mathcal M_l(\\psi_{li}, q_l) {\\bf c}_{li}\\). This is per observations, so computing \\(C_{l}\\) is \\(O(M^2m)\\)\nFitting SER Once we computed the coefficients we update the SER in \\(O(Mnp)\\). We just sum the coefficients to construct the polynomial posterior for each variable). Then \\(O(pM^3)\\) to perform root finding and make the Gaussian approximation for \\(q_l\\).\nComputing moments We need to evaluate the moments \\(\\mathbb E[\\psi_{li}^k]\\) for \\(k=1, \\dots, M-1\\). \\(O(Mnp)\\)? This is fast if \\(q_l\\) is Gaussian but the scaling factor may be large if we need to compute numerically. If we are making the Gaussian approximation, that costs \\(O(M^3p)\\).\nUpdating \\(C\\) Again \\(O(nM^2)\\). We need to construct \\(\\mathcal M (\\psi_{li}, q_l)\\) and compute the matrix-vector product \\(\\mathcal M_l {\\bf c}\\).\nTotal \\(O(M^2n + Mnp + M^3p)\\)"
  },
  {
    "objectID": "research/logistic_abf_accuracy/index.html",
    "href": "research/logistic_abf_accuracy/index.html",
    "title": "Wakefield ABF Accuracy",
    "section": "",
    "text": "Code\nlibrary(dplyr)\nlibrary(ggplot2)\n\nsigmoid <- function(x){1/(1 + exp(-x))}\n\nsimulate <- function(n, beta0=0, beta=1){\n  x <- rnorm(n)\n  logit <- beta0 + beta*x\n  y <- rbinom(n, 1, sigmoid(logit))\n  return(list(x=x, y=y, beta0=beta0, beta=beta))\n}\n\nfit_glm_no_intercept <- function(sim){\n  fit <- with(sim, glm(y ~ x + 0, family = 'binomial'))\n  tmp <- unname(summary(fit)$coef[1,])\n  betahat <- tmp[1]\n  shat2 <- tmp[2]^2\n  z <- with(sim, (beta - betahat)/sqrt(shat2))\n  res <- list(\n    intercept=0,\n    betahat = betahat,\n    shat2 = shat2,\n    z = z\n  )\n  return(res)\n}\n\nfit_glm <- function(sim, prior_variance=1){\n  fit <- with(sim, glm(y ~ x + 1, family = 'binomial'))\n  tmp <- unname(summary(fit)$coef[2,])\n  betahat <- tmp[1]\n  shat2 <- tmp[2]^2\n  z <- with(sim, (beta - betahat)/sqrt(shat2))\n  res <- list(\n    intercept = unname(fit$coefficients[1]),\n    betahat = betahat,\n    shat2 = shat2,\n    z = z\n  )\n  return(res)\n}\n\ncompute_log_abf1 <- function(betahat, shat2, prior_variance){\n  W <- prior_variance\n  V <- shat2\n  z <- betahat / sqrt(shat2)\n  labf <- 0.5 * log(V /(V + W)) + 0.5 * z^2 * W /(V + W)\n  return(labf)\n}\n\ncompute_log_abf2 <- function(betahat, shat2, prior_variance){\n  dnorm(betahat, 0, sqrt(shat2 + prior_variance), log = T) - \n    dnorm(betahat, 0, sqrt(shat2), log=T)\n}\n\ncompute_log_abf <- function(glm_summary, prior_variance=1){\n  labf <- with(glm_summary, compute_log_abf1(betahat, shat2, prior_variance))\n  return(labf)\n}\n\ncompute_exact_log_abf_no_intercept <- function(sim, prior_variance){\n  glm_fit <- fit_glm_no_intercept(sim)\n  f <- function(b){\n    logits <- sim$x * b\n    sum(dbinom(x=sim$y, size=1, prob=sigmoid(logits), log = T)) + \n      dnorm(b, 0, sqrt(prior_variance), log = T)\n  }\n  max <- f(glm_fit$betahat)\n  \n  f2 <- function(b){purrr::map_dbl(b, ~exp(f(.x) - max))}\n  \n  lower <- glm_fit$betahat - sqrt(glm_fit$shat2) * 6\n  upper <- glm_fit$betahat + sqrt(glm_fit$shat2) * 6\n  quadres <- cubature::pcubature(f2, lower, upper, tol = 1e-10)\n  log_bf <- (log(quadres$integral) + max) - sum(dbinom(sim$y, 1, 0.5, log=T))\n  return(log_bf)\n}\n\ncompute_exact_log_bf <- function(sim, prior_variance){\n  glm_fit <- fit_glm(sim)\n  f <- function(b){\n    logits <- sim$x * b + glm_fit$intercept\n    sum(dbinom(x=sim$y, size=1, prob=sigmoid(logits), log = T)) + \n      dnorm(b, 0, sqrt(prior_variance), log = T)\n  }\n  max <- f(glm_fit$betahat)\n  \n  f2 <- function(b){purrr::map_dbl(b, ~exp(f(.x) - max))}\n  \n  lower <- glm_fit$betahat - sqrt(glm_fit$shat2) * 6\n  upper <- glm_fit$betahat + sqrt(glm_fit$shat2) * 6\n  quadres <- cubature::pcubature(f2, lower, upper, tol = 1e-10)\n  log_bf <- (log(quadres$integral) + max) - sum(dbinom(sim$y, 1, mean(sim$y), log=T))\n  return(log_bf)\n}\n\ncompute_vb_log_bf <- function(sim, prior_variance=1){\n  vb <- with(sim, logisticsusie::fit_univariate_vb(x, y, tau0=1/prior_variance)) \n  log_vb_bf <- tail(vb$elbos, 1) - sum(dbinom(sim$y, 1, mean(sim$y), log=T))\n  return(log_vb_bf)\n}"
  },
  {
    "objectID": "research/logistic_abf_accuracy/index.html#demonstrations",
    "href": "research/logistic_abf_accuracy/index.html#demonstrations",
    "title": "Wakefield ABF Accuracy",
    "section": "Demonstrations",
    "text": "Demonstrations\nHere we simulate \\(x_i \\sim N(0, 1)\\) and \\(y_i \\sim \\text{Bernoulli}(\\sigma(x_i \\beta + \\beta_0))\\)\n\n\\(\\beta = 0.1\\), \\(\\beta_0 = 0\\)\nWe simulate 10 replications at each sample size \\(n = 2^6, 2^7, \\dots, 2^{16}\\). While we see relatively good agreement between the ABF and exact BF on a log-scale, we do see the pattern that the error tends to increase with increasing samples size. Furthermore, these “small” errors on the log scale actually translate to large errors between the BF and ABF.\n\n\nCode\n# range of n, 10 reps\nn <- 2^(6:15)\nbf_comparison_01 <- purrr::map_dfr(n, ~bf_comparison_rep(10, .x, 0.1))\n\n# \"good\" agreement the exact BF and ABF\npar(mfrow=c(1, 2))\nplot(bf_comparison_01$log_bf,\n     bf_comparison_01$log_abf,\n     xlab='log BF', ylab = 'log ABF'); abline(0, 1, col='red')\nplot(bf_comparison_01$log_bf,\n     bf_comparison_01$log_abf - bf_comparison_01$log_bf,\n     xlab = 'log BF', ylab = 'log ABF - log BF'); abline(h=0, col='red')\n\n\n\n\n\nCode\n# bf_comparison_01 %>% \n#   tidyr::pivot_longer(ends_with('bf'), names_to='method', values_to='log_bf') %>%\n#   ggplot(aes(x=as.factor(n), y=log_bf, color=method)) +\n#   geom_boxplot()\n\n\nbf_comparison_01 %>% \n  mutate(\n    abf_rel_error = exp(log_abf - log_bf) - 1,\n    vbf_rel_error = exp(log_vbf - log_bf) - 1) %>%\n  tidyr::pivot_longer(ends_with('error'), names_to='method', values_to='relative_error') %>%\n  ggplot(aes(x=as.factor(n), y=relative_error, color=method)) +\n  geom_boxplot()\n\n\n\n\n\nSurprisingly as \\(n\\) grows, the relative error between the BF and the ABF increases! At \\(n=2^9\\) things look pretty good, but at \\(n=2^15\\) the relative error is quite large! Let’s inspect the \\(z\\)-scores at these simulation settings– do they look normally distributed?\n\nConfirming normality\nBoth \\(n=2^9\\) and \\(n=2^{15}\\) are fairly large sample sizes. We confirm that \\(z = \\frac{\\beta - \\hat\\beta}{\\hat s}\\) looks normal\n\n\nCode\nglm_sim_reps <- function(reps, n, beta){\n  purrr::map_dfr(1:reps, ~simulate(n, beta=beta) %>% fit_glm() %>% data.frame()) %>%\n    mutate(rep = 1:reps)\n}\n\nrep1 <- glm_sim_reps(1000, 2^9, 0.1)\nshapiro.test(rep1$z)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  rep1$z\nW = 0.99826, p-value = 0.4108\n\n\nCode\nks.test(rep1$z, pnorm)\n\n\n\n    One-sample Kolmogorov-Smirnov test\n\ndata:  rep1$z\nD = 0.030303, p-value = 0.3175\nalternative hypothesis: two-sided\n\n\nCode\nrep2 <- glm_sim_reps(1000, 2^15, 0.1)\nshapiro.test(rep2$z)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  rep2$z\nW = 0.99693, p-value = 0.05116\n\n\nCode\nks.test(rep2$z, pnorm)\n\n\n\n    One-sample Kolmogorov-Smirnov test\n\ndata:  rep2$z\nD = 0.027911, p-value = 0.4172\nalternative hypothesis: two-sided\n\n\n\n\nCode\np1 <- ggplot(rep1,aes(x=z)) +\n       geom_line(stat = \"ecdf\")+\n       geom_point(stat=\"ecdf\",size=2) +\n       stat_function(fun=pnorm,color=\"red\") +\n       labs(title=\"eCDF and normal CDF, n = 2^9\")\np2 <- ggplot(rep2,aes(x=z)) +\n       geom_line(stat = \"ecdf\")+\n       geom_point(stat=\"ecdf\",size=2) +\n       stat_function(fun=pnorm,color=\"red\") +\n       labs(title=\"eCDF and normal CDF, n=2^15\")\n\ncowplot::plot_grid(p1, p2)\n\n\n\n\n\n\n\nCode\nsubset_sim <- function(sim, m){\n  sim2 <- sim\n  sim2$x <- head(sim2$x, m)\n  sim2$y <- head(sim2$y, m)\n  return(sim2)\n}\n\nbeta <- 0.1\nsim <- simulate(2^16, beta=beta)\ntitrate <- purrr::map_dfr(5:16, ~sim %>% subset_sim(2^.x) %>% fit_glm() %>% data.frame())\nplotrix::plotCI(\n  x= 5:16,\n  y= titrate$betahat - beta,\n  li = (titrate$betahat - beta) - 2 * sqrt(titrate$shat2),\n  ui = (titrate$betahat - beta) + 2 * sqrt(titrate$shat2)\n); abline(h=0, lty=3)\n\n\n\n\n\nI was wondering if glm applies a conservative correction to it’s estimate of the standard error. The standard errors come from the squareroot of the diagonal of the negative inverse Fisher information matrix since asymptotically.\n\\[\n\\hat \\beta \\sim N(\\beta,  - \\mathcal I(\\beta)^{-1})\n\\] I think peoples use \\(- \\mathcal I(\\hat \\beta)\\) as an estimator for the precision matrix. Ignoring the intercept I just computed \\(I = \\sum_i \\nabla^2_{\\beta}\\log p (y |x, \\beta, \\beta_0)\\), and used \\(s_2 = \\sqrt{I^{-1}}\\) as a new estimate of the standard error. Despite some hadn waiving, it actually agrees very well with the standard errors reported by glm– which is to say this is not the problem.\n\n\nCode\nrecompute_shat2 <- function(betahat, intercept, x){\n  logits <- betahat * x + intercept\n  return(1/sum(sigmoid(logits) * sigmoid(-logits) * x^2))\n}\n\ntitrate <- titrate %>%\n  mutate(m = 2^(5:16)) %>%\n  rowwise() %>%\n  mutate(shat2_recompute = recompute_shat2(betahat, intercept, head(sim$x, m))) %>%\n  ungroup()\n\nplot(titrate$shat2, titrate$shat2_recompute); abline(0, 1, col='red')\n\n\n\n\n\n\n\n\n\\(\\beta = 1\\)\nFor larger \\(\\beta\\) the discrepency between ABF and BF is much more obvious!\n\n\nCode\nn <- 2^(6:15)\nbf_comparison_1 <- purrr::map_dfr(n, ~bf_comparison_rep(10, .x, 1))\n\nplot(bf_comparison_1$log_bf, bf_comparison_1$log_abf); abline(0, 1, col='red')\n\n\n\n\n\nCode\nbf_comparison_1 %>% \n  tidyr::pivot_longer(ends_with('bf'), names_to='method', values_to='log_bf') %>%\n  ggplot(aes(x=as.factor(n), y=log_bf, color=method)) +\n  geom_boxplot()\n\n\n\n\n\nCode\nbf_comparison_1 %>% \n  mutate(\n    abf_rel_error = exp(log_abf - log_bf) - 1,\n    vbf_rel_error = exp(log_vbf - log_bf) - 1) %>%\n  tidyr::pivot_longer(ends_with('error'), names_to='method', values_to='error') %>%\n  ggplot(aes(x=as.factor(n), y=error, color=method)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nCode\nplot(bf_comparison_1$log_vbf, bf_comparison_1$log_abf);abline(0, 1, col='red')\n\n\n\n\n\n\n\nCode\nn <- 2^(6:15)\nbf_comparison_01 <- purrr::map_dfr(n, ~bf_comparison_rep(10, .x, 1))\nbf_comparison_01 %>% \n  mutate(log_abf_error = (log_abf - log_bf), log_vbf_error = (log_vbf - log_bf)) %>%\n  tidyr::pivot_longer(ends_with('error'), names_to='method', values_to='error') %>%\n  ggplot(aes(x=as.factor(n), y=log_bf, color=method)) +\n  geom_boxplot()"
  },
  {
    "objectID": "research/logistic_abf_accuracy/index.html#an-explaination",
    "href": "research/logistic_abf_accuracy/index.html#an-explaination",
    "title": "Wakefield ABF Accuracy",
    "section": "An explaination",
    "text": "An explaination\n\\[\n\\begin{aligned}\nABF &= \\int \\frac{N(\\hat \\beta| \\beta, s^2)}{N(\\hat\\beta | 0, s^2)} p(\\beta) d\\beta\\\\\nBF &= \\int \\frac{p({\\bf y} | {\\bf x}, \\beta)}{p({\\bf y} | \\beta = 0)} p(\\beta) d\\beta\n\\end{aligned}\n\\]\nFirst, here is an example where there is a large discrepancy between the log ABF and log BF– over 20 log-likelihood units! We also note that including/excluding the intercept in glm doesn’t really make a dent.\n\n\nCode\nset.seed(2)\nsim <- simulate(1000)\nfit <- fit_glm(sim, prior_variance=1)\nfit_no_intercept <- fit_glm_no_intercept(sim)\n\n# show there is a discrepency ~20 log-likelihood unit\nwith(fit, compute_log_abf1(betahat, shat2, 1))\n\n\n[1] 71.33497\n\n\nCode\nwith(fit_no_intercept, compute_log_abf1(betahat, shat2, 1))\n\n\n[1] 71.46433\n\n\nCode\ncompute_vb_log_bf(sim, 1)\n\n\n[1] 93.52508\n\n\nUsing this simulated example we plot the likelihood ratio and approximate log-likelihood ratio (with respect to the null model \\(\\beta=0\\)) over the range of \\(\\beta\\). We plot the likelihood ratio in black, and plot the approximate likelihood ratio in red. We see that the likelihood ratios agree at \\(\\beta = 0\\)– this is not surprising because they should both be equal to \\(1\\) (or \\(0\\), on the log scale as shown in the plot).\nThe mode and the curvature of this likelihood ratio approximation are determined by the effect size and standard error, so the approximation is completely specified. We can see that this leaves a gap between the likelihood ratio and the approximate likelihood ratio near the mode. Assuming a flat prior on \\(\\beta\\), this is the region that will contribute most to the integral.\nWhile the likelihood is well approximated by a Gaussian near it’s mode, the quality of the Gaussian approximation is poor in the tails. As the sample size increases, \\(\\beta = 0\\) is more “in the tail”. So we see that the approximation to the likelihood ratio used in ABF is requiring the approximate likelihood ratio to agree with the exact likelihood ratio at some point in the tail– but this is not particularly relevant for getting a good approximation of the BF, as it is the area in the body of the distribution that contributes most to the integral.\nIn order for the ABF to perform well in large samples, the Gaussian approximation would need to improve in the tails faster than the point \\(\\beta=0\\) gets pushed into the tail. E.g. for the Gaussian approximation to be good, we’d want that the curvature estimated at the mode is also the curvature we’d estimate everywhere else (e.g. at \\(\\beta = 0\\)).\n\n\nCode\ncompare_lrs <- function(sim, prior_variance=1, k=1, plot_correction=F){\n  fit <- fit_glm(sim, prior_variance=1)\n  fit_no_intercept <- fit_glm_no_intercept(sim)\n  \n  # plot ll\n  ll <- function(b){with(sim, sum(y*x*b - log(1 + exp(x*b))))}\n  ll_vec <- function(b){purrr::map_dbl(b, ~ll(.x))}\n  asymptotic_ll <- function(b){dnorm(b, mean=fit$betahat, sd=sqrt(fit$shat2), log=T)}\n\n  ll0 <- ll(0)\n  asymptotic_ll0 <- asymptotic_ll(0)\n  \n  ll_mle <- ll(fit$betahat)\n  asymptotic_ll_mle <- asymptotic_ll(fit$betahat)\n  \n  mle = fit$betahat\n  xs <- seq(mle - k* abs(mle), mle + k* abs(mle), by=0.1)\n  \n  diff = (ll_mle - asymptotic_ll_mle) - ll0\n  ll_xs <- ll_vec(xs)\n  asymptotic_ll_xs <- asymptotic_ll(xs)\n  ll_prior <- dnorm(xs, 0, sd = sqrt(prior_variance), log = T)\n  \n  ylim <- range(asymptotic_ll_xs - asymptotic_ll0, ll_xs - ll0)\n  plot(xs,ll_xs - ll0, type='l', ylim = ylim,\n       xlab= 'beta', ylab='log likelihood ratio')\n  lines(xs, asymptotic_ll_xs - asymptotic_ll0, col='red')\n  \n  if(plot_correction){\n    lines(xs, asymptotic_ll_xs - asymptotic_ll_mle + (ll_mle - ll0), col='blue')\n  }\n  abline(h=0, lty=3)\n  abline(v=0, lty=3)\n  abline(v=fit$betahat)\n}\n\ncompare_lrs(sim, prior_variance = 1, k=1.5)\n\n\n\n\n\n\n\nCode\ncompare_lrs(sim, prior_variance = 1, k=50)\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1, 4))\nsimulate(100, beta0 = 0.1) %>% compare_lrs(k=1.5)\nsimulate(1000, beta0 = 0.1) %>% compare_lrs(k=1.5)\nsimulate(10000, beta0 = 0.1) %>% compare_lrs(k=1.5)\nsimulate(100000, beta0 = 0.1) %>% compare_lrs(k=1.5)\n\n\n\n\n\n\nFamily of LR approximations\nWrite the likelihood ratio as \\[\nLR(\\beta_1, \\beta_2) = \\frac{\\mathcal {L}(\\beta_1)}{\\mathcal {L}(\\beta_2)}\n\\]\nLet \\(\\widehat {LR}(\\beta_1, \\beta_2)\\) be the approximate likelihood ratio under the normal approximation \\(\\beta \\sim N(\\hat \\beta, s^2)\\).\nThe approximation to the likelihood ratio against the null used in ABF is simply\n\\[\n\\widehat{LR}_{ABF}(\\beta) = \\widehat{LR}(\\beta, 0)\n\\] However if we note that\n\\[\nLR(\\beta, 0) = LR(\\beta, \\beta^*) \\times LR(\\beta^*, 0)\n\\]\nwe can derive a family of approximations for the likelihood ratio against the null\n\\[\n\\widehat{LR}_{\\beta^*}(\\beta) = \\widehat{LR}(\\beta, \\beta^*) LR(\\beta^*,0)\n\\] And note that \\(\\widehat{LR}_{ABF}\\) is a special case where \\(\\beta^*=0\\). We’ve seen that this choice makes the approximation good in the tail– which isn’t very important for getting an accurate approximate BF. Perhaps the best choice would be make the approximate LR match the exact LR at the MAP estimate. But a second good choice, and one that does not require knowledge of the prior could be to set \\(\\beta^* = \\beta_{MLE}\\). This choice is shown in blue on the updated plot\n\n\nCode\ncompare_lrs(sim, prior_variance = 1, k=1.5, plot_correction=T)\n\n\n\n\n\nWe can use the"
  },
  {
    "objectID": "research/random_intercepts/index.html",
    "href": "research/random_intercepts/index.html",
    "title": "Random intercepts",
    "section": "",
    "text": "Code\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(tidyr)\nlibrary(kableExtra)\n\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\nWhat happens when you ignore the randomness in a random intercept model? Here is a quick demonstration to show that the behavior is very different for linear regression and logistic regression."
  },
  {
    "objectID": "research/random_intercepts/index.html#linear-regression",
    "href": "research/random_intercepts/index.html#linear-regression",
    "title": "Random intercepts",
    "section": "Linear regression",
    "text": "Linear regression\nFor linear models if the intercept \\(b_0 \\sim N(\\mu_0, \\sigma^2_0)\\) then fitting the linear regression will just have the effect of having a larger residual variance. Standard statistical software estimates the residual variance, and for modest sample size compared to the number of features this can be done quite well.\n\\[\ny = b_0 + b x + \\tilde\\epsilon; \\tilde\\epsilon \\sim N(0, \\sigma^2_0 + \\sigma^2)\n\\]\n\\[\ny = \\mu_0 + b x + \\tilde\\epsilon; \\tilde\\epsilon \\sim N(0, \\sigma^2_0 + \\sigma^2)\n\\]\nSo we should expect OLS to give us unbiased estimate of the intercept and effect.\n\n\nCode\nlist2tibble <- function(l){\n    dplyr::tibble(!!!l)\n}\n\nsimulation_linear <- function(n, b0, b, std0, std){\n    std <- sqrt(std0^2 + std^2)\n    x <- rnorm(n)\n    y <- b0 + x * b + rnorm(n) * std\n    fit <- lm(y ~ x)\n    coef <- summary(fit)$coef\n    res <-list(n=n, b0 = b0, b=b, std0 = std0, b0hat = coef[1, 1], b0_shat=coef[1, 2], bhat = coef[2,1], shat = coef[2, 2]) |> list2tibble()\n    return(res)\n}\n\nsimrep_linear <- function(k, std0){\n    purrr::map_dfr(1:k, ~simulation_linear(100, -1, 1, std0, 1))\n}\n\ncap <- 'The random intercept is not a problem for linear regression, the effect estimates will remain unbiased'\n\nstd0 <- c(0, 0.25, 0.5, 1.0, 2.0, 4.0, 8.0)\nres <- purrr::map_dfr(std0, ~simrep_linear(100, .x))\nres %>%\n    rowwise() %>%\n    mutate(covered = (b < bhat + 2*shat) & (b > bhat - 2*shat)) %>%\n    ungroup() %>%\n    group_by(std0) %>%\n    summarize(coverage=mean(covered)) %>%\n    kbl(caption=cap) %>%\n    kable_classic(full_width = T, html_font = \"Cambria\")\n\n\n\n\nThe random intercept is not a problem for linear regression, the effect estimates will remain unbiased\n \n  \n    std0 \n    coverage \n  \n \n\n  \n    0.00 \n    0.95 \n  \n  \n    0.25 \n    0.96 \n  \n  \n    0.50 \n    0.95 \n  \n  \n    1.00 \n    0.97 \n  \n  \n    2.00 \n    0.95 \n  \n  \n    4.00 \n    0.93 \n  \n  \n    8.00 \n    0.96"
  },
  {
    "objectID": "research/random_intercepts/index.html#logistic-regression",
    "href": "research/random_intercepts/index.html#logistic-regression",
    "title": "Random intercepts",
    "section": "Logistic regression",
    "text": "Logistic regression\nFor logistic regression, and GLMs generally, it is a different story.\n\n\nCode\nsimulation_logistic <- function(n, b0, b, std0){\n    x <- rnorm(n)\n    logit <- b0 + x * b + rnorm(n) * std0\n    y <- rbinom(n, 1, 1 / (1 + exp(-logit)))\n    fit <- glm(y ~ x, family='binomial')\n    coef <- summary(fit)$coef\n    res <-list(n=n, b0 = b0, b=b, std0 = std0, b0hat = coef[1, 1], b0_shat=coef[1, 2], bhat = coef[2,1], shat = coef[2, 2]) |> list2tibble()\n    return(res)\n}\n\nsimrep <- function(k, std0){\n    purrr::map_dfr(1:k, ~simulation_logistic(100, -1, 1, std0))\n}\n\nstd0 <- c(0, 0.25, 0.5, 1.0, 2.0, 4.0, 8.0)\n\nres <- purrr::map_dfr(std0, ~simrep(100, .x))\n\ncap <- 'As the variance of the random intercept increases, the coverage rate of the 95% C.I. decreases'\n\nres %>%\n    rowwise() %>%\n    mutate(covered = (b < bhat + 2*shat) & (b > bhat - 2*shat)) %>%\n    ungroup() %>%\n    group_by(std0) %>%\n    summarize(coverage=mean(covered)) %>%\n    kbl(caption=cap) %>%\n    kable_classic(full_width = T, html_font = \"Cambria\")\n\n\n\n\nAs the variance of the random intercept increases, the coverage rate of the 95% C.I. decreases\n \n  \n    std0 \n    coverage \n  \n \n\n  \n    0.00 \n    0.95 \n  \n  \n    0.25 \n    0.93 \n  \n  \n    0.50 \n    0.90 \n  \n  \n    1.00 \n    0.97 \n  \n  \n    2.00 \n    0.60 \n  \n  \n    4.00 \n    0.21 \n  \n  \n    8.00 \n    0.08"
  },
  {
    "objectID": "research/quadrature_for_peaked_functions/index.html",
    "href": "research/quadrature_for_peaked_functions/index.html",
    "title": "Quadrature for peaked functions",
    "section": "",
    "text": "Up until now, we have been fixing the intercept at the MLE– which requires fitting the logistic regression anyways. To do a quadrature only approximation of the BF and posterior mean, we would need to do a 2d quadrature over both the effect and the intercept (with a flat prior)\nFor likelihoods that are very peaked, I think it is easy for quadrature rules to miss the most important parts of the integral. This continues to be true of adaptive quadrature methods. Adaptive quadratures typically divide the integration region into subintervals and perform a low-order quadrature on each subinterval. It then performs a a higher-order quadrature (ideally nested, i.e. reusing the evaluations from the low-order quadrature). If the difference between the low and high order quadrature is small the procedure stops, otherwise it subdivides the interval. If both evaluations miss the high-density peak (don’t evaluate the function at points in the peak), the integral will be severely underestimated. If the evaluation points include the high density regions, performance is recovered.\nIn short, it seems that performing quadrature well involves requires knowing where there is non-negligible density. When the likelihood is peaked, picking a quadrature rule that has evaluation points in the high density basically requires at least approximately optimizing the function.\n\nIn the below example we try to numerically integrate the density function for \\(N(0, \\sigma^2)\\) with \\(\\sigma = 0.01\\). This is super peaky, with almost all the density around 0. We call stats::integrate, which implements an adaptive quadrature rule, using various intervals of the form \\([-5 + \\epsilon, 5+\\epsilon]\\), for \\(\\epsilon = 0.05k, \\;\\; k \\in \\{0, \\dots, 19\\}\\) We see that for most settings the adaptive quadrature rule approximates the integral as evaluating to \\(0\\). Only a small number of intervals (e.g. \\(\\epsilon = 0\\)) do we have an evaluation point close enough to \\(0\\) to pick up the peak.\n\n\nCode\nmake_density_functions <- function(mean, sd){\n  f <- function(x){\n    dnorm(x, mean = mean, sd = sd)\n  }\n  return(f)\n}\n\nf <- make_density_functions(0, 0.01)\n\n# only some settings of the interval accurately approximate the interval\n# these happen to take quadrature points in the high denisty peak\neps <- seq(0.1, 1, by=0.05)\nfor(e in eps){\n  int <- integrate(f, -5 + e, 5 + e)\n  res <- paste('eps = ', e, ' I =', int$value, ' absolute error = ', int$abs.error, collapse = NULL)\n  print(res)\n}\n\n\n[1] \"eps =  0.1  I = 0.999999999999999  absolute error =  2.00000737316951e-07\"\n[1] \"eps =  0.15  I = 0.999999999999892  absolute error =  3.15982721577762e-05\"\n[1] \"eps =  0.2  I = 0.999999999999506  absolute error =  5.75015101922371e-05\"\n[1] \"eps =  0.25  I = 2.47857565447408e-12  absolute error =  4.82102731481746e-12\"\n[1] \"eps =  0.3  I = 1.00000000000018  absolute error =  5.60799437807659e-05\"\n[1] \"eps =  0.35  I = 1.40171384460606e-05  absolute error =  2.65009793238846e-05\"\n[1] \"eps =  0.4  I = 2.23240368349137e-08  absolute error =  4.29728852052684e-08\"\n[1] \"eps =  0.45  I = 1.35504815537514e-20  absolute error =  2.58390642318225e-20\"\n[1] \"eps =  0.5  I = 1  absolute error =  2.78510096202405e-08\"\n[1] \"eps =  0.55  I = 1.00000000000003  absolute error =  1.44958923085081e-05\"\n[1] \"eps =  0.6  I = 1.21967660615216e-05  absolute error =  2.3257702739113e-05\"\n[1] \"eps =  0.65  I = 2.27134937806163e-22  absolute error =  4.33117831277196e-22\"\n[1] \"eps =  0.7  I = 4.75672475153124e-22  absolute error =  0\"\n[1] \"eps =  0.75  I = 1.92254994128285e-05  absolute error =  3.63479760707719e-05\"\n[1] \"eps =  0.8  I = 1.77175730319243e-14  absolute error =  3.19744231092045e-14\"\n[1] \"eps =  0.85  I = 0.999999999999898  absolute error =  7.47222853081375e-06\"\n[1] \"eps =  0.9  I = 9.10781435973747e-21  absolute error =  1.72193507837645e-20\"\n[1] \"eps =  0.95  I = 4.16499572039637e-44  absolute error =  7.81290905495735e-44\"\n[1] \"eps =  1  I = 1.84550997515897e-18  absolute error =  3.4631142762742e-18\""
  },
  {
    "objectID": "research/constrained_multinomial_stickbreaking/index.html",
    "href": "research/constrained_multinomial_stickbreaking/index.html",
    "title": "Constrained multinomial stick-breaking",
    "section": "",
    "text": "Code\nsigmoid <- function(x){1/(1 + exp(-x))}\n\ntilde_pi2pi <- function(tpi){\n  tmp <- c(1, head(cumprod(1 - tpi), -1))\n  pi <- tmp * tpi\n  pi <- c(pi, (1 - sum(tmp * pi)))\n  return(pi)\n}\n\nmake_pi <- function(K, b0, b, x){\n  psi <-  do.call(cbind, purrr::map(b0, ~b + x + .x))\n  tilde_pi <- sigmoid(psi)\n  tpi <- tilde_pi[12, ]\n  pi <- do.call(rbind, purrr::map(1:nrow(tilde_pi), ~tilde_pi2pi(tilde_pi[.x,])))\n  return(pi)\n}\n\nplot_pi <- function(pi, idx, x){\n  par(mfrow = c(1, length(idx)))\n  K <- ncol(pi)\n  for(i in idx){\n    plot(1:K, pi[i,], type = 'b', xlab = 'K', ylab = 'prob', main=paste0('x = ', x[i]))\n  }\n}\n\n\n\nShared\n\\(\\psi_k \\equiv \\psi\\; \\forall\\  k \\in[0, K-1]\\)\n\n\nCode\nK <- 20\nb0 <- rep(0, K)\nb <- 1\nx <- seq(-3, 3.2, by=0.2)\npi <- make_pi(K, b0, b, x)\nplot_pi(pi, c(6, 11, 21, 26), x)\n\n\n\n\n\n\n\nFixed prediction, seperate intercept\n\n\nCode\nK <- 10\nb0 <- rep(0, K)\nb <- 1\nx <- seq(-3, 3, by=0.2)\npi <- make_pi(K, b0, b, x)\nplot_pi(pi, c(6, 11, 21, 26), x)\n\n\n\n\n\n\n\nCode\nK <- 10\nb0 <- rnorm(10)\nb <- 1\nx <- seq(-3, 3, by=0.2)\npi <- make_pi(K, b0, b, x)\nplot_pi(pi, c(6, 11, 21, 26), x)\n\n\n\n\n\n\n\nCode\nK <- 10\nb0 <- 1:K\nb <- 1\nx <- seq(-5, 5, by=0.1)\npi <- make_pi(K, b0, b, x)\nplot(pi[40,])\n\n\n\n\n\n\n\nCode\nK <- 10\nb0 <- rnorm(K)\nb <- 1\nx <- seq(-5, 5, by=0.1)\npi <- make_pi(K, b0, b, x)\n\npar(mfrow = c(2, 3))\nplot(pi[10,])\nplot(pi[20,])\nplot(pi[30,])\nplot(pi[40,])\nplot(pi[50,])\nplot(pi[60,])"
  },
  {
    "objectID": "research/multinomial_stickbreaking/index.html",
    "href": "research/multinomial_stickbreaking/index.html",
    "title": "Multinomial stick-breaking",
    "section": "",
    "text": "We’re interested in modelling multinomial or categorical data in the case where the probability of each category depends on side information. For \\(\\pi: \\mathcal X \\rightarrow \\Delta^{K-1}\\)\n\\[\n{\\bf y} \\sim \\text{Multinomial}(n, {\\bf \\pi}({\\bf x})) \\\\\n\\]\nCommonly \\(\\pi({\\bf x})\\) is written as a composition \\(\\pi = \\sigma \\circ \\eta\\), where \\(\\sigma: \\mathbb R^K \\rightarrow \\Delta^{K-1}\\) is the softmax functions defined element wise as \\(\\sigma({\\bf \\eta})_i = \\left(\\frac{e^{\\eta_i}}{\\sum_{j=1}^K e^{\\eta_j}}\\right)\\), and \\(\\eta:\\mathcal X \\rightarrow \\mathbb R^K\\) is some other function mapping the covariates \\({\\bf x}\\) to a set of unormalized log probabilities.\nThe trouble with this formulation is it is not easy to express uncertainty in the map \\(\\eta\\). As a simple example consider multinomial linear regression where \\(\\eta(z)_k = \\beta_k^T z\\) for some \\(\\beta_k \\in \\mathbb R^d\\). \\(\\pi = \\sigma \\circ \\eta\\) is differential, and point estimates of \\(B =\\{\\beta_k\\}_{k=1, \\dots, K}\\) could be obtained through gradient based optimization. In contrast if we take a Bayesian approach and specify a prior on \\(\\beta_k \\sim g_k\\; k \\in [K]\\) obtaining the posterior distribution over \\(B\\) involves evaluating a nasty integral of the soft max.\n\\[\n\\int_{B} \\sigma(\\eta(z ; B)) dB\n\\]\nThere is plenty of work on bounding softmax with functions that are easier to integrate (Bouchard, n.d.; Titsias RC AUEB 2016), but it is hard problem to get anlytic bounds that are easy to work.\nThere is also quite a bit of work developing bounds for the sigmoid function (softmax with \\(K=2\\), usually people describe softmax as a generalization of sigmoid to \\(K > 2\\)). In particular, techniques for constructing local approximations are popular in variational inference (Jaakkola and Jordan, n.d.; Saul and Jordan 1998). These local approximations are tight at a point, but the quality of the bound decays as you get far from that point. Thus, these approximation techniques require selecting/optimizing at what point the bound is tight.\nWe’re operating under the assumption that it is easier to construct good bounds for the sigmoid function compared to the softmax function. We are going to explore a construction of the Categorical/Multinomial distribution that let us utilize these bounds."
  },
  {
    "objectID": "research/multinomial_stickbreaking/index.html#multinomial-stick-breaking",
    "href": "research/multinomial_stickbreaking/index.html#multinomial-stick-breaking",
    "title": "Multinomial stick-breaking",
    "section": "Multinomial stick breaking",
    "text": "Multinomial stick breaking\nThe the multinomial logit construction \\(\\eta\\) is a set of unnormalized log probabilities This is not the only way to construct a multinomial distribution. We can also use a stick breaking construction. In stick breaking we start with a “stick” of length \\(1\\). At the first step we break off a fraction of the stick \\(p_1\\). The remainder of the stick is now length \\(1 - p_1\\). At each successive step we break off a fraction of the remaining stick. After \\(K-1\\) breaks we have broken the stick into \\(K\\) pieces, giving a discrete probability distribution over \\(K\\) categories. Clearly, we can use this process to construct and distribution \\(\\pi\\) over \\(K\\) categories where\n\\[\n\\begin{aligned}\n\\pi_1 &= p_1 \\\\\n\\pi_k &= p_k \\prod_{j < k}(1 - p_k)\n\\end{aligned}\n\\]\nNoting that \\(\\left(1 - \\sum_{j < k} \\pi_j \\right)\\) is the length of the remaining stick after \\(k-1\\) breaks, we can also write\n\\[\n\\begin{aligned}\n\\pi_k &= p_k \\left(1 - \\sum_{j < k} \\pi_j \\right)\n\\end{aligned}\n\\]\nIn the stick breaking construction, \\(\\nu_k,\\; k \\in[K-1]\\) will be a set of log odds such that \\(p_k = \\sigma(\\nu_k)\\) gives the proportion of the stick broken off at step \\(k\\). Using the stick breaking constructiong we can write the multinational pmf as a product of binomial pmfs.\n\\[\n\\text{Multinomial}({\\bf y}; n, \\pi) = \\prod_{k=1}^{K-1} \\text{Binomial}(y_k; n_k, p_k)\n\\]\nWhere \\(n_k = n - \\sum_{j < k} y_j\\) counts the number of remaining trials, conditional on the first \\(k-1\\) draws. This constructing is not new, it has been proposed by several authors (Khan et al. 2012; Linderman, Johnson, and Adams, n.d.).\nTo do multinomial regression we will write \\(\\nu_k = \\beta_k^T {\\bf z}\\). \\(nu_k\\) gives the log odds of selecting category \\(k\\) given that we did not select category \\(1, \\dots, k-1\\)."
  },
  {
    "objectID": "research/multinomial_stickbreaking/index.html#stick-breaking-for-variational-inference",
    "href": "research/multinomial_stickbreaking/index.html#stick-breaking-for-variational-inference",
    "title": "Multinomial stick-breaking",
    "section": "Stick breaking for variational inference",
    "text": "Stick breaking for variational inference\nThe stick breaking construction is particular useful for variational inference. The multinomial log likelihood can be written as a sum of \\(K-1\\) terms, each a binomial log-likelihood. By selecting a variational approximation where the \\(\\nu_k\\) factorize, the variational objective can be optimized in an embarrassingly parallel fashion– the multinomial regression reduces to a set of \\(K-1\\) independent binomial regression problems. Each of these problems still requires additional approximation of the sigmoid function for tractable inference, but these can be dealt with more easily."
  },
  {
    "objectID": "research/multinomial_stickbreaking/index.html#a-distribution-of-pi",
    "href": "research/multinomial_stickbreaking/index.html#a-distribution-of-pi",
    "title": "Multinomial stick-breaking",
    "section": "A distribution of \\(\\pi\\)",
    "text": "A distribution of \\(\\pi\\)\nWhile stick breaking can be used to construct any discrete distribution, we should take note that the distribution on \\(\\pi\\) is dependent on the distribution we specify for the breakpoints and\nThe Dirichlet \\(Dir((\\alpha_1, \\dots, \\alpha_K))\\) can be constructed through stick breaking, where the break points are\n\\[p_k \\sim Beta(\\alpha_k, \\sum_{j > k } \\alpha_j)\\]\nAgain \\(\\pi_1 = p_1\\), and \\(p_k = (1 - \\sum_{j < k} \\pi_j) p_k\\). If \\(\\alpha_i = \\alpha\\; \\forall i \\in [K]\\) then then the Dirichlet is said to be symmetric– permuting category labels won’t change the likelihood of the sample. Notice that in this case \\(p_k \\sim Beta (\\alpha, (K- k) \\alpha)\\). We should expect to break off smaller fractions of the stick for small \\(k\\) than for large \\(k\\). This makes sense. A necessary condition for the Dirichlet to be exchangeable is that the stick lengths have the same marginal distribution. In order for the stick lengths to have the same marginal distribution, at each successive step we need to balance out the fact that the stick is getting shorter by taking larger fraction of the stick at each step (ultimately \\(\\mathbb E[p_{K-1}] = \\frac{1}{2}\\)).\nIn the code below we simulate the Dirichlet distribution using stick breaking with a Beta distribution. We see that across 10000 simulations each category is equally likely to show up on top.\n\n\nCode\n#' Sample from a Dirichlet distribution using the stick breaking construction\ndirichlet_from_beta_stick_breaking <- function(alpha, K){\n  if(length(alpha) == 1){ \n    alpha <- rep(alpha, K)\n  }\n  beta <- rev(cumsum(rev(alpha))) - alpha # sum {j < k} \\alpha_j\n  p <- rbeta(K, alpha, beta)\n  tmp <- c(1, head(cumprod(1 - p), -1))\n  pi <- c(p * tmp)\n  return(pi)\n}\n\n# each component equally likely to have the most probability mass\ntable(purrr::map_int(1:10000, ~which.max(\n  dirichlet_from_beta_stick_breaking(1, 4))))\n\n\n\n   1    2    3    4 \n2458 2539 2492 2511 \n\n\nTODO: sample for \\(K=3\\)\nQ: What distribution of \\(\\nu_k\\) would give an exchangeable distribution for \\(\\pi\\) (basically, what is the stick-breaking construction for a symmetric Dirichlet?)"
  },
  {
    "objectID": "research/multinomial_stickbreaking/index.html#ordering-of-the-categories",
    "href": "research/multinomial_stickbreaking/index.html#ordering-of-the-categories",
    "title": "Multinomial stick-breaking",
    "section": "Ordering of the categories",
    "text": "Ordering of the categories\nSuccessive categories seem to have less and less information, as \\(n_k \\leq n_j\\) for \\(k > j\\). It seems odd that permuting the category labels would change how certain we are about each \\(\\nu_k\\). Can we make sense of this?"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research Notes",
    "section": "",
    "text": "Constrained multinomial stick-breaking\n\n\n\n\n\nMultinomial stickbreaking is an easy\n\n\n\n\n\n\nApr 1, 2023\n\n\nKarl Tayeb\n\n\n\n\n\n\n\n\nImplementation polynomial approximation VB\n\n\n\n\n\nRough implementation of of approximate VB regression, where we approximate the data likelihood with a polynomial. We implement a mean field gaussian variational approximation. We demonstrate the method on Gaussian linear model (no approximation), and Bernoulli-logit (logistic regression) and Poisson-log (poisson regression) models with varying approximation degree.\n\n\n\n\n\n\nMar 15, 2023\n\n\nKarl Tayeb\n\n\n\n\n\n\n\n\nMode seeking in mean field VB for RSS + sparse prior\n\n\n\n\n\nUsing polynomial approximations to perform Bayesian regression\n\n\n\n\n\n\nApr 1, 2023\n\n\nKarl Tayeb\n\n\n\n\n\n\n\n\nMultinomial stick-breaking\n\n\n\n\n\nExploration of multinomial stickbreaking\n\n\n\n\n\n\nMar 19, 2023\n\n\nKarl Tayeb\n\n\n\n\n\n\n\n\nPolynomial approximation SuSiE\n\n\n\n\n\nWe extend the sum of single effects regression to support arbitrary likelihood and link function by representing (approximately) the log likelihood of each observations as a polynomial in the linear prediction. Once this approximation is made we can treat inference in multiple likelihoods with a uniform inference procedure. Furthermore, we can make the polynomial approximation arbitrarily precise by increasing the degree of the approximation.\n\n\n\n\n\n\nMar 15, 2023\n\n\nKarl Tayeb\n\n\n\n\n\n\n\n\nPolynomial approximation for variational Bayes\n\n\n\n\n\nUsing polynomial approximations to perform Bayesian regression\n\n\n\n\n\n\nMar 15, 2023\n\n\nKarl Tayeb\n\n\n\n\n\n\n\n\nQuadrature for peaked functions\n\n\n\n\n\n\n\n\n\n\n\n\nJun 28, 2023\n\n\n\n\n\n\n\n\nRandom intercepts\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nWakefield ABF Accuracy\n\n\n\n\n\nWe noticed that the error in Wakefield’s ABF grows with increasing sample size. This is surprising, because at these large sample sizes asymptotic normality kicks in. The issue can be more clearly seen by considering the ABF as an integral over an approximation to the likelihood ratio. The approximation used in the ABF ensure that the approximate likelihood ratio at \\(0\\) is 1, but that the curvature and mode of the approximation are determined by the asymptotic distribution of the MLE. This leads to a gap in the approximate likelihood ratio and the exact likelihood ratio around the mode. As the normal approximation becomes more peaked, the accuracy of the ABF is increasingly determined by the quality of the likelihood ratio approximation in a small neighborhood around the mode which is off by this gap, which appears to grow with sample size.\n\n\n\n\n\n\nApr 25, 2023\n\n\nKarl Tayeb\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Karl Tayeb’s Website",
    "section": "",
    "text": "This is my research website. For now, that means a collection of notes that are useful to me. Feel free to look around."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! I’m Karl Tayeb. I’m a PhD Student at the University of Chicago working with (Matthew Stephens)[https://stephenslab.uchicago.edu/] and Yoav Gilad. Before starting graduate school I\nThis is my research website. For now, that means a collection of notes that are useful to me. Feel free to look around."
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#aim-0",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#aim-0",
    "title": "Committee Meeting",
    "section": "Aim 0",
    "text": "Aim 0\nDevelop general approaches for applying SuSiE to a wide class of non-Gaussian models.\n\n\nNeed good version of logistic SuSiE (and multinomial SuSiE) for GSEA and covariate moderated FDR application (Aim 1 and 2)\nWe identified serious limitations in our old approach\nTwo new approaches, both extend beyond the logistic case"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#sum-of-single-effects-regression-susie",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#sum-of-single-effects-regression-susie",
    "title": "Committee Meeting",
    "section": "Sum of single effects regression (SuSiE)",
    "text": "Sum of single effects regression (SuSiE)\n\\({\\bf y} \\in \\mathbb R^n\\), \\(X \\in \\mathbb R^{n \\times p}\\)\nSum of single effects: there are at most \\(L\\) non-zero effects\n\\[\\begin{align}\n{\\bf y} &\\sim N(X\\beta, {\\bf I}_n \\sigma^2) \\\\\n\\beta &= \\sum_{l=1}^L b_l \\gamma_l \\\\\nb_l &\\sim N(0, \\sigma^2_0) \\\\\n\\gamma_l &\\sim \\text{Multinomial}(1, \\pi)\n\\end{align}\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#what-makes-susie-work",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#what-makes-susie-work",
    "title": "Committee Meeting",
    "section": "What makes SuSiE work?",
    "text": "What makes SuSiE work?\nWe will quickly review variational inference for SuSiE.\nTwo key ingredients:\n\nVariational approximation \\(q(\\beta) = \\prod_l q(\\beta_l)\\)\nGaussian likelihood reduces coordinate updates to solving a SER"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#single-effect-regression-ser",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#single-effect-regression-ser",
    "title": "Committee Meeting",
    "section": "Single Effect Regression (SER)",
    "text": "Single Effect Regression (SER)\n\n\nModel\nSingle effect regression assumes there is exactly one non-zero effect.\n\\({\\bf y} \\in \\mathbb R^n\\), \\(X \\in \\mathbb R^{n \\times p}\\) \\[\n\\begin{aligned}\n{\\bf y} &\\sim N(X\\beta, {\\bf I}_p \\sigma^2_0) \\\\\n\\beta &= b \\gamma \\\\\nb &\\sim N(0, \\sigma^2_0)\\\\\n\\gamma &\\sim \\text{Multinomial}(1, \\pi)\n\\end{aligned}\n\\]\n\nInference is easy\n\nCompute the posterior for \\(j = 1,\\dots , p\\) separate univariate regression \\[p(b | X, {\\bf y}, \\gamma_j = 1)\\]\nCompute posterior inclusion probabilities\n\n\\[\np(\\gamma_j = 1 | {\\bf y}, X) \\propto BF_j \\pi_j\n\\]\n\\[\nBF_j = \\frac{p({\\bf y} | X, \\gamma_j=1 )}{p({\\bf y}  | \\beta=0)}\n\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#variational-inference",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#variational-inference",
    "title": "Committee Meeting",
    "section": "Variational inference",
    "text": "Variational inference\nInference as an optimization problem:\n\\[\\begin{align}\np(\\beta | {\\bf y}, X) = \\arg \\max_q F(q),\n\\quad F(q) := \\mathbb E_q \\left[ \\log p ({\\bf y} | X, \\beta) \\right] - KL[q || p]\n\\end{align}\\]\n\n\\[\\begin{align}\nF(q)\n&= \\mathbb E_q \\left[ \\log p ({\\bf y} | X, \\beta) \\right] - KL[q || p] \\\\\n&= \\mathbb E_q \\left[ \\log p ({\\bf y},  \\beta | X) \\right] + H(q) \\\\\n&= \\left(\\mathbb E_q \\left[ \\log p ({\\bf y},  \\beta | X) - Z \\right] + H(q) \\right) + Z \\\\\n&= - KL[q || p_{post}] + Z\n\\end{align}\\]\n\n\nWe’ll write the variational objective for SER \\(F_{SER}(q; {\\bf y}, X, \\sigma_0)\\)"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#susie-model",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#susie-model",
    "title": "Committee Meeting",
    "section": "SuSiE: model",
    "text": "SuSiE: model\n\\[\n\\begin{aligned}\ny &\\sim N(X\\beta, \\sigma^2_0) \\\\\n\\beta &= \\sum_l b_l \\gamma_l \\\\\nb_l &\\sim N(0, \\sigma^2_0)\\\\\n\\gamma_l &\\sim \\text{Multinomial}(1, \\pi)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#susie-variational-approximation",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#susie-variational-approximation",
    "title": "Committee Meeting",
    "section": "SuSiE: variational approximation",
    "text": "SuSiE: variational approximation\nRestrict \\(q\\) to some family \\(\\mathcal Q\\)\n\\[\\begin{align}\nq^*(\\beta) = \\arg \\max_{q \\in \\mathcal Q} F(q)\n\\end{align}\\]\nSuSiE uses \\(\\mathcal Q = \\{q : q(\\beta) = \\prod_l q_l(\\beta_l)\\}\\)\n\n\\[\nF_{SuSiE}(q; X, {\\bf y}) = \\mathbb E_{q} \\left[\n      -\\frac{1}{2\\sigma^2}||{\\bf y} - {\\bf X} \\beta_{-l} - X\\beta_l||^2_2)\n  \\right] + KL[q || p_{SuSiE}]\n\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#susie-coordinate-ascent-ibss",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#susie-coordinate-ascent-ibss",
    "title": "Committee Meeting",
    "section": "SuSiE: coordinate ascent (IBSS)",
    "text": "SuSiE: coordinate ascent (IBSS)\nDefine the residual \\({\\bf r}_l = {\\bf y} - X\\beta_{-l}\\)\n\n\\[\n\\begin{aligned}\nF_{SuSiE}(q_l; q_{-l}, X, {\\bf y})\n&= \\mathbb E_{q_l} \\left[\n  \\mathbb E_{q_{-l}} \\left[\n      -\\frac{1}{2\\sigma^2}||{\\bf y} - {\\bf X} \\beta_{-l} - X\\beta_l||^2_2)\n  \\right]\n\\right] - KL[q || p] \\\\\n&= \\mathbb E_{q_l} \\left[\n  \\mathbb E_{q_{-l}} \\left[\n      -\\frac{1}{2\\sigma^2}|| {\\bf r}_l - X\\beta_l||^2_2)\n  \\right]\n\\right] - KL[q_l || p_l]  + C_1 \\\\\n&= \\mathbb E_{q_l} \\left[ \\color{blue}{\n    -\\frac{1}{2\\sigma^2} \\left(|| \\mathbb E_{q_{-l}} {\\bf r}_l- X\\beta_l||^2_2 + \\mathbb V ||r_l||^2_2 \\right)}\n\\right] - KL[q_l || p_l]  + C_1\\\\\n&= \\mathbb E_{q_l} \\left[\n    -\\frac{1}{2\\sigma^2} || \\mathbb E_{q_{-l}} {\\bf r}_l- X\\beta_l||^2_2]\n\\right] - KL[q_l || p_l]  + C_2  \\\\\n&= F_{SER}(q_l; X, \\mathbb E_{q{-l}}{\\bf r}_l) + C_2\n\\end{aligned}\n\\]\n\n\nKey: coordinate updates in \\(\\mathcal Q\\) reduce to solving an SER"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#generalizing-susie",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#generalizing-susie",
    "title": "Committee Meeting",
    "section": "Generalizing SuSiE",
    "text": "Generalizing SuSiE\nSimplification depends on properties of the Gaussian log likelihood\n\\[\n\\begin{aligned}\n\\log p(y | {\\bf x}, \\beta) = y ({\\bf x}^T \\beta) - \\log(1 + \\exp({\\bf x}^T \\beta))\n\\end{aligned}\n\\]\n\nNo simplification when taking expectations over \\(q_{-l}\\)\n\\[\n\\begin{aligned}\n\\mathbb E_{q_{-l}} \\left[ \\log p(y | {\\bf x}, \\beta) \\right]\n= y ({\\bf x}^T \\beta_l + {\\bf x}^T  \\bar \\beta_{-l}) - \\color{red}{\\mathbb E_{q_{-l}}\\left[\\log(1 + \\exp({\\bf x}^T \\beta)) \\right]}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#jaakkola-jordan-bound",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#jaakkola-jordan-bound",
    "title": "Committee Meeting",
    "section": "Jaakkola-Jordan bound",
    "text": "Jaakkola-Jordan bound\nIdea: construct local approximation to the log-likelihood. Tune the approximation to be tight near the posterior mode.\n\nFor all \\(\\xi \\in \\mathbb R\\), \\(\\psi = {\\bf x}^T \\beta\\)\n\\[\n\\log p(y | \\psi) \\geq \\frac{1}{2} \\log \\sigma(\\xi) +  \\frac{1}{2} \\left((2y -1) \\psi - \\xi\\right) - \\frac{1}{4\\xi}\\tanh(\\frac{\\xi}{2}) (\\psi^2 - \\xi^2)\n\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#jj-bound-is-bad-for-variable-selection",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#jj-bound-is-bad-for-variable-selection",
    "title": "Committee Meeting",
    "section": "JJ Bound is bad for variable selection",
    "text": "JJ Bound is bad for variable selection\n\n\nNaive application of JJ bound to SER uses very loose ELBO for most variables\n\n\n\n\n\n\n\n\n\nWinner take all: Coverage of 90% credible sets across 1k SER simulations\n\n\nmethod\ncoverage\n\n\n\n\nuvb_ser\n0.945\n\n\nvb_ser\n0.845"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#simulations-vb-logistic-susie-l5",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#simulations-vb-logistic-susie-l5",
    "title": "Committee Meeting",
    "section": "Simulations: VB logistic SuSiE \\(L=5\\)",
    "text": "Simulations: VB logistic SuSiE \\(L=5\\)\nVarying effect sizes, correlation structure, and num. of non-zero effects"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#overview",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#overview",
    "title": "Committee Meeting",
    "section": "Overview",
    "text": "Overview\n\n\nHeuristic approach for applying SuSiE to non-Gaussian models\nIdea: apply IBSS, using expected predictions as fixed offsets\n\n\n\n\\[\\begin{align}\nF_{SuSiE}(q)\n&= \\mathbb E_q[\\log p({\\bf y} | X, \\beta_l, \\beta_{-l})] - \\sum_l KL[q_l || p_l] \\\\\n&= \\mathbb E_{q_l}[ \\mathbb E_{q_{-l}}[\\log p({\\bf y} | X, \\beta_l, \\beta_{-l})]] - \\sum_l KL[q_l || p_l] + C_1 \\\\\n&\\color{purple}{\\approx \\mathbb E_{q_l}[\\log p({\\bf y} | X, \\beta_l, \\bar \\beta_{-l})] - KL[q_l || p_l] + C_2} \\\\\n&= F_{SER}(q_l; {\\bf y}, X, \\bar \\beta_{-l}) + C_2\n\\end{align}\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#why-should-this-work",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#why-should-this-work",
    "title": "Committee Meeting",
    "section": "Why should this work?",
    "text": "Why should this work?\n\\[\\begin{align}\nF_{SuSiE}(q)\n&= \\mathbb E_q[\\log p({\\bf y} | X, \\beta_l, \\beta_{-l})] - \\sum_l KL[q_l || p_l] \\\\\n&\\color{purple}{\\approx \\mathbb E_{q_l}[\\log p({\\bf y} | X, \\beta_l, \\bar \\beta_{-l})] - KL[q_l || p_l] + C_2} \\\\\n\\end{align}\\]\n\n\nIf \\(\\log p( {\\bf y} | X, \\beta)\\) is quadratic in \\(\\beta\\) (Gaussian) there \\(\\color{purple}\\approx\\) is \\(=\\)\nInformally, approximation is good if \\(\\log p( {\\bf y} | X, \\beta)\\) is well approximated by a quadratic function in \\(\\beta\\).\nReally we care about \\(\\psi = X\\beta\\)"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#algorithm-single-effect-regression",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#algorithm-single-effect-regression",
    "title": "Committee Meeting",
    "section": "Algorithm: Single effect regression",
    "text": "Algorithm: Single effect regression\nRequire a function \\(G\\) which computes the BF and posterior mean of a Bayesian univariate regression\n\\[\\begin{align}\n{\\bf y} \\sim 1 + {\\bf x} + {\\bf o} \\\\\nb \\sim N(0, \\sigma^2)\n\\end{align}\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#algorithm-gibss",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#algorithm-gibss",
    "title": "Committee Meeting",
    "section": "Algorithm: GIBSS",
    "text": "Algorithm: GIBSS"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#simulations-logistic-gibss-l5",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#simulations-logistic-gibss-l5",
    "title": "Committee Meeting",
    "section": "Simulations: logistic-GIBSS \\(L=5\\)",
    "text": "Simulations: logistic-GIBSS \\(L=5\\)\nMuch better performance compared to direct VB approach"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#advantage-and-limitations",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#advantage-and-limitations",
    "title": "Committee Meeting",
    "section": "Advantage and limitations",
    "text": "Advantage and limitations\n\n\nAdvantages\n\nSeems to work well!\nHighly modular: just need to implement the univariate regression\n\n\n\n\nDisadvantages\n\nHeuristic– not optimizing a clear objective function\nDoes not properly account for uncertainty in \\(\\beta_{-l}\\), only uses posterior mean\n\n\n\n\nOpportunities\n\nCan we analyze the approximation error?\nCan we make guarantees on GIBSS performance asymptotically?"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#status",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#status",
    "title": "Committee Meeting",
    "section": "Status",
    "text": "Status\n\nImplemented in logisticsusie package github rep\nYunqi was able to use GIBSS code to help with survival SuSiE project\nPossibly implement GIBSS for GLMs in susieR package"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#overview-1",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#overview-1",
    "title": "Committee Meeting",
    "section": "Overview",
    "text": "Overview\n\nGIBSS requires computing the BF and posterior mean for each variable\nStandard statistical software for computing MLE\nFor large sample sizes, we can leverage asymptotic normality of the MLE to approximate posterior mean and BF."
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#posterior-mean",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#posterior-mean",
    "title": "Committee Meeting",
    "section": "Posterior mean",
    "text": "Posterior mean\n\\[\n\\begin{aligned}\n\\hat \\beta | \\beta, s^2 &\\sim N(\\beta, s^2) \\\\\n\\beta &\\sim N(0, \\sigma^2)\n\\end{aligned}\n\\]\n\n\\[\n\\beta | \\hat \\beta, s^2 \\sim N \\left(\n\\frac{\\sigma^2}{s^2 + \\sigma^2} \\hat\\beta, \\left(\\frac{1}{s^2} + \\frac{1}{\\sigma^2}\\right)^{-1}\n\\right)\n\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#approximating-the-the-bf",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#approximating-the-the-bf",
    "title": "Committee Meeting",
    "section": "Approximating the the BF",
    "text": "Approximating the the BF\n\\[BF = \\int \\color{red}{\\frac{ p(\\mathcal {\\bf y} | {\\bf x}, \\beta)}{p({\\bf y} | \\beta = 0)}} N(\\beta | 0, \\sigma^2) d\\beta\\]\nApproximate the likelihood ratio in a way that’s easy to integrate\n\\[\nLR(\\beta_1, \\beta_2) = \\frac{p(\\mathcal {\\bf y} | {\\bf x}, \\beta)}{p({\\bf y} | \\beta=0)}\n\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#wakefields-asymptotic-bf",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#wakefields-asymptotic-bf",
    "title": "Committee Meeting",
    "section": "Wakefields asymptotic BF",
    "text": "Wakefields asymptotic BF\nAsymptotically, \\(\\hat \\beta | \\beta, s^2 \\sim N(\\beta, s^2)\\). Then\n\\[\nLR(\\beta, 0)\n= \\frac{p(y | \\hat\\beta, \\beta) p(\\hat\\beta | \\beta)}{p(y | \\hat\\beta, \\beta = 0) p(\\hat\\beta | \\beta = 0)}\n\\approx \\color{red}{\\frac{p(y | \\hat\\beta)}{p(y | \\hat\\beta)}} \\frac{p(\\hat\\beta | \\beta)}{ p(\\hat\\beta | \\beta = 0)} \\approx \\frac{N(\\hat\\beta| \\beta, s^2)}{N(\\hat\\beta| 0, s^2)} = \\widehat{LR}_{ABF}(\\beta, 0).\n\\]\n\nIntegrating over the prior gives Wakefield’s asymptotic Bayes Factor (ABF)\n\\[\nABF = \\int \\widehat{LR}_{ABF}(\\beta, 0) N(\\beta | 0, \\sigma^2_0) d\\beta = \\frac{N(\\hat\\beta | 0, s^2 + \\sigma^2_0)}{N(\\hat\\beta | 0, s^2)}\n\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#a-problem-with-abf",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#a-problem-with-abf",
    "title": "Committee Meeting",
    "section": "A problem with ABF",
    "text": "A problem with ABF\n\nThe asymptotic approximation may not be a good in the tails\nAn issue for \\(\\hat\\beta/s >> 0\\)"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#adjusting-the-abf",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#adjusting-the-abf",
    "title": "Committee Meeting",
    "section": "Adjusting the ABF",
    "text": "Adjusting the ABF\nIdea: use the asymptotic approximation where it is good\n\\[\n\\begin{aligned}\nLR(\\beta, 0)\n&= LR(\\beta, \\hat\\beta) LR(\\hat\\beta, 0) \\\\\n&\\approx \\widehat{LR}_{ABF}(\\beta, \\hat\\beta)LR(\\hat\\beta, 0) \\\\\n&= \\widehat{LR}_{Lap}(\\hat\\beta, 0)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#corrected-abflaplace-approximation",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#corrected-abflaplace-approximation",
    "title": "Committee Meeting",
    "section": "Corrected ABF/Laplace approximation",
    "text": "Corrected ABF/Laplace approximation\n\nRequires knowledge of the LR of the MLE against the null.\nCan dramatically improve approximation of the BF"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#comparison-of-logbf",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#comparison-of-logbf",
    "title": "Committee Meeting",
    "section": "Comparison of logBF",
    "text": "Comparison of logBF"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#comparison-of-logbf-1",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#comparison-of-logbf-1",
    "title": "Committee Meeting",
    "section": "Comparison of logBF",
    "text": "Comparison of logBF"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#abf-correction-reorders-pips",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#abf-correction-reorders-pips",
    "title": "Committee Meeting",
    "section": "ABF Correction reorders PIPs",
    "text": "ABF Correction reorders PIPs"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#laplace-approximation",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#laplace-approximation",
    "title": "Committee Meeting",
    "section": "Laplace approximation",
    "text": "Laplace approximation\nFor some non-negative function \\(f: \\mathbb R \\rightarrow \\mathbb R^+\\), want to compute the integral\n\\[I = \\int f(x) dx,\\]\n\nDefine \\(h(x) = \\log f(x)\\), expand around the maximizer of \\(h\\), \\(x^*\\)\n\\[\\hat h (x) = h(x^*) + \\frac{1}{2} h^{''}(x^*)(x-x^*)^2\\]\n\n\nApproximate with the Gaussian integral\n\\[ \\hat I = \\int \\exp{\\hat h(x))} dx = \\exp{\\hat h(x^*)} \\left(-\\frac{2\\pi}{h^{''}(x^*)}\\right)^{1/2}\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#takeaways",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#takeaways",
    "title": "Committee Meeting",
    "section": "Takeaways",
    "text": "Takeaways\n\nLaplace approximation of the BF better than ABF for variable selection\nAddition information available from standard statistical software for GLMs\nPoor performance of ABF raises concerns about using SuSiE-RSS for summary statistics from non-Gaussian models\nGIBSS + asymptotic approximation looks like a good recipe for GLMs"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#notation",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#notation",
    "title": "Committee Meeting",
    "section": "Notation",
    "text": "Notation\n\\(p\\) an \\(M\\) degree polynomial\n\\({\\bf a} = (a_0, \\dots, a_M)\\) unique coefficients representing \\(p\\) in the monomial basis \\((1, x, \\dots, x^M)\\)\n\\[\\phi_{\\bf a}(x) = \\sum_{k=0}^M a_k x^k = p(x)\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#polynomial-approximation-inference",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#polynomial-approximation-inference",
    "title": "Committee Meeting",
    "section": "Polynomial approximation: inference",
    "text": "Polynomial approximation: inference\n\n\nApproximate the log-likelihood of each observation as a polynomial in linear prediction, \\(\\psi\\). \\({\\bf a}_i\\) s.t. \\(\\log p(y_i | \\psi) \\approx \\phi_{{\\bf a}_i}(\\psi)\\)\nApproximate the log prior density. \\(\\rho\\) s.t. \\(\\log p(\\beta) \\approx \\phi_{\\rho}\\)\nTransform \\({\\bf a}_i\\) so they are a function of \\(\\beta\\). \\({\\bf d}_i\\)\n\\(\\log p(y, \\beta) \\approx \\phi_{\\bf f}(\\beta), \\quad {\\bf f} := \\sum {\\bf d_i} + \\rho\\)"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#polynomial-exponential-family",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#polynomial-exponential-family",
    "title": "Committee Meeting",
    "section": "Polynomial exponential family",
    "text": "Polynomial exponential family\nWith \\({\\bf a} \\in \\mathbb R^M\\), and \\(T(x) = (x, \\dots, x^M)\\)\n\\[\nf(x; {\\bf a}) \\propto \\exp\\{\\langle {\\bf a}, T(x) \\rangle\\}\n\\]\n\nIntercept = normalizing constant. \\(a_0 = - \\log \\int \\exp\\{\\langle {\\bf a}, T(x) \\rangle\\} dx\\)\nHard integral, approximate or handle numerically"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#operations-polynomial-rescaling",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#operations-polynomial-rescaling",
    "title": "Committee Meeting",
    "section": "Operations: polynomial rescaling",
    "text": "Operations: polynomial rescaling\n\\[\\phi_{\\bf a}(cx) = \\phi_{{\\bf d}({\\bf a}, c)}(x)\\]\n\\[\n\\begin{aligned}\n\\sum_m a_m(c x)^m\n&= \\sum_m (a_m c^m) x^m \\\\\n&= \\sum_m {\\bf d}({\\bf a}, c)_m x^m, \\quad {\\bf d}({\\bf a}, c)_m := a_mc^m\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#polynomial-approximate-univariate-regression",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#polynomial-approximate-univariate-regression",
    "title": "Committee Meeting",
    "section": "Polynomial approximate univariate regression",
    "text": "Polynomial approximate univariate regression\n\nFor each \\(i\\), we have \\({\\bf a}_i\\) such that \\(\\log p(y_i | \\psi) \\approx \\phi_{{\\bf a}_i}(\\psi)\\)\nRepresent/approximate the log-prior density with a polynomial with coefficients \\(\\rho\\), i.e. \\(\\log p(\\beta) \\approx \\phi_\\rho(\\beta)\\)\n\n\n\\[\\begin{align}\n\\log p({\\bf y}, \\beta | {\\bf x}, \\beta)\n&\\approx \\sum_i \\phi_{{\\bf a}_i}(x_i \\beta) + \\phi_{\\rho}(\\beta)\\\\\n&= \\sum_i \\phi_{{\\bf d}({\\bf a}_i, x_i)}(\\beta) + \\phi_{\\rho}(\\beta) \\\\\n&= \\phi_{{\\bf f}(A, {\\bf x}, \\rho)}(\\beta), \\quad {\\bf f}(A, {\\bf x}, \\rho) = \\sum_i {\\bf d}({\\bf a}_i, x_i) + \\rho\n\\end{align}\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#example-univariate-regression",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#example-univariate-regression",
    "title": "Committee Meeting",
    "section": "Example: univariate regression",
    "text": "Example: univariate regression"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#increasing-degree-increases-accuracy",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#increasing-degree-increases-accuracy",
    "title": "Committee Meeting",
    "section": "Increasing degree increases accuracy",
    "text": "Increasing degree increases accuracy"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#polynomial-ser",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#polynomial-ser",
    "title": "Committee Meeting",
    "section": "Polynomial SER",
    "text": "Polynomial SER\n\\(A = \\{{\\bf a}_i \\}_{i=1}^n\\), base coefficients\n\\[\n\\hat F_{SER}(q; A, X, \\sigma^2_0) = \\mathbb E_q\\left[\n  \\sum \\phi_{{\\bf a}_i}(\\psi_i)]\\right] - KL[q || p] \\approx F_{SER}(q; {\\bf y}, X, \\sigma^2_0).\n\\]\n\nApproximate the SER by exactly optimizing an approximation of the ELBO\nEach univariate regression is easy to compute from the base coefficient \\(A\\)"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#operations-polynomial-shift",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#operations-polynomial-shift",
    "title": "Committee Meeting",
    "section": "Operations: polynomial shift",
    "text": "Operations: polynomial shift\nFor SuSiE: \\(\\psi = \\psi_l + \\psi_{-l}\\).\n\n\n\\[\\phi_{\\bf a}(x + y) = \\phi_{{\\bf b}({\\bf a}, y)}(x)\\]\n\n\nExpand and collect terms\n\\[\\begin{align}\n\\phi_{\\bf a}(x + y)\n&= \\sum_{m=0}^M a_m(x + y)^m = \\sum_{m=0}^M a_m \\sum_{k=0}^m {m \\choose k} x^k y^{m-k} \\\\\n&= \\sum_{k=0}^M \\left(\\sum_{m=k}^M a_m {m \\choose k}y^{m-k} \\right) x^k \\\\\n&= \\sum_{k=0}^M {\\bf b}({\\bf a}, y)_k x^k, \\quad {\\bf b}({\\bf a}, y)_k := \\left(\\sum_{m=k}^M a_m {m \\choose k}y^{m-k} \\right)\n\\end{align}\\]\n\n\nCan be written as a matrix multiplication \\[\n{\\bf b}({\\bf a}, y) = M(y) {\\bf a}, \\quad M_{ij} = {j \\choose i} y^{j -i}\\;  \\forall j \\geq i, 0 \\text{ otherwise}\n\\]\n\n\nFor random \\(y \\sim p\\), expected polynomial shift\n\\[\n{\\bf c}({\\bf a}, p)  := \\mathbb E_p[{\\bf b}({\\bf a}, y)]  = \\mathcal M(p){\\bf a}, \\quad \\mathcal M(p) := \\mathbb E_p[M(y)]\n\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#polynomial-shift-for-susie",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#polynomial-shift-for-susie",
    "title": "Committee Meeting",
    "section": "Polynomial shift for SuSiE",
    "text": "Polynomial shift for SuSiE\nEach observation must be transformed into a function of \\(\\psi_{l}\\)\nStart with \\({\\bf a}^* = \\mathcal M(q){\\bf a}\\)\n\n“Undo” polynomial shift \\({\\bf a}^{(l)} = M(q_l)^{-1} {\\bf a}^*\\)\nUpdate \\(q_{l, new} \\leftarrow \\arg\\max_{q_l}\\hat F_{SER}(q_l; A^{(l)}, X, \\sigma^2_0)\\)\n\\({\\bf a}^*_{new} \\leftarrow \\mathcal M(q_{l, new}) {\\bf a}^{(l)}\\)\n\n\nThe SER coefficients \\({\\bf a}^{(l)}\\) analogous to expected residuals in IBSS\n\\(\\mathcal M(q_l)\\) requires \\(\\mathbb E_{q_l}\\left[\\psi^k\\right], \\; k = 1, \\dots M\\)"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#coordinate-ascent-in-an-variational-objective",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#coordinate-ascent-in-an-variational-objective",
    "title": "Committee Meeting",
    "section": "Coordinate ascent in an variational objective",
    "text": "Coordinate ascent in an variational objective\n\\[\\begin{align}\n\\hat F_{SuSiE}(q_l; q_{-l}, A, X, \\sigma^2_0)\n&= \\mathbb E_{q_l} \\mathbb E_{q_{-l}} \\sum_i \\phi_{{\\bf a}_i}(\\psi_l + \\psi_{-l}) + KL[q|| p] \\\\\n&= \\mathbb E_{q_l} \\mathbb E_{q_{-l}} \\sum_i \\phi_{{\\bf c}({\\bf a}_i, \\psi_{-l})}(\\psi_l) + KL[q_l|| p_l] + C \\\\\n&= \\mathbb E_{q_l} \\sum_i \\phi_{{\\bf d}({\\bf a}_i,q_{-l})}(\\psi_l) + KL[q_l|| p_l] + C \\\\\n&= \\hat F_{SER} (q_l; D, X, \\sigma^2_0) + C, \\quad D = \\{{\\bf d}({\\bf a}_i,q_{-l})\\}_{i=1}^n\n\\end{align}\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#polynomial-interpolation",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#polynomial-interpolation",
    "title": "Committee Meeting",
    "section": "Polynomial interpolation",
    "text": "Polynomial interpolation\n\n\n\\(f : \\mathbb R \\rightarrow \\mathbb R\\)\n\\(n+1\\) distinct points \\(x_0 < \\dots < x_{n}\\)\nThere exists a unique \\(p \\in P_n\\) such that \\(p\\) interpolates \\(f\\) at \\(x_0, \\dots, x_n\\).\n\\[\\begin{align}\np(x) = \\sum_{i=0}^n f(x_i) L_{n, i}(x), \\quad L_{n, i}(x) = \\prod_{k \\neq i} \\frac{x - x_k}{x_i - x_k}\n\\end{align}\\]"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#chebyshev-interpolation",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#chebyshev-interpolation",
    "title": "Committee Meeting",
    "section": "Chebyshev interpolation",
    "text": "Chebyshev interpolation\n\n\n\n\n\n\nInterpolation at the zeros of Chebyshev polynomials \\(x_k = \\cos \\left(\\frac{2k-1}{2M}\\pi \\right), k = 1, \\dots, M\\)\nFast interpolation via re-scaled DFT of \\(f(x_1), \\dots, f(x_M)\\)\nApproximately minimizes \\(||f - \\hat f||_{\\infty}\\)"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#chebyshev-interpolation-glm-likelihoods",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#chebyshev-interpolation-glm-likelihoods",
    "title": "Committee Meeting",
    "section": "Chebyshev interpolation: GLM likelihoods",
    "text": "Chebyshev interpolation: GLM likelihoods"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#simulations",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#simulations",
    "title": "Committee Meeting",
    "section": "Simulations",
    "text": "Simulations"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#caveats-designing-good-polynomial-approximations",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#caveats-designing-good-polynomial-approximations",
    "title": "Committee Meeting",
    "section": "Caveats: designing good polynomial approximations",
    "text": "Caveats: designing good polynomial approximations\n\nRequire the leading degree of the polynomial approximation to be negative so that \\(\\int \\exp \\phi_{\\bf a} < \\infty\\)\nApproximation interval should contain the prediction for each observation with high probability for the approximation to be accurate \\(\\mathbb P( \\beta^T {\\bf x}_i \\in [L, R] | {\\bf y}, X) \\approx 1\\)\nQuality of the approximation (generally) decreases as approximation interval becomes wider"
  },
  {
    "objectID": "presentations/committee_meeting_june23/committee_meeting_june23.html#opportunities-1",
    "href": "presentations/committee_meeting_june23/committee_meeting_june23.html#opportunities-1",
    "title": "Committee Meeting",
    "section": "Opportunities",
    "text": "Opportunities\n\nIs there a faster way to perform “expected polynomial shift” operation?\nOther computationally convenient ways to approximation \\(\\log p(y | \\psi)\\)?"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#last-domino-constructing-base-approximation",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#last-domino-constructing-base-approximation",
    "title": "Gilad Lab WIP",
    "section": "Last domino: constructing base approximation",
    "text": "Last domino: constructing base approximation\n\n\nRecall: if we find a good polynomial approximation to the log-likelihood for each observation we can cheaply approximate inference in the non-conjugate model\nPrevious strategy: Chebyshev interpolating polynomials can approximately minimize \\(||f - \\hat f||_{\\infty}\\) on a target interval.\nProblem: interpolating polynomials are not guarunteed to have negative leading coefficient (a requirement for this approach)\nWhat is a general strategy for computing legal polynomial approximations?"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#susie-model",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#susie-model",
    "title": "Gilad Lab WIP",
    "section": "SuSiE: model",
    "text": "SuSiE: model\n\\[\n\\begin{aligned}\ny &\\sim N(X\\beta, \\sigma^2_0) \\\\\n\\beta &= \\sum_l b_l \\gamma_l \\\\\nb_l &\\sim N(0, \\sigma^2_0)\\\\\n\\gamma_l &\\sim \\text{Multinomial}(1, \\pi)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#variational-inference",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#variational-inference",
    "title": "Gilad Lab WIP",
    "section": "Variational inference",
    "text": "Variational inference\nInference as an optimization problem:\n\\[\\begin{align}\np(\\beta | {\\bf y}, X) = \\arg \\max_q F(q),\n\\quad F(q) := \\mathbb E_q \\left[ \\log p ({\\bf y} | X, \\beta) \\right] - KL[q || p]\n\\end{align}\\]\n\n\\[\\begin{align}\nF(q)\n&= \\mathbb E_q \\left[ \\log p ({\\bf y} | X, \\beta) \\right] - KL[q || p] \\\\\n&= \\mathbb E_q \\left[ \\log p ({\\bf y},  \\beta | X) \\right] + H(q) \\\\\n&= \\left(\\mathbb E_q \\left[ \\log p ({\\bf y},  \\beta | X) - Z \\right] + H(q) \\right) + Z \\\\\n&= - KL[q || p_{post}] + Z\n\\end{align}\\]\n\n\nWe’ll write the variational objective for SER \\(F_{SER}(q; {\\bf y}, X, \\sigma_0)\\)"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#susie-variational-approximation",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#susie-variational-approximation",
    "title": "Gilad Lab WIP",
    "section": "SuSiE: variational approximation",
    "text": "SuSiE: variational approximation\nRestrict \\(q\\) to some family \\(\\mathcal Q\\)\n\\[\\begin{align}\nq^*(\\beta) = \\arg \\max_{q \\in \\mathcal Q} F(q)\n\\end{align}\\]\nSuSiE uses \\(\\mathcal Q = \\{q : q(\\beta) = \\prod_l q_l(\\beta_l)\\}\\)\n\n\\[\nF_{SuSiE}(q; X, {\\bf y}) = \\mathbb E_{q} \\left[\n      -\\frac{1}{2\\sigma^2}||{\\bf y} - {\\bf X} \\beta_{-l} - X\\beta_l||^2_2)\n  \\right] + KL[q || p_{SuSiE}] + C(\\sigma)\n\\]"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#susie-coordinate-ascent-ibss",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#susie-coordinate-ascent-ibss",
    "title": "Gilad Lab WIP",
    "section": "SuSiE: coordinate ascent (IBSS)",
    "text": "SuSiE: coordinate ascent (IBSS)\nDefine the residual \\({\\bf r}_l = {\\bf y} - X\\beta_{-l}\\)\n\n\\[\n\\begin{aligned}\nF_{SuSiE}(q_l; q_{-l}, X, {\\bf y})\n&= \\mathbb E_{q_l} \\left[\n  \\mathbb E_{q_{-l}} \\left[\n      -\\frac{1}{2\\sigma^2}||{\\bf y} - {\\bf X} \\beta_{-l} - X\\beta_l||^2_2)\n  \\right]\n\\right] - KL[q || p] \\\\\n&= \\mathbb E_{q_l} \\left[\n  \\mathbb E_{q_{-l}} \\left[\n      -\\frac{1}{2\\sigma^2}|| {\\bf r}_l - X\\beta_l||^2_2)\n  \\right]\n\\right] - KL[q_l || p_l]  + C_1 \\\\\n&= \\mathbb E_{q_l} \\left[ \\color{blue}{\n    -\\frac{1}{2\\sigma^2} \\left(|| \\mathbb E_{q_{-l}} {\\bf r}_l- X\\beta_l||^2_2 + \\mathbb V ||r_l||^2_2 \\right)}\n\\right] - KL[q_l || p_l]  + C_1\\\\\n&= \\mathbb E_{q_l} \\left[\n    -\\frac{1}{2\\sigma^2} || \\mathbb E_{q_{-l}} {\\bf r}_l- X\\beta_l||^2_2]\n\\right] - KL[q_l || p_l]  + C_2  \\\\\n&= F_{SER}(q_l; X, \\mathbb E_{q{-l}}{\\bf r}_l) + C_2\n\\end{aligned}\n\\]\n\n\nKey: coordinate updates in \\(\\mathcal Q\\) reduce to solving an SER"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#generalized-ibss-gibss",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#generalized-ibss-gibss",
    "title": "Gilad Lab WIP",
    "section": "Generalized IBSS (GIBSS)",
    "text": "Generalized IBSS (GIBSS)\n\n\nHeuristic approach for applying SuSiE to non-Gaussian models\nIdea: apply IBSS, using expected predictions as fixed offsets\n\n\n\n\\[\\begin{align}\nF_{SuSiE}(q)\n&= \\mathbb E_q[\\log p({\\bf y} | X, \\beta_l, \\beta_{-l})] - \\sum_l KL[q_l || p_l] \\\\\n&= \\mathbb E_{q_l}[ \\mathbb E_{q_{-l}}[\\log p({\\bf y} | X, \\beta_l, \\beta_{-l})]] - \\sum_l KL[q_l || p_l] + C_1 \\\\\n&\\color{purple}{\\approx \\mathbb E_{q_l}[\\log p({\\bf y} | X, \\beta_l, \\bar \\beta_{-l})] - KL[q_l || p_l] + C_2} \\\\\n&= F_{SER}(q_l; {\\bf y}, X, \\bar \\beta_{-l}) + C_2\n\\end{align}\\]"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#why-should-this-work",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#why-should-this-work",
    "title": "Gilad Lab WIP",
    "section": "Why should this work?",
    "text": "Why should this work?\n\\[\n\\begin{aligned}\nF_{SuSiE}(q)\n&= \\mathbb E_q[\\log p({\\bf y} | X, \\beta_l, \\beta_{-l})] - \\sum_l KL[q_l || p_l] \\\\\n&\\color{purple}{\\approx \\mathbb E_{q_l}[\\log p({\\bf y} | X, \\beta_l, \\bar \\beta_{-l})] - KL[q_l || p_l] + C_2} \\\\\n\\end{aligned}\n\\]\n\n\nIf \\(\\log p( {\\bf y} | X, \\beta)\\) is quadratic in \\(\\beta\\) (Gaussian) there \\(\\color{purple}\\approx\\) is \\(=\\)\nInformally, approximation is good if \\(\\log p( {\\bf y} | X, \\beta)\\) is well approximated by a quadratic function in \\(\\beta\\).\nReally we care about \\(\\psi = X\\beta\\)"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#algorithm-single-effect-regression",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#algorithm-single-effect-regression",
    "title": "Gilad Lab WIP",
    "section": "Algorithm: Single effect regression",
    "text": "Algorithm: Single effect regression\nRequire a function \\(G\\) which computes the BF and posterior mean of a Bayesian univariate regression\n\\[\\begin{align}\n{\\bf y} \\sim 1 + {\\bf x} + {\\bf o} \\\\\nb \\sim N(0, \\sigma^2)\n\\end{align}\\]"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#advantage-and-limitations",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#advantage-and-limitations",
    "title": "Gilad Lab WIP",
    "section": "Advantage and limitations",
    "text": "Advantage and limitations\n\n\nAdvantages\n\nSeems to work well!\nHighly modular: just need to implement the univariate regression\n\n\n\n\nDisadvantages\n\nHeuristic– not optimizing a clear objective function\nDoes not properly account for uncertainty in \\(\\beta_{-l}\\), only uses posterior mean\n\n\n\n\nOpportunities\n\nCan we analyze the approximation error?\nCan we make guarantees on GIBSS performance asymptotically?"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#sources-of-error",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#sources-of-error",
    "title": "Gilad Lab WIP",
    "section": "Sources of error",
    "text": "Sources of error\n\n\nGIBSS with exact SER\n\nTreating random effects as fixed effects (only need to pass posterior means)\n\n\n\n\nGIBSS with asymptotic approximation\n\nAsymptotic approximation of posterior means (only need to compute MLE and std. errors)\nApproximation of BFs"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#algorithm-gibss",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#algorithm-gibss",
    "title": "Gilad Lab WIP",
    "section": "Algorithm: GIBSS",
    "text": "Algorithm: GIBSS"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#simulations-logistic-gibss-l5",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#simulations-logistic-gibss-l5",
    "title": "Gilad Lab WIP",
    "section": "Simulations: logistic-GIBSS \\(L=5\\)",
    "text": "Simulations: logistic-GIBSS \\(L=5\\)\nMuch better performance compared to direct VB approach"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#lasso-simulation",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#lasso-simulation",
    "title": "Gilad Lab WIP",
    "section": "Lasso simulation",
    "text": "Lasso simulation\nFit lasso to each gene list, simulate new gene lists from lasso fit (x20) - Gene sets: MSigDb Hallmark + C2 - ~7500 genes, ~4500 gene sets"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#lasso-simulation-cs-summary",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#lasso-simulation-cs-summary",
    "title": "Gilad Lab WIP",
    "section": "Lasso simulation CS summary",
    "text": "Lasso simulation CS summary\nSome of the CSs are large (uninformative)\n\n\n\n“Confident” discoveries 260/527 vs lasso 460 effects\nSuSiE has good CS coverage ~94%\nLasso has high FP rate ~34%\n\n\n\n\n\n\n\n# Causal x CS size\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n12\n\n\n\n\n\n\n0\n\n\n17\n\n\n3\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n1\n\n\n244\n\n\n6\n\n\n4\n\n\n2\n\n\n2\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n\n\n2\n\n\n0\n\n\n14\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n\n\n3\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#gibss-for-logistic-regression",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#gibss-for-logistic-regression",
    "title": "Gilad Lab WIP",
    "section": "GIBSS for logistic regression",
    "text": "GIBSS for logistic regression\nGoal: demonstrate improvement from using Laplace ABF over ABF in GIBSS for settings commonly found in case-control GWAS\n\nImplication: Naive application of “summary stat” based finemapping methods not appropriate for e.g. logistic regression, GLMs, GLMMs."
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#realistic-simulation-plan",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#realistic-simulation-plan",
    "title": "Gilad Lab WIP",
    "section": "Realistic simulation plan",
    "text": "Realistic simulation plan\nUKBB genotype data\n\nSimulate from UKBB imputed genotypes\nSelect 50k individual per UKBB, keep SNPs with MAF > 1% in sample\n\nSimulation parameters\n\n\n\\[\n\\begin{aligned}\ny_i \\sim Bernoulli(p_i) \\\\\n\\log \\frac{p_i}{1 - p_i} = \\beta_{0i} + \\beta x_i\\\\\n\\beta_{0i} \\sim N(b_0, \\sigma^2_0) \\\\\n\\beta \\sim N(0, \\sigma^2)\n\\end{aligned}\n\\]\n\n\nRandom intercept models genetic contributions at un-linked loci, larger \\(\\sigma^2_0\\) means focal SNP explains smaller fraction of \\(h^2\\)\n\\(\\sigma^2\\) should be selected to reflect un-normalized effect sizes in GWAS\nTune \\(b_0\\), \\(\\sigma^2_0\\) to reflect case-control ratios and polygenecity of trait"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#this-simulation",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#this-simulation",
    "title": "Gilad Lab WIP",
    "section": "This simulation",
    "text": "This simulation\n\n\n\\[\n\\begin{aligned}\ny_i \\sim Bernoulli(p_i) \\\\\n\\log \\frac{p_i}{1 - p_i} = \\beta_{0i} + \\beta x_i\\\\\n\\beta_{0i} \\sim N(b_0, \\sigma^2_0) \\\\\n\\beta \\sim N(0, \\sigma^2)\n\\end{aligned}\n\\]\n\n\nRandom intercept models genetic contributions at un-linked loci, larger \\(\\sigma^2_0\\) means focal SNP explains smaller fraction of \\(h^2\\)\n\\(\\sigma^2\\) should be selected to reflect un-normalized effect sizes in GWAS\nTune \\(b_0\\), \\(\\sigma^2_0\\) to reflect case-control ratios and polygenecity of trait\n\n\n\n\n\\(\\sigma^2_0 = 0\\) (fixed intercept)\nSelect causal SNP to have MAF in \\((0.2, 0.25)\\)\nTuned \\(b_0\\) to give frequency of cases to approx. \\(1 - 10\\%\\)\n50 replicates on one region of chromosome 1."
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#logistic-susie-abf-vs-laplace",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#logistic-susie-abf-vs-laplace",
    "title": "Gilad Lab WIP",
    "section": "Logistic SuSiE ABF vs Laplace",
    "text": "Logistic SuSiE ABF vs Laplace\n\n\n\n95% CS coverage\n\n\nMethod\nCoverage\n\n\n\n\nabf\n0.953\n\n\nlaplace_abf\n0.977\n\n\n\n\n\n{fig-align=“right”, height=10%}"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#gibss-for-survival-analysis",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#gibss-for-survival-analysis",
    "title": "Gilad Lab WIP",
    "section": "GIBSS for survival analysis",
    "text": "GIBSS for survival analysis\n\n\nYunqi has been investigating specifically the application of SuSiE for survival analysis.\n\n‘Non-standard’: optimization of partial likelihood to get MLE and standard error, censored data\nFits within the GIBSS framework!\n\n\n\n\n\n\n\nIndependent\n\n\n\n\n\n\n\nCorrelated (real genotypes)"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#survival-susie-vs-bvsnlp",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#survival-susie-vs-bvsnlp",
    "title": "Gilad Lab WIP",
    "section": "Survival SuSiE vs BVSNLP",
    "text": "Survival SuSiE vs BVSNLP\n\n\n\n\n\nSurvival SuSiE without ABF correction\n\n\n\n\n\n\n\nSurvival SuSiE with ABF correction"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#overview",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#overview",
    "title": "Gilad Lab WIP",
    "section": "Overview",
    "text": "Overview\n\nGIBSS requires computing the BF and posterior mean for each variable\nStandard statistical software for computing MLE\nFor large sample sizes, we can leverage asymptotic normality of the MLE to approximate posterior mean and BF."
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#posterior-mean",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#posterior-mean",
    "title": "Gilad Lab WIP",
    "section": "Posterior mean",
    "text": "Posterior mean\n\\[\n\\begin{aligned}\n\\hat \\beta | \\beta, s^2 &\\sim N(\\beta, s^2) \\\\\n\\beta &\\sim N(0, \\sigma^2)\n\\end{aligned}\n\\]\n\n\\[\n\\beta | \\hat \\beta, s^2 \\sim N \\left(\n\\frac{\\sigma^2}{s^2 + \\sigma^2} \\hat\\beta, \\left(\\frac{1}{s^2} + \\frac{1}{\\sigma^2}\\right)^{-1}\n\\right)\n\\]"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#approximating-the-the-bf",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#approximating-the-the-bf",
    "title": "Gilad Lab WIP",
    "section": "Approximating the the BF",
    "text": "Approximating the the BF\n\\[BF = \\int \\color{red}{\\frac{ p(\\mathcal {\\bf y} | {\\bf x}, \\beta)}{p({\\bf y} | \\beta = 0)}} N(\\beta | 0, \\sigma^2) d\\beta\\]\nApproximate the likelihood ratio in a way that’s easy to integrate\n\\[\nLR(\\beta_1, \\beta_2) = \\frac{p(\\mathcal {\\bf y} | {\\bf x}, \\beta)}{p({\\bf y} | \\beta=0)}\n\\]"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#wakefields-asymptotic-bf",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#wakefields-asymptotic-bf",
    "title": "Gilad Lab WIP",
    "section": "Wakefields asymptotic BF",
    "text": "Wakefields asymptotic BF\nAsymptotically, \\(\\hat \\beta | \\beta, s^2 \\sim N(\\beta, s^2)\\). Then\n\\[\nLR(\\beta, 0)\n= \\frac{p(y | \\hat\\beta, \\beta) p(\\hat\\beta | \\beta)}{p(y | \\hat\\beta, \\beta = 0) p(\\hat\\beta | \\beta = 0)}\n\\approx \\color{red}{\\frac{p(y | \\hat\\beta)}{p(y | \\hat\\beta)}} \\frac{p(\\hat\\beta | \\beta)}{ p(\\hat\\beta | \\beta = 0)} \\approx \\frac{N(\\hat\\beta| \\beta, s^2)}{N(\\hat\\beta| 0, s^2)} = \\widehat{LR}_{ABF}(\\beta, 0).\n\\]\n\nIntegrating over the prior gives Wakefield’s asymptotic Bayes Factor (ABF)\n\\[\nABF = \\int \\widehat{LR}_{ABF}(\\beta, 0) N(\\beta | 0, \\sigma^2_0) d\\beta = \\frac{N(\\hat\\beta | 0, s^2 + \\sigma^2_0)}{N(\\hat\\beta | 0, s^2)}\n\\]"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#a-problem-with-abf",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#a-problem-with-abf",
    "title": "Gilad Lab WIP",
    "section": "A problem with ABF",
    "text": "A problem with ABF\n\n\\(LR(\\beta, 0) \\approx \\widehat {LR}_{ABF}(\\beta, 0)\\)\n\\(LR(0, 0) = \\widehat {LR}_{ABF}(0, 0) = 1\\)\nThe asymptotic approximation may not be a good in the tails, an issue for \\(\\hat\\beta/s >> 0\\)"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#adjusting-the-abf",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#adjusting-the-abf",
    "title": "Gilad Lab WIP",
    "section": "Adjusting the ABF",
    "text": "Adjusting the ABF\nIdea: use the asymptotic approximation where it is good\n\\[\n\\begin{aligned}\nLR(\\beta, 0)\n&= LR(\\beta, \\hat{\\beta}) LR(\\hat{\\beta}, 0) \\\\\n&\\approx \\widehat{LR}_{ABF}(\\beta, \\hat\\beta)LR(\\hat\\beta, 0) \\\\\n&= \\widehat{LR}_{Lap}(\\hat\\beta, 0)\n\\end{aligned}\n\\]\n\nRequires an extra piece of information: \\(LR(\\hat\\beta, 0)\\)"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#corrected-abflaplace-approximation",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#corrected-abflaplace-approximation",
    "title": "Gilad Lab WIP",
    "section": "Corrected ABF/Laplace approximation",
    "text": "Corrected ABF/Laplace approximation\n\nRequires knowledge of the LR of the MLE against the null.\nCan dramatically improve approximation of the BF"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#comparison-of-logbf",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#comparison-of-logbf",
    "title": "Gilad Lab WIP",
    "section": "Comparison of logBF",
    "text": "Comparison of logBF"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#comparison-of-logbf-1",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#comparison-of-logbf-1",
    "title": "Gilad Lab WIP",
    "section": "Comparison of logBF",
    "text": "Comparison of logBF"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#abf-correction-reorders-pips",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#abf-correction-reorders-pips",
    "title": "Gilad Lab WIP",
    "section": "ABF Correction reorders PIPs",
    "text": "ABF Correction reorders PIPs"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#laplace-approximation",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#laplace-approximation",
    "title": "Gilad Lab WIP",
    "section": "Laplace approximation",
    "text": "Laplace approximation\nFor some non-negative function \\(f: \\mathbb R \\rightarrow \\mathbb R^+\\), want to compute the integral\n\\[I = \\int f(x) dx,\\]\n\nDefine \\(h(x) = \\log f(x)\\), expand around the maximizer of \\(h\\), \\(x^*\\)\n\\[\\hat h (x) = h(x^*) + \\frac{1}{2} h^{''}(x^*)(x-x^*)^2\\]\n\n\nApproximate with the Gaussian integral\n\\[ \\hat I = \\int \\exp{\\hat h(x))} dx = \\exp{\\hat h(x^*)} \\left(-\\frac{2\\pi}{h^{''}(x^*)}\\right)^{1/2}\\] . . .\nPotential next step: saddle point approximations?"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#takeaways",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#takeaways",
    "title": "Gilad Lab WIP",
    "section": "Takeaways",
    "text": "Takeaways\n\nLaplace approximation of the BF better than ABF for variable selection\nAddition information available from standard statistical software for GLMs\nPoor performance of ABF raises concerns about using SuSiE-RSS for summary statistics from non-Gaussian models\nGIBSS + asymptotic approximation looks like a good recipe for GLMs"
  },
  {
    "objectID": "presentations/gilad_lab_july23/committee_meeting_june23.html#fast-implementation",
    "href": "presentations/gilad_lab_july23/committee_meeting_june23.html#fast-implementation",
    "title": "Gilad Lab WIP",
    "section": "Fast implementation",
    "text": "Fast implementation\nProblem\n\nGIBSS requires evaluating many univariate regression\nGLMs fit via IRLS (equivalently, Newton-Raphson with step-size \\(\\gamma = 1\\)). \\(\\beta_t = \\beta_{t-1} - \\gamma H^{-1}_{t-1} g_{t-1}\\)\n\\(p\\) 2d optimization problems that can be carried out in parallel\n\nImplementation\n\nImplemented in Python using Google’s jax (numpy, with automatic differentiation, JIT compilation, and vectorization)\nNewton-Raphson with halving step-size\nEasy to extend to many GLMs (or any regression with twice-differentiable log-likelihood)– just define a log_likelihood function.\nsusiepy package at http://github.com/karltayeb/susiepy\n\nOpen questions for optimizing implementation\n\nCoordinate ascent faster on machine compared to full Newton updates?\nAvoid fitting intercept for each variable separately?\nAvoid checking for likelihood increase\nExploit sparsity in \\(X\\) for faster implementation?\nPartial updates of “inner loop” while fitting GIBBS"
  }
]